位置编码 (Position Encoding)
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-09&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-12%2000%3A14%3A55&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-09 11:03:29
toc_title: 位置编码 (PE)
top: false
draft: false
hidden: false
level: 0
tag: [dl_transformer]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [Transformer](./README.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [核心概念](#核心概念)
- [绝对位置编码](#绝对位置编码)
    - [正弦位置编码](#正弦位置编码)
    - [可学习位置编码](#可学习位置编码)
- [相对位置编码](#相对位置编码)
    - [SHAW](#shaw)
    - [XLNet](#xlnet)
    - [T5](#t5)
    - [DeBERTa](#deberta)
    - [ALiBi](#alibi)
- [旋转位置编码 🔥](#旋转位置编码-)
- [参考资料](#参考资料)
    - [面试问题整理](#面试问题整理)
<!--END_SECTION:toc-->

---

## 核心概念

- **动机**:
    - 自注意力的本质是 **加权求和**, 在数学上是一种 **集合** 操作, 具有 **置换等变性 (Permutation Equivariant)**;
    - 这意味着 **即使打乱输入序列的顺序, 注意力机制也会产生相同的分布**;
        > [测试置换等变性.py](./code/test_permutation.py)
    - 为了使其能够理解序列的 **顺序** 概念, 必须显式引入 **位置信息**;
- **方法** (如何引入位置信息)
    1. 在输入模型前, 对词嵌入施加位置编码 (如 **正弦位置编码**);
    2. 在计算注意力前, 对 Q/K 施加变换 (如 **RoPE**);
    3. 在得到注意力后, 对 logits 施加偏置 (如 **ALiBi**);
- **理想属性**
    - **唯一性**: 每个位置的编码唯一 (所有位于位置 $i$ 的 token 有相同的编码)
    - **确定性**: 由确定性公式或算法生成, 便于模型学习/泛化;
    - **外推性 (Extrapolation)**: 能泛化到比训练序列更长的输入;
    - **相对位置感知**: 两个位置编码之间应该蕴含 **相对位置** 信息, 并具有简单的数学关系 (最好是线性的);
    - **距离衰减**: 距离越远的两个位置, 注意力越弱;
    - **扩展性**: 能自然扩展到多维数据 (图像, 视频);

## 绝对位置编码

### 正弦位置编码

- **来源/使用模型**: Transformer 原版
- **思路/原理**: 使用具有固定模式的三角函数为每个位置生成独一无二的编码向量;
- **公式**:
    - $PE_{(pos, 2i)} = \sin(\dfrac{pos}{10000^{2i/d_{model}}}), \hspace{1em} PE_{(pos, 2i+1)} = \cos(\dfrac{pos}{10000^{2i/d_{model}}})$
    - 其中
        - $pos$ 是 token 的位置索引 (从 $0$ 到 `max_len`);
        - $i$ 是编码向量的维度索引 (从 $0$ 到 $d_{model}-1$);
        - $d_{model}$ 是编码向量的总维度;
    - 生成过程:
        ```python
        # max_len, d_model = 3, 4

        # 1. 初始化一个全零的位置编码矩阵 [max_len, d_model]
        pe = np.zeros((max_len, d_model))

        # 2. 生成位置序列 [0, 1, 2, ..., max_len-1], 并转换为列向量 [max_len, 1]
        pos = np.arange(0, max_len).reshape(-1, 1)

        # 3. 计算分母中的项 (取倒数): inv = 10000^(-2i/d_model)
        #    公式: 10000^(-2i/d_model) 等价于 exp( (-2i / d_model * log(10000)))
        inv = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))

        # 4. 对偶数索引 (2i) 位置应用正弦函数, 奇数索引 (2i+1) 位置应用余弦函数
        pe[:, 0::2] = np.sin(pos * inv)
        pe[:, 1::2] = np.cos(pos * inv)
        ```
- **优点**:
    - 唯一, 确定, 无需学习 (节省参数);
    - 具备一定的 **外推能力 (extrapolation)**, 可处理比训练更长的序列;
    - 其数学特性 **蕴含相对位置信息**, 即 $PE(pos+k)$ 可表示为 $PE(pos)$ 的线性函数;
- **缺点**:
    - 外推能力有限/效果不佳;
- **拓展**:
    - 正弦位置编码中的 "波长" 是什么意思? 不同维度对应的波长有何不同?
        > 波长 $\lambda$ 由公式中的 $10000^{2i/d_{model}}$ 决定. 随着维度 $i$ 从 $0$ 增加到 $d_{model}/2 - 1$, 波长从 $2π$ (高频, 变化快) 增长到 $10000 * 2π$ (低频, 变化慢); **这使模型能同时捕获近距离和远距离的位置关系**.

### 可学习位置编码

- **来源/使用模型**: BERT
- **方法**: 随机初始化一个大小为 `(max_seq_len, d_model)` 的矩阵作为位置编码参数, 然后与模型的其他参数一起通过训练学习得到;
- **优点**:
    - 灵活, 可能学到更适合特定任务的位置模式;
- **缺点**:
    - **不具备外推能力** (内插), 严格依赖于训练时见过的位置 (`max_len`);
    - 增加了模型参数量;

## 相对位置编码

- **来源**: Shaw et al., XLNet, T5, DeBERTa, ALiBi
- **思路**: 对 **带有绝对位置编码** 的 Attention 打分公式进行展开, 并通过 **修改其中的分项** 来引入相对位置信息;
    - 带绝对位置编码的注意力打分公式为:
        $$
        q_i k_j^\top = (x_i W_Q + p_i W_Q)(x_j W_K + p_j W_K)^\top
        $$
    - 展开:
        $$
        q_i k_j^\top
            = \underbrace{x_i W_Q W_K^\top x_j^\top}_{\text{1.输入–输入}}
            + \underbrace{x_i W_Q W_K^\top p_j^\top}_{\text{2.输入–位置}}
            + \underbrace{p_i W_Q W_K^\top x_j^\top}_{\text{3.位置–输入}}
            + \underbrace{p_i W_Q W_K^\top p_j^\top}_{\text{4.位置–位置}}
        $$
- **不同的相对位置编码就是在这四项中做删减、替换或组合**:
- **优点**:
    - **外推能力更好**
- **缺点**:
    - 实现复杂, 增加了计算复杂度;

### SHAW
> SHAW 是作者名
- **思路**: 去掉 $p_i W_Q$, 并将 $p_j W_K$ 替换为相对位置向量 $R^K_{i,j}$, 其中 $R^K_{i,j}$ 依赖于相对距离 $i-j$:
    $$
    q_i k_j^\top
        = x_i W_Q \left( x_j W_K + R^K_{i,j} \right)^\top
    $$

### XLNet
- **思路**: 将 $p_j$ 替换为相对位置向量 $R_{i-j}$, 将 $p_i$ 替换为可训练向量 $u, v$, 其中 $R_{i-j}$ 用正弦位置编码生成:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        + x_i W_Q W_{K,R}^\top R_{i-j}^\top
        + u\, W_K^\top x_j^\top
        + v\, W_{K,R}^\top R_{i-j}^\top
    $$

### T5
- **思路**: 去掉 **2, 3** 两项, 并用可训练偏置 $\beta_{i,j}$ 代替 **4** 项, 其中 $\beta_{i,j}$ 通过 **分桶** 映射相对距离后查表得到:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        + \beta_{i,j}
    $$

### DeBERTa
- **思路**: 去掉 **4** 项, 保留 **2, 3** 两项, 并替换为相对位置向量, 其中 $R_{i,j}$ 依赖相对距离:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        + x_i W_Q W_K^\top R_{i,j}^\top
        + R_{j,i} W_Q W_K^\top x_j^\top
    $$

### ALiBi
- **思路**: 去掉 **2, 3** 两项, 并用 **固定线性偏置** 代替 **4** 项, 其中 $m$ 为第 $h$ 个注意力头的常数斜率:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        - m · (i − j)
    $$


<!-- ✅❌⭕❓✔️☑️⚠️🔄➡️↔️📌📍🔖🏷️💡📝 -->
## 旋转位置编码 🔥
> RoPE, Rotary Positional Encoding
- **来源**: RoFormer
- **设计目标**:
    - 编码本身能表达 token 的 **绝对位置信息**;
    - 在计算注意力 (Query 和 Key 的点积) 时, 内积结果能够天然体现 token 之间的 **相对位置关系**;
    - 换言之, 如果两个向量在经过某种绝对位置编码后, 它们点积的结果只取决于它们的相对位置, 那么就实现了设计目标;
- **思路/原理/直觉/动机**:
    - 数学上, 如果有一种变换, 使对两个元素 **相乘** 可以表达成它们 **差的形式**, 理论上就达成了目标;
    - **旋转变换可以满足这一性质**;
        - 对向量施加旋转, **不同旋转角度可以表达绝对位置**;
        - 而两个向量经过旋转后点积, 其结果只与它们的 **相对位置 (旋转的角度差)** 有关, 与各自实际旋转的角度无关;
    - **综上**, 通过对输入向量进行 **旋转** 即可达成目标 —— **利用绝对位置编码的方式, 实现相对位置编码的效果**;
- **做法**:
    - 通过 **旋转矩阵** 对 Q/K 向量进行变换, 使内积结果 **仅依赖于相对位置**;
- **计算步骤**
    ```txt
    === RoPE 计算步骤演示 ===
    步骤 1: 向量两两分组
    [[0.497, -0.138], [0.648, 1.523], [-0.234, -0.234]] ...

    步骤 2: 计算各组的旋转角度, 数量为 D/2
    [3.0, 0.3, 0.03] ...

    步骤 3: 分组应用旋转
    原始向量 (分组): [[0.497, -0.138], [0.648, 1.523], [-0.234, -0.234]] ...
    旋转后向量 (分组): [[-0.472, 0.207], [0.169, 1.646], [-0.227, -0.241]] ...
    旋转后向量 (展开): [-0.472, 0.207, 0.169, 1.646, -0.227, -0.241] ...

    范数比较
    原始范数: 2.48947373
    旋转后范数: 2.48947373
    ```
- **旋转角度** 计算:
    - 因 **两两分组**, 对第 $m$ 和 $m+1$ 个 token, 其在第 $i$ 个维度 ($i \in {1,2,..,d/2}$) 上的旋转角度为:
    $$
    \theta_i = m \cdot \dfrac{1}{10000^{2(i-1)/d}}
    $$
    - 其中
        - $m$ 为 token 位置索引;
        - $i$ 为 向量 的分量索引;
            > 公式中加入 $i$ 的作用: 让向量中不同维度的分量负责不同尺度的位置信息;
            >> - 低维 ($i$ 小): $θ_i$ 较大 (高频), 波长短; 这些维度对位置变化非常敏感, 旋转很快; 它们负责捕捉局部、精细的位置关系;  
            >> - 高维 ($i$ 大): $θ_i$ 较小 (低频), 波长长; 这些维度旋转缓慢, 负责提供全局、稳定的位置信号;
    - **为什么这么设计 $\theta$?** —— **远程衰减性**, 即内积随相对距离增大而衰减, 这与 **距离越远, 依赖越弱** 的直觉一致;
- **优点**:
    - 具备绝对位置的形式, 产生相对位置的效果 (从绝对位置推导出相对关系);
    - 优秀的 **外推性**;
    - 随着相对距离的增加, 依赖性自然衰减;
        > 旋转角度差越大, 两个向量的点积越小;
- **代码演示**:
    > [test_rope.py](./code/test_rope.py)
    - **相对性验证**: $\langle \text{RoPE}(q, m), \text{RoPE}(k, n) \rangle = \langle q, \text{RoPE}(k, m - n) \rangle$
        > $\text{RoPE}(vec, pos)$: 对位于 $pos$ 的向量 $vec$ 旋转 $pos \cdot \theta$ 度; 多维情况需两两分组后进行旋转 (详见代码);
        ```txt
        === RoPE 相对性验证 ===
        m    n    m-n    dot_val     ro_dot_val
        ----------------------------------------
        0    0    0      -4.081900   -4.081900
        4    4    0      -4.081900   -4.081900
        10   0    10     -2.769302   -3.336345
        15   5    10     -2.769302   -3.336345
        18   8    10     -2.769302   -3.336345
        6    16   -10    -3.336345   -2.769302
        16   26   -10    -3.336345   -2.769302
        3    13   -10    -3.336345   -2.769302
        ```
        - **直观解释**: 对于两个经过 RoPE 旋转后的向量 $q_m$ (位于 $m$) 和 $k_n$ (位于 $n$), 它们的点积结果只与它们的相对位置 $m-n$ 有关, 与绝对位置无关;

## 参考资料
- [设计位置编码 - HuggingFace Blog](https://huggingface.co/blog/zh/designing-positional-encoding)
- [让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces](https://kexue.fm/archives/8130)
- [十分钟读懂旋转编码 (RoPE) - 知乎](https://zhuanlan.zhihu.com/p/647109286)
    - [再论大模型位置编码及其外推性 (万字长文) - 知乎](https://zhuanlan.zhihu.com/p/675243992)


### 面试问题整理
> [位置编码问题整理](./Transformer面试题-位置编码.md)