位置编码
===
<!--START_SECTION:badge-->

![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-09&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-10%2014%3A09%3A29&label_color=gray&color=thistle&style=flat-square)

<!--END_SECTION:badge-->
<!--info
date: 2025-09-09 11:03:29
top: false
draft: true
hidden: true
level: 0
tag: [dl_transformer]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [Transformer](./README.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [核心概念](#核心概念)
- [绝对位置编码](#绝对位置编码)
    - [正余弦位置编码](#正余弦位置编码)
    - [可学习位置编码](#可学习位置编码)
- [相对位置编码](#相对位置编码)
        - [SHAW](#shaw)
        - [XLNet](#xlnet)
        - [T5](#t5)
        - [DeBERTa](#deberta)
        - [ALiBi](#alibi)
- [旋转位置编码](#旋转位置编码)
- [参考资料](#参考资料)
<!--END_SECTION:toc-->

---

## 核心概念

- **动机**: 
    - 自注意力的本质是 **加权求和**, 在数学上是一种 **集合** 操作, 具有 **置换等变性 (Permutation Equivariant)**; 
    - 这意味着 **即使打乱输入序列的顺序, 注意力机制也会产生相同的分布**;
        > [测试置换等变性.py](./code/test_permutation.py)
    - 为了使其能够理解序列的 **顺序** 概念, 必须显式引入 **位置信息**;
- **方法** (如何引入位置信息)
    1. 在输入模型前, 对词嵌入施加位置编码 (如 **正余弦位置编码**);
    2. 在计算注意力前, 对 Q/K 施加变换 (如 **RoPE**);
    3. 在得到注意力后, 对 logits 施加偏置 (如 **ALiBi**);
- **理想属性**
    - **唯一性**: 每个位置的编码唯一 (所有位于位置 $i$ 的 token 有相同的编码)
    - **确定性**: 由确定性公式或算法生成, 便于模型学习/泛化;
    - **外推性 (Extrapolation)**: 能泛化到比训练序列更长的输入;
    - **相对位置感知**: 两个位置编码之间应该蕴含 **相对位置** 信息, 并具有简单的数学关系 (最好是线性的);
    - **扩展性**: 能自然扩展到多维数据 (图像, 视频);

## 绝对位置编码

### 正余弦位置编码

- **来源/使用模型**: Transformer 原版
- **思路/原理**: 使用具有固定模式的三角函数为每个位置生成独一无二的编码向量;
- **公式**:
    - $PE_{(pos, 2i)} = \sin(\dfrac{pos}{10000^{2i/d_{model}}}), \hspace{1em} PE_{(pos, 2i+1)} = \cos(\dfrac{pos}{10000^{2i/d_{model}}})$
    - 其中 
        - $pos$ 是 token 的位置索引 (从 $0$ 到 `max_len`); 
        - $d_{model}$ 是编码向量的总维度;
        - $i$ 是编码向量的维度索引 (从 $0$ 到 $d_{model}-1$);
    - 生成过程:
        ```python
        # max_len, d_model = 3, 4

        # 1. 初始化一个全零的位置编码矩阵 [max_len, d_model]
        pe = np.zeros((max_len, d_model))

        # 2. 生成位置序列 [0, 1, 2, ..., max_len-1], 并转换为列向量 [max_len, 1]
        pos = np.arange(0, max_len).reshape(-1, 1)

        # 3. 计算分母中的项 (取倒数): inv = 10000^(-2i/d_model)
        #    公式: 10000^(-2i/d_model) 等价于 exp( (-2i / d_model * log(10000)))
        inv = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))

        # 4. 对偶数索引 (2i) 位置应用正弦函数, 奇数索引 (2i+1) 位置应用余弦函数
        pe[:, 0::2] = np.sin(pos * inv)
        pe[:, 1::2] = np.cos(pos * inv)
        ```
- **优点**: 
    - 唯一, 确定, 无需学习 (节省参数);
    - 具备一定的 **外推能力 (extrapolation)**, 可处理比训练更长的序列; 
    - 其数学特性 **蕴含相对位置信息**, 即 $PE(pos+k)$ 可表示为 $PE(pos)$ 的线性函数;
- **缺点**: 
    - 外推能力有限/效果不佳;

### 可学习位置编码

- **来源/使用模型**: BERT
- **方法**: 随机初始化一个大小为 `(max_seq_len, d_model)` 的矩阵作为位置编码参数, 然后与模型的其他参数一起通过训练学习得到;
- **优点**:
    - 灵活, 可能学到更适合特定任务的位置模式;
- **缺点**:
    - **不具备外推能力** (内插), 严格依赖于训练时见过的位置 (`max_len`);
    - 增加了模型参数量;

## 相对位置编码

- **来源**: Shaw et al., XLNet, T5, DeBERTa, ALiBi
- **思路**: 对 **带有绝对位置编码** 的 Attention 打分公式进行展开, 并通过 **修改其中的分项** 来引入相对位置信息;
    - 带绝对位置编码的注意力打分公式为:
        $$
        q_i k_j^\top = (x_i W_Q + p_i W_Q)(x_j W_K + p_j W_K)^\top
        $$
    - 展开:
        $$
        q_i k_j^\top 
            = \underbrace{x_i W_Q W_K^\top x_j^\top}_{\text{1.输入–输入}}
            + \underbrace{x_i W_Q W_K^\top p_j^\top}_{\text{2.输入–位置}}
            + \underbrace{p_i W_Q W_K^\top x_j^\top}_{\text{3.位置–输入}}
            + \underbrace{p_i W_Q W_K^\top p_j^\top}_{\text{4.位置–位置}}
        $$
- **不同的相对位置编码就是在这四项中做删减、替换或组合**:
- **优点**:
    - **外推能力更好**
- **缺点**:
    - 实现复杂, 增加了计算复杂度;

#### SHAW
> SHAW 是作者名
- **思路**: 去掉 $p_i W_Q$, 并将 $p_j W_K$ 替换为相对位置向量 $R^K_{i,j}$, 其中 $R^K_{i,j}$ 依赖于相对距离 $i-j$: 
    $$
    q_i k_j^\top 
        = x_i W_Q \left( x_j W_K + R^K_{i,j} \right)^\top
    $$

#### XLNet
- **思路**: 将 $p_j$ 替换为相对位置向量 $R_{i-j}$, 将 $p_i$ 替换为可训练向量 $u, v$, 其中 $R_{i-j}$ 用正弦位置编码生成:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        + x_i W_Q W_{K,R}^\top R_{i-j}^\top
        + u\, W_K^\top x_j^\top
        + v\, W_{K,R}^\top R_{i-j}^\top
    $$

#### T5
- **思路**: 去掉 **2, 3** 两项, 并用可训练偏置 $\beta_{i,j}$ 代替 **4** 项, 其中 $\beta_{i,j}$ 通过 **分桶** 映射相对距离后查表得到:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        + \beta_{i,j}
    $$

#### DeBERTa
- **思路**: 去掉 **4** 项, 保留 **2, 3** 两项, 并替换为相对位置向量, 其中 $R_{i,j}$ 依赖相对距离:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        + x_i W_Q W_K^\top R_{i,j}^\top
        + R_{j,i} W_Q W_K^\top x_j^\top
    $$

#### ALiBi
- **思路**: 去掉 **2, 3** 两项, 并用 **固定线性偏置** 代替 **4** 项, 其中 $m$ 为第 $h$ 个注意力头的常数斜率:
    $$
    q_i k_j^\top
        = x_i W_Q W_K^\top x_j^\top
        - m · (i − j)
    $$


## 旋转位置编码
> RoPE, Rotary Positional Encoding
- **来源**: RoFormer
- **思路/原理**: 通过 **旋转矩阵 (rotation matrix)** 对 Q/K 向量进行变换, 使内积结果 **仅依赖于相对位置**;
- **优点**: 
    - 优秀的 **外推性**;
    - 具备绝对位置的形式, 产生相对位置的效果 (从绝对位置推导出相对关系);
        > 向量旋转的角度是绝对的 (与 token 的 pos 相关), 向量的点积是相对的 (只与 toekn 间的相对距离有关)
- **代码演示**:
    > [test_rope.py](./code/test_rope.py)
    - **相对性验证**: $<\text{RoPE}(q, m), \text{RoPE}(k, n)> = <q, \text{RoPE}(k, m - n)>$
        > $\text{RoPE}(vec, pos)$: 对位于 $pos$ 的向量 $vec$ 旋转 $pos \cdot \theta$ 度; 多维情况需两两分组后进行旋转 (详见代码);
        ```txt
        === RoPE 相对性验证 ===
        m    n    m-n    dot_val     ro_dot_val
        ----------------------------------------
        0    0    0      -4.081900   -4.081900
        4    4    0      -4.081900   -4.081900
        10   0    10     -2.769302   -3.336345
        15   5    10     -2.769302   -3.336345
        18   8    10     -2.769302   -3.336345
        6    16   -10    -3.336345   -2.769302
        16   26   -10    -3.336345   -2.769302
        3    13   -10    -3.336345   -2.769302
        ```
        - **直观解释**: 对于两个经过 RoPE 旋转后的向量 $q_m$ (位于 $m$) 和 $k_n$ (位于 $n$), 它们的点积结果只与它们的相对位置 $m-n$ 有关, 与绝对位置无关;
    - **计算步骤**
        ```txt
        === RoPE 计算步骤演示 ===
        步骤 1: 向量两两分组
        [[0.497, -0.138], [0.648, 1.523], [-0.234, -0.234]] ...

        步骤 2: 计算各组的旋转角度, 数量为 D/2
        [3.0, 0.3, 0.03] ...

        步骤 3: 分组应用旋转
        原始向量 (分组): [[0.497, -0.138], [0.648, 1.523], [-0.234, -0.234]] ...
        旋转后向量 (分组): [[-0.472, 0.207], [0.169, 1.646], [-0.227, -0.241]] ...
        旋转后向量 (展开): [-0.472, 0.207, 0.169, 1.646, -0.227, -0.241] ...

        范数比较
        原始范数: 2.48947373
        旋转后范数: 2.48947373
        ```
        - **旋转角度**: 因 **两两分组**, 对第 $m$ 和 $m+1$ 个 token, 其在第 $i$ 个维度 ($i \in {1,2,..,d/2}$) 上的旋转角度为:
            $$
            \theta_i = m \cdot \dfrac{1}{10000^{2(i-1)/d}}
            $$
            - **为什么这么设计 $\theta$?** —— **远程衰减性**, 即内积随相对距离增大而衰减, 这与 **距离越远, 依赖越弱** 的直觉一致;

## 参考资料
- [设计位置编码 - HuggingFace Blog](https://huggingface.co/blog/zh/designing-positional-encoding)
- [让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces](https://kexue.fm/archives/8130)
- [十分钟读懂旋转编码（RoPE） - 知乎](https://zhuanlan.zhihu.com/p/647109286)
    - [再论大模型位置编码及其外推性（万字长文） - 知乎](https://zhuanlan.zhihu.com/p/675243992)