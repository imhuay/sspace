位置编码
===
<!--START_SECTION:badge-->

![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-09&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-10%2014%3A09%3A29&label_color=gray&color=thistle&style=flat-square)

<!--END_SECTION:badge-->
<!--info
date: 2025-09-09 11:03:29
top: false
draft: true
hidden: true
level: 0
tag: [dl_transformer]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [Transformer](./README.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [核心概念](#核心概念)
- [绝对位置编码](#绝对位置编码)
    - [正余弦位置编码](#正余弦位置编码)
    - [可学习位置编码](#可学习位置编码)
- [相对位置编码](#相对位置编码)
- [旋转位置编码](#旋转位置编码)
- [参考资料](#参考资料)
<!--END_SECTION:toc-->

---

## 核心概念

- **动机**: 
    - 自注意力的本质是 **加权求和**, 在数学上是一种 **集合** 操作, 具有 **置换等变性 (Permutation Equivariant)**; 
    - 这意味着 **即使打乱输入序列的顺序, 注意力机制也会产生相同的分布**;
        > [测试置换等变性.py](./code/test_permutation.py)
    - 为了使其能够理解序列的 **顺序** 概念, 必须显式引入 **位置信息**;
- **方法** (如何引入位置信息)
    1. 在输入模型前, 对词嵌入施加位置编码 (如 **正余弦位置编码**);
    2. 在计算注意力前, 对 Q/K 施加变换 (如 **RoPE**);
    3. 在得到注意力后, 对 logits 施加偏置 (如 **ALiBi**);
- **理想属性**
    - **唯一性**: 每个位置的编码唯一 (所有位于位置 $i$ 的 token 有相同的编码)
    - **确定性**: 由确定性公式或算法生成, 便于模型学习/泛化;
    - **外推性 (Extrapolation)**: 能泛化到比训练序列更长的输入;
    - **相对位置感知**: 两个位置编码之间应该蕴含 **相对位置** 信息, 并具有简单的数学关系 (最好是线性的);
    - **扩展性**: 能自然扩展到多维数据 (图像, 视频);


## 绝对位置编码

### 正余弦位置编码

<!-- ✅❌⭕❓✔️☑️⚠️🔄➡️↔️📌📍🔖🏷️💡📝 -->
- **来源/使用模型**: Transformer 原版
- **公式**:
    - $PE_{(pos, 2i)} = \sin(\dfrac{pos}{10000^{2i/d_{model}}})$
    - $PE_{(pos, 2i+1)} = \cos(\dfrac{pos}{10000^{2i/d_{model}}})$
    - 其中 $pos$ 是 token 的位置索引, $i$ 是位置编码向量的分量索引;
- **优点**:
    - ⭕唯一性
    - ⭕确定性
    - 🔄外推性: 正弦余弦函数具有周期性, 有一定外推能力;
    - ⭕相对位置信息: 对于某个固定的偏移量 $k$, $PE(pos+k)$ 可以表示为 $PE(pos)$ 的线性函数; 这意味着模型可以很容易地学会关注相对位置信息;

### 可学习位置编码

- **来源/使用模型**: BERT 等
- **方法**: 随机初始化一个大小为 `(max_seq_len, d_model)` 的矩阵作为位置编码参数, 然后与模型的其他参数一起通过训练学习得到;
- **优点**:
    - 灵活, 可能学到更适合特定任务的位置模式;
- **缺点**:
    - 严格的内插, 无法处理比训练时更长的序列;
    - 增加了参数量;

## 相对位置编码


## 旋转位置编码

- **传统方法的主要缺点**: **外推性差**
    > **外推性**: 在训练长度之外的序列上也能有效工作;
- **代码演示**:
    > [test_rope.py](./code/test_rope.py)
    - **相对性验证**
        ```txt
        === RoPE 相对性验证 ===
        m    n    m-n    dot_val     ro_dot_val
        ----------------------------------------
        0    0    0      -4.081900   -4.081900
        4    4    0      -4.081900   -4.081900
        10   0    10     -2.769302   -3.336345
        15   5    10     -2.769302   -3.336345
        18   8    10     -2.769302   -3.336345
        6    16   -10    -3.336345   -2.769302
        16   26   -10    -3.336345   -2.769302
        3    13   -10    -3.336345   -2.769302
        ```
        - 对于两个经过 RoPE 编码的向量 $q_m$ (位于 $m$) 和 $k_n$ (位于 $n$), 它们的点积结果只与相对位置 $m-n$ 有关, 而与绝对位置 $m$ 和 $n$ 无关;
    - **计算步骤**
        ```txt
        === RoPE 计算步骤演示 ===
        步骤 1: 向量两两分组
        [[0.497, -0.138], [0.648, 1.523], [-0.234, -0.234]] ...

        步骤 2: 计算各组的旋转角度, 数量为 D/2
        [3.0, 0.3, 0.03] ...

        步骤 3: 分组应用旋转
        原始向量 (分组): [[0.497, -0.138], [0.648, 1.523], [-0.234, -0.234]] ...
        旋转后向量 (分组): [[-0.472, 0.207], [0.169, 1.646], [-0.227, -0.241]] ...
        旋转后向量 (展开): [-0.472, 0.207, 0.169, 1.646, -0.227, -0.241] ...

        范数比较
        原始范数: 2.48947373
        旋转后范数: 2.48947373
        ```
        - **旋转角度**: 因 **两两分组**, 对第 $m$ 和 $m+1$ 个 token, 其在第 $i$ 个维度 ($i \in {1,2,..,d/2}$) 上的旋转角度为:
            $$
            \theta_i = m \cdot \dfrac{1}{10000^{2(i-1)/d}}
            $$
            - **为什么这么设计 $\theta$?** —— **远程衰减性**, 即内积随相对距离增大而衰减, 这与 **距离越远, 依赖越弱** 的直觉一致;

## 参考资料
- [设计位置编码 - HuggingFace Blog](https://huggingface.co/blog/zh/designing-positional-encoding)
- [十分钟读懂旋转编码（RoPE） - 知乎](https://zhuanlan.zhihu.com/p/647109286)
    - [再论大模型位置编码及其外推性（万字长文） - 知乎](https://zhuanlan.zhihu.com/p/675243992)