Transformer 面试问题整理
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-06&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-12%2000%3A14%3A55&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-06 13:48:27
top: false
draft: false
hidden: true
section_number: true
level: 99
tag: [dl_transformer]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [Transformer](./README.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [1. **模型框架**](#1-模型框架)
    - [1.1. ✅ 一句话解释 Transformer 的核心思想](#11--一句话解释-transformer-的核心思想)
        - [1.1.1. 💡 Transformer 的归纳偏置是什么? 与 CNN/RNN 有何不同?](#111--transformer-的归纳偏置是什么-与-cnnrnn-有何不同)
        - [1.1.2. ✅ 为什么 Transformer 比 RNN/LSTM 更好](#112--为什么-transformer-比-rnnlstm-更好)
    - [1.2. ✅ 简述 Transformer 中 Encoder 和 Decoder 各自的作用和结构](#12--简述-transformer-中-encoder-和-decoder-各自的作用和结构)
        - [1.2.1. ✅ 为什么大多数通用大模型选择 Decoder-Only (CausalLM) 架构?](#121--为什么大多数通用大模型选择-decoder-only-causallm-架构)
    - [1.3. ✅ 说明自注意力机制的计算过程](#13--说明自注意力机制的计算过程)
        - [1.3.1. ✅ 为什么要对 QK 的点积进行缩放?](#131--为什么要对-qk-的点积进行缩放)
        - [1.3.2. ✅ 多头注意力中 "多头" 的动机是什么, 是如何实现的?](#132--多头注意力中-多头-的动机是什么-是如何实现的)
        - [1.3.3. ✅ 为什么 Decoder 中计算自注意力需要 "掩码"?](#133--为什么-decoder-中计算自注意力需要-掩码)
    - [1.4. ✅ Cross Attention 中的 Q, K, V 分别来自哪里?](#14--cross-attention-中的-q-k-v-分别来自哪里)
- [2. **位置编码**](#2-位置编码)
    - [2.1. ✅ **为什么需要位置编码?** (位置编码的必要性, 自注意力的缺陷)](#21--为什么需要位置编码-位置编码的必要性-自注意力的缺陷)
        - [2.1.1. ✅ **什么是"置换不变性"?** (自注意力为何对置换不敏感?)](#211--什么是置换不变性-自注意力为何对置换不敏感)
        - [2.1.2. ✅ **位置编码是如何引入到模型中的?** (常见的引入方式)](#212--位置编码是如何引入到模型中的-常见的引入方式)
        - [2.1.3. ✅ 多头注意力中, 位置参数是共享的吗?](#213--多头注意力中-位置参数是共享的吗)
    - [2.2. 📌 介绍常见的位置编码](#22--介绍常见的位置编码)
        - [2.2.1. ⬆️ 除了正弦位置编码, 还有哪些位置编码方式?](#221-️-除了正弦位置编码-还有哪些位置编码方式)
    - [2.3. 🏷️ 绝对位置编码](#23-️-绝对位置编码)
        - [2.3.1. ✅ 介绍常见的绝对位置编码 (正弦位置编码, 可学习位置编码)](#231--介绍常见的绝对位置编码-正弦位置编码-可学习位置编码)
        - [2.3.2. ✅ 正弦位置编码有哪些优点?](#232--正弦位置编码有哪些优点)
            - [2.3.2.1. ✅ 解释为何正弦位置编码蕴含了相对位置信息? (给出三角恒等式推导)](#2321--解释为何正弦位置编码蕴含了相对位置信息-给出三角恒等式推导)
            - [2.3.2.2. ✅ 正弦位置编码的 "波长" 是什么意思? 不同维度对应的波长有何不同?](#2322--正弦位置编码的-波长-是什么意思-不同维度对应的波长有何不同)
            - [2.3.2.3. ✅ 为什么正弦位置编码使用 **加法** 引入而不是 **拼接**?](#2323--为什么正弦位置编码使用-加法-引入而不是-拼接)
        - [2.3.3. ✅ **可学习位置编码** 有哪些应用场景?](#233--可学习位置编码-有哪些应用场景)
    - [2.4. 🏷️ 相对位置编码](#24-️-相对位置编码)
        - [2.4.1. ✅ 什么是相对位置编码? 与绝对位置编码的核心区别是什么?](#241--什么是相对位置编码-与绝对位置编码的核心区别是什么)
            - [2.4.1.1. ⬆️ 位置编码中 **相对** 与 **绝对** 的含义](#2411-️-位置编码中-相对-与-绝对-的含义)
        - [2.4.2. ✅ 比较 **绝对位置编码** 与 **相对位置编码**](#242--比较-绝对位置编码-与-相对位置编码)
            - [2.4.2.1. ⬆️ **相对位置编码** 对比 **绝对位置编码** 有哪些优势?](#2421-️-相对位置编码-对比-绝对位置编码-有哪些优势)
        - [2.4.3. ✅ 位置编码中的 "外推" 问题指的是什么?](#243--位置编码中的-外推-问题指的是什么)
            - [2.4.3.1. ⚠️ 如何评估 "外推失败"? (诊断图, 对照实验)](#2431-️-如何评估-外推失败-诊断图-对照实验)
        - [2.4.4. ⚠️ ALiBi 的线性偏置如何确保长距离外推?](#244-️-alibi-的线性偏置如何确保长距离外推)
            - [2.4.4.1. ⚠️ 斜率 per-head 设计的直觉是什么? 为什么头越多可覆盖更广距离带?](#2441-️-斜率-per-head-设计的直觉是什么-为什么头越多可覆盖更广距离带)
            - [2.4.4.2. ⚠️ 为什么 ALiBi 在超长上下文中更稳定? 它的潜在代价是什么?](#2442-️-为什么-alibi-在超长上下文中更稳定-它的潜在代价是什么)
        - [2.4.5. ❓ 相对位置偏置 (Shaw/T5) 如何控制参数规模?](#245--相对位置偏置-shawt5-如何控制参数规模)
    - [2.5. 🏷️ **旋转位置编码 (RoPE)**](#25-️-旋转位置编码-rope)
        - [2.5.1. ✅ 介绍 **旋转位置编码** (动机/做法/优势)](#251--介绍-旋转位置编码-动机做法优势)
        - [2.5.2. ✅ RoPE 为什么能把绝对位置转成相对位移不变的注意力? (写出关键等式)](#252--rope-为什么能把绝对位置转成相对位移不变的注意力-写出关键等式)
        - [2.5.3. ✅ RoPE 中的 **底数** (`d`) 或 **频率基** (`θ`) 是如何影响位置编码效果的?](#253--rope-中的-底数-d-或-频率基-θ-是如何影响位置编码效果的)
            - [2.5.3.1. ⬆️ 什么是 **相位绕回**/**相位失真**?](#2531-️-什么是-相位绕回相位失真)
            - [2.5.3.2. ⬆️ 底数 `d` **太大/太小** (频率基 `θ` 太小/太大) 会发生什么?](#2532-️-底数-d-太大太小-频率基-θ-太小太大-会发生什么)
            - [2.5.3.3. ⬆️ 为什么 RoPE 默认设置 `d = 10000`?](#2533-️-为什么-rope-默认设置-d--10000)
            - [2.5.3.4. ⬆️ 为什么 **长上下文模型** 会设置一个更大的底数 (`d`)?](#2534-️-为什么-长上下文模型-会设置一个更大的底数-d)
            - [2.5.3.5. ⬆️ 旋转角公式中加入 `i` 的作用是什么?](#2535-️-旋转角公式中加入-i-的作用是什么)
        - [2.5.4. ⚠️ 列举几种提升 RoPE 长度外推稳定性的策略, 并说明各自副作用](#254-️-列举几种提升-rope-长度外推稳定性的策略-并说明各自副作用)
    - [2.6. ✅ 该如何选择不同的位置编码? (嵌入加法 / 注意力偏置 / 旋转编码)](#26--该如何选择不同的位置编码-嵌入加法--注意力偏置--旋转编码)
        - [2.6.1. ⚠️ 在 GPU 显存占用与推理时延上分别有什么差异?](#261-️-在-gpu-显存占用与推理时延上分别有什么差异)
        - [2.6.2. ⚠️ 如何将已有绝对位置模型迁移到 RoPE? 或者 ALiBi?](#262-️-如何将已有绝对位置模型迁移到-rope-或者-alibi)
    - [2.7. 💡 在视觉Transformer (ViT) 中, 是如何处理位置编码的?](#27--在视觉transformer-vit-中-是如何处理位置编码的)
        - [2.7.1. ❓ 2D 位置信息与 1D 文本位置信息有何不同? 如何为 2D 网格设计位置编码?](#271--2d-位置信息与-1d-文本位置信息有何不同-如何为-2d-网格设计位置编码)
- [3. **训练与推理**](#3-训练与推理)
    - [3.1. ✅ 说明 Decoder 在训练与推理阶段的差异](#31--说明-decoder-在训练与推理阶段的差异)
        - [3.1.1. 推理阶段, 怎么优化随着输出序列越来越长带来的开销?](#311-推理阶段-怎么优化随着输出序列越来越长带来的开销)
        - [3.1.2. 描述 KV Cache 的动机, 方法, 效果](#312-描述-kv-cache-的动机-方法-效果)
        - [3.1.3. 解释 "曝光偏差", 怎么引起的, 怎么缓解?](#313-解释-曝光偏差-怎么引起的-怎么缓解)
    - [3.2. 介绍常见的序列生成策略](#32-介绍常见的序列生成策略)
        - [3.2.1. 对比 BeamSearch 和 贪心搜索 的优劣](#321-对比-beamsearch-和-贪心搜索-的优劣)
        - [3.2.2. 为什么 LLM 在文本创作中倾向于使用 Sampling, 而不是 BeamSearch?](#322-为什么-llm-在文本创作中倾向于使用-sampling-而不是-beamsearch)
    - [3.3. 如何控制生成序列的长度和终止?](#33-如何控制生成序列的长度和终止)
    - [3.4. 怎么抑制 LLM 生成过程中的 重复问题?](#34-怎么抑制-llm-生成过程中的-重复问题)
- [4. **拓展问题**](#4-拓展问题)
    - [4.1. 非自回归模型是如何解码的? 与自回归解码的优劣](#41-非自回归模型是如何解码的-与自回归解码的优劣)
<!--END_SECTION:toc-->

---

## 1. **模型框架**

### 1.1. ✅ 一句话解释 Transformer 的核心思想
> **思路**: Transformer 的核心 → Attention → 全局动态建模序列中各位置之间的依赖关系

#### 1.1.1. 💡 Transformer 的归纳偏置是什么? 与 CNN/RNN 有何不同?
> 思路: **什么是归纳偏置** → **Transformer (位置编码 + 全局依赖)** / **CNN (局部性 + 平移不变性)** / **RNN (顺序性 + 马尔可夫假设)**

<details><summary><b>详述</b></summary>

- **什么是归纳偏置**
    - 在机器学习中, 归纳偏置是指模型在学习之前**对数据分布或任务结构的先验假设**;
- **Transformer**
    - **最小结构假设**: 除位置编码, 无强结构先验;
    - **全局依赖**: 依赖自注意力机制学习任意位置间的关系;
- **差异**:
    - CNN/RNN: 有较强的结构先验 (局部性 或 顺序性);
        - **优点**: 数据量不大也能学到一定模式
        - **缺点**: 强先验限制了表达能力
    - Transformer: 弱先验, 几乎不假设输入的内在结构 (位置关系通过显式编码输入);
        - **优点**: 灵活, 可以学习更丰富的模式
        - **缺点**: 需要更多数据和计算

</details>

#### 1.1.2. ✅ 为什么 Transformer 比 RNN/LSTM 更好
> **思路**: Transformer 的优势: 1) 长程依赖/全局交互, 2) 并行计算/训练速度

### 1.2. ✅ 简述 Transformer 中 Encoder 和 Decoder 各自的作用和结构
> **Encoder**: (文本表示, 自注意力 → FFN);  
> **Decoder**: (自回归, 掩码自注意力 → 交叉注意力 → FFN)

<details><summary><b>详述</b></summary>

- **Encoder**:
    - **作用**: 对输入序列编码, 将其表示为 **富含上下文信息的隐状态序列**;  
    - **结构**: $N$ 个相同的层堆叠结构, 每个层包含 2 个子层:  
        1. **多头自注意力** → **残差** → **层归一化**;
        2. **前馈网络** → **残差** → **层归一化**;
    - **输入**: Token 嵌入 + 位置编码;
    - **输出**: 上下文表示序列 (维度同输入);
- **Decoder**:
    - **作用**: 以**自回归**方式, 根据 Encoder 输出和已生成前缀, **逐词**生成目标序列;
    - **结构**: $N$ 个相同的层堆叠结构, 每个层包含 3 个子层:  
        1. **掩码多头自注意力** → **残差** → **层归一化**;
        2. **交叉注意力** → **残差** → **层归一化**;
        3. **前馈网络** → **残差** → **层归一化**;
    - **输入**: 目标序列右移一位的嵌入 + 位置编码 + Encoder 输出;
    - **输出**: 对下一个 token 的概率分布;

</details>


#### 1.2.1. ✅ 为什么大多数通用大模型选择 Decoder-Only (CausalLM) 架构?
> 思路: **LLM 的核心能力** 是自回归生成, 与 Decoder 的的工作模式相匹配;

<details><summary><b>详述</b></summary>

<!-- - **开场白**: Decoder-Only 相较于 Encoder-Decoder 的优势主要来源于现实中的实践 -->
- **任务匹配**
    - LLM 的核心能力是 **"给定上下文, 预测下一个 token"**, 这与 Decoder 的工作模式匹配;
    - Encoder-Decoder 架构是为 **Seq2Seq** 任务设计的 —— **先对输入进行编码, 再解码到输出** —— 对于单纯的生成任务, Encoder 部分可能并非必要, 实践中这种更复杂的架构也没有表现出优势;
- **效率优势**:
    - **参数效率**
        - Decoder-Only 中所有参数专注于核心任务; Encoder-Decoder 中参数分散在编码和解码两部分;
        - **在给定参数量预算下**, 将所有参数都投入到 Decoder 的上限更高 —— 更符合 **Scaling Laws**;
        - 在海量数据上训练后, Decoder-Only 模型展现出强大的 **涌现能力**; 在零样本泛化上优于 Encoder-Decoder;
            > **Causal Decoder** 严格遵守从左到右, 只看历史, 不看未来 (包括 Prompt 部分)
    - **训练效率**
        - **Decoder-Only 的训练目标只有一个**: Next Token 预测;
        - Encoder-Decoder 往往是**多任务联合训练**, 更容易出现训练不稳定的情况, 需要平衡各任务的 Loss;
    - **工程优势**
        - 所有主流大模型 (GPT, LLaMA等) 都采用此架构, 整个软硬件生态都针对其进行了极度优化;
- **参考资料**
    - [解码器仅架构: 探究大语言模型 (LLM) 采用Decoder-only架构的原因-百度开发者中心](https://developer.baidu.com/article/detail.html?id=2145079)
    - [为什么当前的大型语言模型 (LLMs) 普遍采用 "仅解码器" (Decoder-only) 架构? _decoder-only自回归模型架构-CSDN博客](https://blog.csdn.net/Listennnn/article/details/147934482)
    - [面试官问我: 大模型为何都用 Decoder only 架构? _大模型为什么是基于decoder-CSDN博客](https://blog.csdn.net/2401_84033492/article/details/143260251)

</details>


### 1.3. ✅ 说明自注意力机制的计算过程
> Q/K/V 投影 → 计算注意力分数 → 缩放与归一化 → 加权求和
>> $Q, K, V = XW^Q, XW^K, XW^V → QK^\top → \text{softmax}(\frac{QK^\top}{\sqrt{d_k}}) → \text{softmax}(\frac{QK^\top}{\sqrt{d_k}})V$

#### 1.3.1. ✅ 为什么要对 QK 的点积进行缩放?
> 防止点击 ($QK^\top$) 的数值过大引发梯度消失; 缩放因子是 $\sqrt{d_k}$ (其中 $d_k$ 为输入向量 $K$ 的维度)
>> **数学解释**: **两个均值为 0、方差为 1 的 d 维向量, 其点积的均值为 0、方差为 d**; 直接 softmax 会出现数值极小的分量, 反向传播时这些分量的梯度会趋于零, 导致梯度消失;

#### 1.3.2. ✅ 多头注意力中 "多头" 的动机是什么, 是如何实现的?
> **动机**: 将特征空间切分成多个独立的低维子空间 → 学习不同的注意力分布/不同的依赖关系;  
> **实现**: 将 Q/K/V 投影到多个低维子空间 → 每个头独立执行 Attention → 将结果拼接后再整体投影;
>> 实际并不会真的独立执行多个 Attention, 而是利用 **张量操作和广播机制** 一次完成;

<details><summary><b>代码演示</b></summary>

```python
def attn(self, x, mask):
    """
    x: [B, L, d_model]
    mask: [B, 1, 1, L]  -  Padding Mask
       or [B, 1, L, L]  -  Causal Mask
    """
    # 1. 线性映射到 Q, K, V
    #    [B, L, d_model]
    Q, K, V = self.W_Q(x), self.W_K(x), self.W_V(x)
    d_k = K.size(-1) // self.num_head  # 每个头的维度: d_model // H
    # 2. 重排为多头形式:
    #    [B, L, H*d_k] → [B, H, L, d_k]
    Q = einops.rearrange(Q, 'B L (H d) -> B H L d', H=self.num_head)
    K = einops.rearrange(K, 'B L (H d) -> B H L d', H=self.num_head)
    V = einops.rearrange(V, 'B L (H d) -> B H L d', H=self.num_head)
    # 3. 计算注意力权重 (scale → mask → softmax):
    #    [B, H, L, d_k] @ [B, H, d_k, L] → [B, H, L, L]
    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)
    A = torch.softmax(scores + mask, dim=-1)
    # 4. 合并多头 → 投影
    #    [B, H, L, d_k] → [B, L, H*d_k] = [B, L, d_model]
    O = einops.rearrange(A @ V, 'B H L d -> B L (H d)')
    O = self.W_O(O)
    return O
```

</details>

#### 1.3.3. ✅ 为什么 Decoder 中计算自注意力需要 "掩码"?
> 维持自回归特性, 防止数据泄露

<details><summary><b>详述</b></summary>

- **核心目的: 维持自回归特性, 防止数据泄露**;
    - Decoder 的任务是 **自回归生成 (auto-regressive generation)**, 即逐个预测下一个 token;
    - 在生成第 `t` 个 token 时, 模型只能依据 **已经生成的 `1` 到 `t-1` 个 token**;
    - 若不加掩码, 模型在训练时会在计算第 `t` 个位置的注意力时 **"看到" 整个目标序列** (包括未来的 `t+1, t+2, ...` token), 这相当于 **数据泄露 (data leakage)**;
    - 掩码通过遮蔽 (设为负无穷) 当前位置之后的所有未来 token, 确保注意力权重仅基于历史信息, 从而 **强制训练与推理的行为保持一致**;
- **实现方式: 前瞻掩码 (Look-ahead Mask)**;
    - 掩码通常是一个 **下三角矩阵 (lower triangular matrix)**, 其对角线及左侧元素为 `0` (允许参与计算), 右上角元素为 `-inf` (被遮蔽);
    - 经过 softmax 后, 被遮蔽位置的权重变为 `0`, 从而在计算加权和时忽略这些未来信息;
- **一句话总结**: 掩码通过遮蔽未来信息, 确保 Decoder 在训练时只能基于历史上下文进行预测, 从而模拟推理时的自回归生成过程, 防止作弊;

</details>

### 1.4. ✅ Cross Attention 中的 Q, K, V 分别来自哪里?
> Q 来自 Decoder 上一层的输出; K, V 来自 Encoder 最后一层的输出;
>> **作用**: Decoder 在生成当前 token 时, 根据 **Cross-Atternion** 获取与当前生成最相关的源序列信息;

## 2. **位置编码**

### 2.1. ✅ **为什么需要位置编码?** (位置编码的必要性, 自注意力的缺陷)
> **思路**: 自注意力机制具有 **"置换不变性"** → 需要位置编码提供的**顺序信息**;

<details><summary><b>详述</b></summary>

- **核心原因**
    - 自注意力机制的核心/本质是 **加权求和**, 在数学上不依赖输入顺序 → **置换不变性**;
- **解决方法**
    - 通过显式加入 **位置编码** 为模型提供 **顺序感知** 能力;

</details>

#### 2.1.1. ✅ **什么是"置换不变性"?** (自注意力为何对置换不敏感?)
> 打乱序列的输入顺序, 其自注意力计算的加权结果不变

#### 2.1.2. ✅ **位置编码是如何引入到模型中的?** (常见的引入方式)
1. 在输入模型前, 对词嵌入施加位置编码 (如 **正弦位置编码**);
    > 一般是生成一个与 token 维度相同的向量, 进行逐位相加
2. 在计算注意力前, 对 Q/K 施加变换 (如 **RoPE**);
3. 在得到注意力后, 对 logits 施加偏置 (如 **ALiBi**);

#### 2.1.3. ✅ 多头注意力中, 位置参数是共享的吗?
> 默认 Transformer 中位置参数是跨头共享的, 以保证位置一致性 (参考系一致), 提升参数效率 (减少过拟合); 但会 **牺牲不同头在位置感知上的多样性与多尺度能力**; 因此在需要让 **不同头专注不同距离模式** 时, 也会采用分头位置参数的设计 (如 Swin Transformer);

### 2.2. 📌 介绍常见的位置编码
> 绝对位置编码 (正弦位置编码, 可学习位置编码), 相对位置编码 (SHAW, XLNet, T5, DeBERTa, ALiBi), 旋转位置编码 (RoPE)
>> [位置编码](./位置编码.md)

#### 2.2.1. ⬆️ 除了正弦位置编码, 还有哪些位置编码方式?

### 2.3. 🏷️ 绝对位置编码

#### 2.3.1. ✅ 介绍常见的绝对位置编码 (正弦位置编码, 可学习位置编码)
> [绝对位置编码](./位置编码.md#绝对位置编码)

#### 2.3.2. ✅ 正弦位置编码有哪些优点?
> 唯一性, 确定性, 外推性, 相对位置
>> [正弦位置编码](./位置编码.md#正弦位置编码)

##### 2.3.2.1. ✅ 解释为何正弦位置编码蕴含了相对位置信息? (给出三角恒等式推导)
> 根据三角函数的性质 (sin/cos 相位差公式), **任意两个位置的编码可以通过相对位移的线性组合互相推导**, 从而在绝对编码中隐含了相对位置信息;

$$
\begin{aligned}
    \sin(\omega_i (p+k)) &= \sin(\omega_i p)\cos(\omega_i k) + \cos(\omega_i p)\sin(\omega_i k) \\
    \cos(\omega_i (p+k)) &= \cos(\omega_i p)\cos(\omega_i k) - \sin(\omega_i p)\sin(\omega_i k)
\end{aligned}
$$

##### 2.3.2.2. ✅ 正弦位置编码的 "波长" 是什么意思? 不同维度对应的波长有何不同?
> 波长 $\lambda$ 由公式中的 $10000^{2i/d_{model}}$ 决定. 随着维度 $i$ 从 $0$ 增加到 $d_{model}/2 - 1$, 波长从 $2π$ (高频, 变化快) 增长到 $10000 * 2π$ (低频, 变化慢); **这使模型能同时捕获近距离和远距离的位置关系**.

##### 2.3.2.3. ✅ 为什么正弦位置编码使用 **加法** 引入而不是 **拼接**?
> 简洁, 直观, 计算与参数效率; 同一表示空间融合语义与位置; **拼接** 是 **加法** 的一种特殊情况;
>> [transformer里PE为什么不采用concatenation的方式? - 知乎](https://www.zhihu.com/question/4717410141)

<details><summary><b>详述</b></summary>

- **实用角度**:
    - 直观, 简洁, 保持维度一致;
    - **计算与参数效率** (加入 PE 多出来的维度意味着更大的隐藏层);
        > ~~不成立, 计算量可以通过隐层维度控制~~, 神经网络设计中, 隐层的维度很少会小于输入层 (降维);
    - **已证实有效性**;
- **表示空间**:
    - **加法** 将位置向量视为对词向量的 "偏移", 直接在同一表示空间中融合语义与位置信息;
    - **拼接** 把语义和位置分成两个 **子空间**, 模型需要额外学习如何跨空间交互;
    - 在高维嵌入空间中, 词嵌入子空间与位置编码子空间往往近似 **正交**;
        - 直接加法, 模型也能通过线性变换分别处理语义和位置成分, **不必依赖拼接来保证正交**;
- **表达能力**:
    - 加法的 **表达能力更强**, **拼接** 只是 **加法** 的一种特殊情况;
    - 考虑以下情况:
        - 使用 **拼接** 方法, 设置词嵌入部分的维度为 $d_w$, 位置向量的维度为 $d_p$;
        - 设置 **加法** 的维度为 $d_w + d_p$,
        - 假设 **拼接** 方法更好, 那么模型可以通过学习, 使 **加法** 中词嵌入后 $d_p$ 维的分量为 0, 位置向量前 $d_w$ 维的分量为 0;
        - 从而模拟 **拼接** 的效果, 或者说 **拼接** 只是 **加法** 的一种特殊情况;

</details>

#### 2.3.3. ✅ **可学习位置编码** 有哪些应用场景?
> 序列长度固定或可控; 数据分布特殊, 位置模式非均匀, 非规则结构 (视觉, 多模态)

- 可学习位置编码的优势在于它能 **拟合特定数据分布、适配固定或可控长度、跨模态对齐非规则结构、自动调节位置权重, 并在迁移中快速适应新任务** —— 这些都是固定公式型编码难以做到的;


### 2.4. 🏷️ 相对位置编码

#### 2.4.1. ✅ 什么是相对位置编码? 与绝对位置编码的核心区别是什么?

- **相对位置编码** 是一种向模型中注入位置信息的方法;
    - 其核心思想是: 模型对 token 间语义关系的理解应更侧重于它们之间的 相对距离, 而非其在序列中的绝对位置;
- **核心区别**:
    - 编码所表达的 "位置信息" 是基于 **全局坐标** 还是 **相对距离** 来定义的;

##### 2.4.1.1. ⬆️ 位置编码中 **相对** 与 **绝对** 的含义
> 在位置编码中, **绝对** 与 **相对** 指的是编码所表达的 "位置信息" 是基于 **全局坐标** 还是 **相对距离** 来定义的;

#### 2.4.2. ✅ 比较 **绝对位置编码** 与 **相对位置编码**
- **绝对位置编码**
    - **定义**: 每个位置 $p$ 都有一个唯一的编码, 直接表示它在序列中的 **全局位置**;
    - **特点**:
        - 编码与位置一一对应, 位置 $i$ 的编码在任何序列中都是相同的;
        - 注意力机制需要通过计算两个绝对编码的差异, 间接推断相对距离;
    - **优点**: 实现简单, 可直接生成任意位置的编码;
    - **缺点**: 对超出训练长度的序列泛化能力依赖编码设计;
    - **常见实现**:
        - 正弦位置编码;
        - 可学习位置向量;
- **相对位置编码**
    - **定义**: 编码直接表示两个位置之间的相对距离 $r=j-i$, 而不是它们的全局坐标;
    - **特点**:
        - 注意力分数中直接引入与距离相关的偏置或变换;
        - 模型天然对平移不敏感 (shift-invariant), 即整体平移序列不会改变相对关系;
    - **优点**: 更容易泛化到不同长度, 尤其是长序列;
    - **缺点**:
        - 实现复杂, 增加了计算复杂度;
        - 需要额外设计来控制参数规模;

##### 2.4.2.1. ⬆️ **相对位置编码** 对比 **绝对位置编码** 有哪些优势?

#### 2.4.3. ✅ 位置编码中的 "外推" 问题指的是什么?
> 训练好的模型能否在 **推理** 时有效处理 **比训练阶段见过更长的输入**

##### 2.4.3.1. ⚠️ 如何评估 "外推失败"? (诊断图, 对照实验)
> 当模型在长于其训练长度的序列上, 其性能指标显著差于在训练长度范围内的性能时, 即为外推失败; 诊断图 (困惑度随位置曲线, 注意力距离分布热图); 对照实验 (长度扫描曲线, 位置策略 A/B 测试)

#### 2.4.4. ⚠️ ALiBi 的线性偏置如何确保长距离外推?
> ALiBi 通过在注意力 logits 中加入 **与相对距离成比例的固定负偏置**, 消除了绝对位置依赖和周期性失真, 使得 **推理时的长距离计算与训练时完全同分布**, 从而天然支持任意长度的外推;

##### 2.4.4.1. ⚠️ 斜率 per-head 设计的直觉是什么? 为什么头越多可覆盖更广距离带?
> ALiBi 的 per‑head 斜率设计是为了 **让不同注意力头在距离衰减上形成多尺度互补**, 头越多, 斜率分布越密、覆盖范围越广, 长短距依赖都能覆盖;

##### 2.4.4.2. ⚠️ 为什么 ALiBi 在超长上下文中更稳定? 它的潜在代价是什么?
> ALiBi 的线性相对偏置不会随长度漂移, 也无相位绕回; 但代价是持续惩罚远距注意力, 对需要高保真跨长距离依赖的任务来说, 它的归纳偏置过于保守;

#### 2.4.5. ❓ 相对位置偏置 (Shaw/T5) 如何控制参数规模?
> 桶化距离 vs 连续函数参数化的权衡?

### 2.5. 🏷️ **旋转位置编码 (RoPE)**

#### 2.5.1. ✅ 介绍 **旋转位置编码** (动机/做法/优势)
> [旋转位置编码](./位置编码.md#旋转位置编码-)

#### 2.5.2. ✅ RoPE 为什么能把绝对位置转成相对位移不变的注意力? (写出关键等式)
> RoPE 通过一个 **基于绝对位置** 的 **旋转变换**, 在计算注意力分数时, 使绝对位置信息自动抵消, 只留下相对位置信息;

<details><summary><b>关键等式</b></summary>

$$
\langle \text{RoPE}(q, m), \text{RoPE}(k, n) \rangle = \langle q, \text{RoPE}(k, m - n) \rangle
$$

- 第一步: 对 Q/K 分别施加与 **绝对位置** 相关的旋转;
- 第二步: 利用 **旋转矩阵的正交性**, 将两个绝对位置的 **旋转差** 化为单一的 **相对位移旋转**
- **结果**: 注意力分数 (Q·K 内积) 只依赖于 **相对位移** $m-n$, 与绝对位置无关;

</details>

#### 2.5.3. ✅ RoPE 中的 **底数** (`d`) 或 **频率基** (`θ`) 是如何影响位置编码效果的?
> 决定了 RoPE 在不同维度的旋转频率分布; `d` 越大 → `θ` 越小 → **强化短距分辨率但长距失真快**; `d` 越小 → `θ` 越大 → **提升长距稳定性但削弱局部精度**;

<details><summary><b>详述</b></summary>

- **旋转角** 公式:
  $$\omega_i = m \theta_i = \frac{m}{10000^{2i/d}}$$
    - 其中
        - $m$ 为 token 位置索引;
        - $i$ 为 向量 的分量索引;
    <!-- - **底数 $d$ 越大** (**频率基 ($\theta$) 越小**), 波长越长, 意味着需要移动更远的距离/旋转更大的角度 (更大的 m) 才能经过一个完整的周期 (2π) → 更晚出现 **相位绕回**/**相位失真** → **长距离更稳定**; -->
- **底数 $d$ 增大/减小的影响**
    - $d$ **增大** → $\theta$ 减小 (波长变长) → 相邻位置的相位差减小小 → 损害模型捕捉 **局部模式** 的能力 / **增强模型的外推能力**;
    - $d$ **减小** → $\theta$ 变大 (波长变短) → 过早发生 **相位失真**/**相位绕回** → **损害模型的外推能力** / 增强了捕捉 **近距离** 位置变化的能力 / 高频震荡, 难以训练;
- **公式中加入 $i$ 的作用**:
    - **让向量中不同维度的分量负责不同尺度的位置信息**;
    - **低维** ($i$ 小): $θ_i$ 较大 (高频), 波长短; 这些维度对位置变化非常敏感, 旋转很快; 它们负责捕捉局部、精细的位置关系;
    - **高维** ($i$ 大): $θ_i$ 较小 (低频), 波长长; 这些维度旋转缓慢, 负责提供全局、稳定的位置信号;

</details>

##### 2.5.3.1. ⬆️ 什么是 **相位绕回**/**相位失真**?
> 三角函数具有 **周期性**, 当旋转角大于 $2\pi$ 时会出现相位重复, 导致远距离位置不可区分 (**相位失真**);

##### 2.5.3.2. ⬆️ 底数 `d` **太大/太小** (频率基 `θ` 太小/太大) 会发生什么?
> `d` 太大 → 损失捕捉 **局部模式** 的能力; `d` 太小 → 损失长距离 **外推能力**;

##### 2.5.3.3. ⬆️ 为什么 RoPE 默认设置 `d = 10000`?
> 不同维度负责不同尺度

##### 2.5.3.4. ⬆️ 为什么 **长上下文模型** 会设置一个更大的底数 (`d`)?
> 延缓相位失真;

##### 2.5.3.5. ⬆️ 旋转角公式中加入 `i` 的作用是什么?
> 控制位置编码中不同维度的旋转频率, 使同时具备长程和短程的多尺度信息, 提升模型对不同距离依赖的表达能力;

#### 2.5.4. ⚠️ 列举几种提升 RoPE 长度外推稳定性的策略, 并说明各自副作用
> 增大频率基底数 $d$; 位置插值 (Position Interpolation); 频率重标定 (如 NTK-aware / 动态 NTK); 分段频率缩放 (如 YaRN / Hybrid Scaling);

<details><summary><b>详述</b></summary>

| 策略 | 核心思路 | 作用 | 副作用 |
|---|---|---|---|
| 增大频率基底数 $d$ | 降低整体频率 | 稳定长距外推 | 短距分辨率下降 |
| 位置插值 | 线性缩放位置索引 | 延缓相位绕回 | 损害高频信息/局部细节, 整体性能下降 |
| NTK-aware | 差异化缩放频率 (高频少缩, 低频多缩) | 长短距兼顾 | 参数敏感 |
| YaRN | 缩放频率 + 调节注意力分布 (温度缩放) | 平衡长短距 | 实现复杂, 调参成本高 |

</details>

### 2.6. ✅ 该如何选择不同的位置编码? (嵌入加法 / 注意力偏置 / 旋转编码)
> **偏好外推**: RoPE, ALiBi; **偏好相对性/平移不变性**: RoPE, 偏置+可学习; **短序列**: 可学习位置编码

#### 2.6.1. ⚠️ 在 GPU 显存占用与推理时延上分别有什么差异?
> **显存占用**: 嵌入加法 O(Nd); 注意力偏置 O(N^2) 或 分桶 O(N); 旋转编码 O(1);  
> **推理时延**: 注意力偏置 > 旋转编码 > 嵌入加法; 对模型整体影响不大;

#### 2.6.2. ⚠️ 如何将已有绝对位置模型迁移到 RoPE? 或者 ALiBi?
> RoPE: 架构改造 (替换位置编码) → 持续预训练 (低学习率) → 微调 → (如果是 RoPE) 推理时做 **位置插值** 或 频率基重标定 (NTK-aware scaling);

### 2.7. 💡 在视觉Transformer (ViT) 中, 是如何处理位置编码的?
> ViT 将图像切分成一系列的图像块 (Patch), 每个 Patch 被视为一个词元; 然后为每个 Patch 的位置 (通常是2D的) 生成一个可学习的位置编码, 与 Patch 的嵌入向量相加.

#### 2.7.1. ❓ 2D 位置信息与 1D 文本位置信息有何不同? 如何为 2D 网格设计位置编码?

## 3. **训练与推理**

### 3.1. ✅ 说明 Decoder 在训练与推理阶段的差异
> **核心差异**: 对 **目标序列** 的 **可见性** 不同;

<details><summary><b>详述</b></summary>

- **训练阶段**:
    - **模式**: **教师强制 (Teacher Forcing)**
    - **过程**:
        - 将完整的目标序列一次性输入 Decoder,
        - 在计算**第 i 个**位置的输出时, 模型可以看到**第 1 到 i-1 位**的真实标签;
    - **特点**:
        - **并行计算**;
        - 整个目标序列可以同时输入, 通过**掩码**确保**当前位置看不到未来信息**, 一次性计算出所有位置的输出;
    - **缺点**:
        - **曝光偏差** (Exposure Bias)
- **推理阶段**:
    - **模式**: **自回归 (Auto-regressive)**
    - **过程**:
        - 从仅包含一个起始符 `<sos>` 的序列开始,
        - 模型每预测出下一个 token, 就**将该 token 追加到输入序列末尾**, 作为生成下一个 token 的上下文,
        - 直到生成结束符 `<eos>` 或达到最大长度;
    - **缺点**:
        - **串行计算**, 效率低;
    - **优化**:
        - **KV Cache**

</details>

#### 3.1.1. 推理阶段, 怎么优化随着输出序列越来越长带来的开销?
> **方法**: KV Cache; **效果**: $O(n^2) → O(n)$

#### 3.1.2. 描述 KV Cache 的动机, 方法, 效果
> 思路: **动机** (重复计算) → **方法** (缓存历史 K/V, 增量计算) → **效果** (降低计算复杂度)

<details><summary><b>详述</b></summary>

- **背景/动机**
    - 在**自回归**生成中, 第 `i` 个 token 的注意力计算需基于前 `i` 个 token `K/V` (含开始符);
    - 其中前 `i-1` 个 token 的 `K/V` 在之前步骤中已计算过, 重复计算导致效率低下;
- **方法**
    - 每步仅计算当前 token 的 `Q/K/V`, 并将新的 `K/V` 追加至缓存 `K_cache/V_cache` 中;
    - 执行 `Attention(Q, K_cache, V_cache)` —— **节省计算量的核心步骤**;
    - 生成当前 token, 并循环此过程;
- **效果**
    - 时间复杂度由 $O(n^2)$ 降至 $O(n)$;
- **代码展开说明**:
    ```python
    # 初始化缓存
    K_cache = torch.empty(batch, 0, d_model)
    V_cache = torch.empty(batch, 0, d_model)

    # --- 第 i 步: 生成第 i 个 token ---
    # 输入: [B, 1, D]
    Xi = torch.randn(batch, 1, d_model)

    # 计算 Q, K, V (假设这是解码器自注意力层)
    Qi = linear_q(Xi)  # [B, 1, D]
    Ki = linear_k(Xi)  # [B, 1, D]  
    Vi = linear_v(Xi)  # [B, 1, D]

    # 更新缓存: 将 Ki, Vi 存入
    K_cache = torch.cat([K_cache, Ki], dim=1) # [B, prev_len + 1, D]
    V_cache = torch.cat([V_cache, Vi], dim=1) # [B, prev_len + 1, D]

    # 计算自注意力
    Ai = attention(Qi, K_cache, V_cache) # [B, 1, D]

    # 经过 FFN 等操作, 生成第 i 个token
    ...
    ```
</details>

#### 3.1.3. 解释 "曝光偏差", 怎么引起的, 怎么缓解?


### 3.2. 介绍常见的序列生成策略
> Greedy Search, Beam Search, Sampling

#### 3.2.1. 对比 BeamSearch 和 贪心搜索 的优劣


#### 3.2.2. 为什么 LLM 在文本创作中倾向于使用 Sampling, 而不是 BeamSearch?


### 3.3. 如何控制生成序列的长度和终止?


### 3.4. 怎么抑制 LLM 生成过程中的 重复问题?


## 4. **拓展问题**

### 4.1. 非自回归模型是如何解码的? 与自回归解码的优劣