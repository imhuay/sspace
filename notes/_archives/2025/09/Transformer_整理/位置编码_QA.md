位置编码问题整理
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-06&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-20%2000%3A18%3A00&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-06 13:48:27
toc_title: 位置编码-QA
top: false
draft: false
hidden: true
section_number: true
level: 0
tags: []
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [位置编码](./位置编码.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [1. 🏷️ 位置编码基础](#1-️-位置编码基础)
    - [1.1. ✅ **为什么需要位置编码?** (位置编码的必要性, 自注意力的缺陷)](#11--为什么需要位置编码-位置编码的必要性-自注意力的缺陷)
    - [1.2. ✅ **什么是"置换不变性"?** (自注意力为何对置换不敏感?)](#12--什么是置换不变性-自注意力为何对置换不敏感)
    - [1.3. ✅ **位置编码是如何引入到模型中的?** (常见的引入方式)](#13--位置编码是如何引入到模型中的-常见的引入方式)
    - [1.4. ✅ 多头注意力中, 位置参数是共享的吗?](#14--多头注意力中-位置参数是共享的吗)
- [2. 📌 介绍常见的位置编码](#2--介绍常见的位置编码)
    - [2.1. ⬆️ 除了正弦位置编码, 还有哪些位置编码方式?](#21-️-除了正弦位置编码-还有哪些位置编码方式)
- [3. 🏷️ 绝对位置编码](#3-️-绝对位置编码)
    - [3.1. ✅ 介绍常见的绝对位置编码 (正弦位置编码, 可学习位置编码)](#31--介绍常见的绝对位置编码-正弦位置编码-可学习位置编码)
    - [3.2. ✅ 正弦位置编码有哪些优点?](#32--正弦位置编码有哪些优点)
        - [3.2.1. ✅ 解释为何正弦位置编码蕴含了相对位置信息? (给出三角恒等式推导)](#321--解释为何正弦位置编码蕴含了相对位置信息-给出三角恒等式推导)
        - [3.2.2. ✅ 正弦位置编码的 "波长" 是什么意思? 不同维度对应的波长有何不同?](#322--正弦位置编码的-波长-是什么意思-不同维度对应的波长有何不同)
        - [3.2.3. ✅ 为什么正弦位置编码使用 **加法** 引入而不是 **拼接**?](#323--为什么正弦位置编码使用-加法-引入而不是-拼接)
    - [3.3. ✅ **可学习位置编码** 有哪些应用场景?](#33--可学习位置编码-有哪些应用场景)
- [4. 🏷️ 相对位置编码](#4-️-相对位置编码)
    - [4.1. ✅ 什么是相对位置编码? 与绝对位置编码的核心区别是什么?](#41--什么是相对位置编码-与绝对位置编码的核心区别是什么)
        - [4.1.1. ⬆️ 位置编码中 **相对** 与 **绝对** 的含义](#411-️-位置编码中-相对-与-绝对-的含义)
    - [4.2. ✅ 比较 **绝对位置编码** 与 **相对位置编码**](#42--比较-绝对位置编码-与-相对位置编码)
        - [4.2.1. ⬆️ **相对位置编码** 对比 **绝对位置编码** 有哪些优势?](#421-️-相对位置编码-对比-绝对位置编码-有哪些优势)
    - [4.3. ✅ 位置编码中的 "外推" 问题指的是什么?](#43--位置编码中的-外推-问题指的是什么)
        - [4.3.1. ⚠️ 如何评估 "外推失败"? (诊断图, 对照实验)](#431-️-如何评估-外推失败-诊断图-对照实验)
    - [4.4. ⚠️ ALiBi 的线性偏置如何确保长距离外推?](#44-️-alibi-的线性偏置如何确保长距离外推)
        - [4.4.1. ⚠️ 斜率 per-head 设计的直觉是什么? 为什么头越多可覆盖更广距离带?](#441-️-斜率-per-head-设计的直觉是什么-为什么头越多可覆盖更广距离带)
        - [4.4.2. ⚠️ 为什么 ALiBi 在超长上下文中更稳定? 它的潜在代价是什么?](#442-️-为什么-alibi-在超长上下文中更稳定-它的潜在代价是什么)
    - [4.5. ❓ 相对位置偏置 (Shaw/T5) 如何控制参数规模?](#45--相对位置偏置-shawt5-如何控制参数规模)
- [5. 🏷️ **旋转位置编码 (RoPE)**](#5-️-旋转位置编码-rope)
    - [5.1. ✅ 介绍 **旋转位置编码** (动机/做法/优势)](#51--介绍-旋转位置编码-动机做法优势)
    - [5.2. ✅ RoPE 为什么能把绝对位置转成相对位移不变的注意力? (写出关键等式)](#52--rope-为什么能把绝对位置转成相对位移不变的注意力-写出关键等式)
    - [5.3. ✅ RoPE 中的 **底数** (`d`) 或 **频率基** (`θ`) 是如何影响位置编码效果的?](#53--rope-中的-底数-d-或-频率基-θ-是如何影响位置编码效果的)
        - [5.3.1. ⬆️ 什么是 **相位绕回**/**相位失真**?](#531-️-什么是-相位绕回相位失真)
        - [5.3.2. ⬆️ 底数 `d` **太大/太小** (频率基 `θ` 太小/太大) 会发生什么?](#532-️-底数-d-太大太小-频率基-θ-太小太大-会发生什么)
        - [5.3.3. ⬆️ 为什么 RoPE 默认设置 `d = 10000`?](#533-️-为什么-rope-默认设置-d--10000)
        - [5.3.4. ⬆️ 为什么 **长上下文模型** 会设置一个更大的底数 (`d`)?](#534-️-为什么-长上下文模型-会设置一个更大的底数-d)
        - [5.3.5. ⬆️ 旋转角公式中加入 `i` 的作用是什么?](#535-️-旋转角公式中加入-i-的作用是什么)
    - [5.4. ⚠️ 列举几种提升 RoPE 长度外推稳定性的策略, 并说明各自副作用](#54-️-列举几种提升-rope-长度外推稳定性的策略-并说明各自副作用)
- [6. ✅ 该如何选择不同的位置编码? (嵌入加法 / 注意力偏置 / 旋转编码)](#6--该如何选择不同的位置编码-嵌入加法--注意力偏置--旋转编码)
    - [6.1. ⚠️ 在 GPU 显存占用与推理时延上分别有什么差异?](#61-️-在-gpu-显存占用与推理时延上分别有什么差异)
    - [6.2. ⚠️ 如何将已有绝对位置模型迁移到 RoPE? 或者 ALiBi?](#62-️-如何将已有绝对位置模型迁移到-rope-或者-alibi)
- [7. 💡 在视觉Transformer (ViT) 中, 是如何处理位置编码的?](#7--在视觉transformer-vit-中-是如何处理位置编码的)
    - [7.1. ❓ 2D 位置信息与 1D 文本位置信息有何不同? 如何为 2D 网格设计位置编码?](#71--2d-位置信息与-1d-文本位置信息有何不同-如何为-2d-网格设计位置编码)
<!--END_SECTION:toc-->

---

## 1. 🏷️ 位置编码基础

### 1.1. ✅ **为什么需要位置编码?** (位置编码的必要性, 自注意力的缺陷)
> **思路**: 自注意力机制具有 **"置换不变性"** → 需要位置编码提供的**顺序信息**;

<details><summary><b>详述</b></summary>

- **核心原因**
    - 自注意力机制的核心/本质是 **加权求和**, 在数学上不依赖输入顺序 → **置换不变性**;
- **解决方法**
    - 通过显式加入 **位置编码** 为模型提供 **顺序感知** 能力;

</details>

### 1.2. ✅ **什么是"置换不变性"?** (自注意力为何对置换不敏感?)
> 打乱序列的输入顺序, 其自注意力计算的加权结果不变

### 1.3. ✅ **位置编码是如何引入到模型中的?** (常见的引入方式)
1. 在输入模型前, 对词嵌入施加位置编码 (如 **正弦位置编码**);
    > 一般是生成一个与 token 维度相同的向量, 进行逐位相加
2. 在计算注意力前, 对 Q/K 施加变换 (如 **RoPE**);
3. 在得到注意力后, 对 logits 施加偏置 (如 **ALiBi**);

### 1.4. ✅ 多头注意力中, 位置参数是共享的吗?
> 默认 Transformer 中位置参数是跨头共享的, 以保证位置一致性 (参考系一致), 提升参数效率 (减少过拟合); 但会 **牺牲不同头在位置感知上的多样性与多尺度能力**; 因此在需要让 **不同头专注不同距离模式** 时, 也会采用分头位置参数的设计 (如 Swin Transformer);

## 2. 📌 介绍常见的位置编码
> 绝对位置编码 (正弦位置编码, 可学习位置编码), 相对位置编码 (SHAW, XLNet, T5, DeBERTa, ALiBi), 旋转位置编码 (RoPE)
>> [位置编码](./位置编码.md)

### 2.1. ⬆️ 除了正弦位置编码, 还有哪些位置编码方式?

## 3. 🏷️ 绝对位置编码

### 3.1. ✅ 介绍常见的绝对位置编码 (正弦位置编码, 可学习位置编码)
> [绝对位置编码](./位置编码.md#绝对位置编码)

### 3.2. ✅ 正弦位置编码有哪些优点?
> 唯一性, 确定性, 外推性, 相对位置
>> [正弦位置编码](./位置编码.md#正弦位置编码)

#### 3.2.1. ✅ 解释为何正弦位置编码蕴含了相对位置信息? (给出三角恒等式推导)
> 根据三角函数的性质 (sin/cos 相位差公式), **任意两个位置的编码可以通过相对位移的线性组合互相推导**, 从而在绝对编码中隐含了相对位置信息;

$$
\begin{aligned}
    \sin(\omega_i (p+k)) &= \sin(\omega_i p)\cos(\omega_i k) + \cos(\omega_i p)\sin(\omega_i k) \\
    \cos(\omega_i (p+k)) &= \cos(\omega_i p)\cos(\omega_i k) - \sin(\omega_i p)\sin(\omega_i k)
\end{aligned}
$$

#### 3.2.2. ✅ 正弦位置编码的 "波长" 是什么意思? 不同维度对应的波长有何不同?
> 波长 $\lambda$ 由公式中的 $10000^{2i/d_{model}}$ 决定. 随着维度 $i$ 从 $0$ 增加到 $d_{model}/2 - 1$, 波长从 $2π$ (高频, 变化快) 增长到 $10000 * 2π$ (低频, 变化慢); **这使模型能同时捕获近距离和远距离的位置关系**.

#### 3.2.3. ✅ 为什么正弦位置编码使用 **加法** 引入而不是 **拼接**?
> 简洁, 直观, 计算与参数效率; 同一表示空间融合语义与位置; **拼接** 是 **加法** 的一种特殊情况;
>> [transformer里PE为什么不采用concatenation的方式? - 知乎](https://www.zhihu.com/question/4717410141)

<details><summary><b>详述</b></summary>

- **实用角度**:
    - 直观, 简洁, 保持维度一致;
    - **计算与参数效率** (加入 PE 多出来的维度意味着更大的隐藏层);
        > ~~不成立, 计算量可以通过隐层维度控制~~, 神经网络设计中, 隐层的维度很少会小于输入层 (降维);
    - **已证实有效性**;
- **表示空间**:
    - **加法** 将位置向量视为对词向量的 "偏移", 直接在同一表示空间中融合语义与位置信息;
    - **拼接** 把语义和位置分成两个 **子空间**, 模型需要额外学习如何跨空间交互;
    - 在高维嵌入空间中, 词嵌入子空间与位置编码子空间往往近似 **正交**;
        - 直接加法, 模型也能通过线性变换分别处理语义和位置成分, **不必依赖拼接来保证正交**;
- **表达能力**:
    - 加法的 **表达能力更强**, **拼接** 只是 **加法** 的一种特殊情况;
    - 考虑以下情况:
        - 使用 **拼接** 方法, 设置词嵌入部分的维度为 $d_w$, 位置向量的维度为 $d_p$;
        - 设置 **加法** 的维度为 $d_w + d_p$,
        - 假设 **拼接** 方法更好, 那么模型可以通过学习, 使 **加法** 中词嵌入后 $d_p$ 维的分量为 0, 位置向量前 $d_w$ 维的分量为 0;
        - 从而模拟 **拼接** 的效果, 或者说 **拼接** 只是 **加法** 的一种特殊情况;

</details>

### 3.3. ✅ **可学习位置编码** 有哪些应用场景?
> 序列长度固定或可控; 数据分布特殊, 位置模式非均匀, 非规则结构 (视觉, 多模态)

- 可学习位置编码的优势在于它能 **拟合特定数据分布、适配固定或可控长度、跨模态对齐非规则结构、自动调节位置权重, 并在迁移中快速适应新任务** —— 这些都是固定公式型编码难以做到的;


## 4. 🏷️ 相对位置编码

### 4.1. ✅ 什么是相对位置编码? 与绝对位置编码的核心区别是什么?

- **相对位置编码** 是一种向模型中注入位置信息的方法;
    - 其核心思想是: 模型对 token 间语义关系的理解应更侧重于它们之间的 相对距离, 而非其在序列中的绝对位置;
- **核心区别**:
    - 编码所表达的 "位置信息" 是基于 **全局坐标** 还是 **相对距离** 来定义的;

#### 4.1.1. ⬆️ 位置编码中 **相对** 与 **绝对** 的含义
> 在位置编码中, **绝对** 与 **相对** 指的是编码所表达的 "位置信息" 是基于 **全局坐标** 还是 **相对距离** 来定义的;

### 4.2. ✅ 比较 **绝对位置编码** 与 **相对位置编码**
- **绝对位置编码**
    - **定义**: 每个位置 $p$ 都有一个唯一的编码, 直接表示它在序列中的 **全局位置**;
    - **特点**:
        - 编码与位置一一对应, 位置 $i$ 的编码在任何序列中都是相同的;
        - 注意力机制需要通过计算两个绝对编码的差异, 间接推断相对距离;
    - **优点**: 实现简单, 可直接生成任意位置的编码;
    - **缺点**: 对超出训练长度的序列泛化能力依赖编码设计;
    - **常见实现**:
        - 正弦位置编码;
        - 可学习位置向量;
- **相对位置编码**
    - **定义**: 编码直接表示两个位置之间的相对距离 $r=j-i$, 而不是它们的全局坐标;
    - **特点**:
        - 注意力分数中直接引入与距离相关的偏置或变换;
        - 模型天然对平移不敏感 (shift-invariant), 即整体平移序列不会改变相对关系;
    - **优点**: 更容易泛化到不同长度, 尤其是长序列;
    - **缺点**:
        - 实现复杂, 增加了计算复杂度;
        - 需要额外设计来控制参数规模;

#### 4.2.1. ⬆️ **相对位置编码** 对比 **绝对位置编码** 有哪些优势?

### 4.3. ✅ 位置编码中的 "外推" 问题指的是什么?
> 训练好的模型能否在 **推理** 时有效处理 **比训练阶段见过更长的输入**

#### 4.3.1. ⚠️ 如何评估 "外推失败"? (诊断图, 对照实验)
> 当模型在长于其训练长度的序列上, 其性能指标显著差于在训练长度范围内的性能时, 即为外推失败; 诊断图 (困惑度随位置曲线, 注意力距离分布热图); 对照实验 (长度扫描曲线, 位置策略 A/B 测试)

### 4.4. ⚠️ ALiBi 的线性偏置如何确保长距离外推?
> ALiBi 通过在注意力 logits 中加入 **与相对距离成比例的固定负偏置**, 消除了绝对位置依赖和周期性失真, 使得 **推理时的长距离计算与训练时完全同分布**, 从而天然支持任意长度的外推;

#### 4.4.1. ⚠️ 斜率 per-head 设计的直觉是什么? 为什么头越多可覆盖更广距离带?
> ALiBi 的 per‑head 斜率设计是为了 **让不同注意力头在距离衰减上形成多尺度互补**, 头越多, 斜率分布越密、覆盖范围越广, 长短距依赖都能覆盖;

#### 4.4.2. ⚠️ 为什么 ALiBi 在超长上下文中更稳定? 它的潜在代价是什么?
> ALiBi 的线性相对偏置不会随长度漂移, 也无相位绕回; 但代价是持续惩罚远距注意力, 对需要高保真跨长距离依赖的任务来说, 它的归纳偏置过于保守;

### 4.5. ❓ 相对位置偏置 (Shaw/T5) 如何控制参数规模?
> 桶化距离 vs 连续函数参数化的权衡?

## 5. 🏷️ **旋转位置编码 (RoPE)**

### 5.1. ✅ 介绍 **旋转位置编码** (动机/做法/优势)
> [旋转位置编码](./位置编码.md#旋转位置编码-)

### 5.2. ✅ RoPE 为什么能把绝对位置转成相对位移不变的注意力? (写出关键等式)
> RoPE 通过一个 **基于绝对位置** 的 **旋转变换**, 在计算注意力分数时, 使绝对位置信息自动抵消, 只留下相对位置信息;

<details><summary><b>关键等式</b></summary>

$$
\langle \text{RoPE}(q, m), \text{RoPE}(k, n) \rangle = \langle q, \text{RoPE}(k, m - n) \rangle
$$

- 第一步: 对 Q/K 分别施加与 **绝对位置** 相关的旋转;
- 第二步: 利用 **旋转矩阵的正交性**, 将两个绝对位置的 **旋转差** 化为单一的 **相对位移旋转**
- **结果**: 注意力分数 (Q·K 内积) 只依赖于 **相对位移** $m-n$, 与绝对位置无关;

</details>

### 5.3. ✅ RoPE 中的 **底数** (`d`) 或 **频率基** (`θ`) 是如何影响位置编码效果的?
> 决定了 RoPE 在不同维度的旋转频率分布; `d` 越大 → `θ` 越小 → **强化短距分辨率但长距失真快**; `d` 越小 → `θ` 越大 → **提升长距稳定性但削弱局部精度**;

<details><summary><b>详述</b></summary>

- **旋转角** 公式:
  $$\omega_i = m \theta_i = \frac{m}{10000^{2i/d}}$$
    - 其中
        - $m$ 为 token 位置索引;
        - $i$ 为 向量 的分量索引;
    <!-- - **底数 $d$ 越大** (**频率基 ($\theta$) 越小**), 波长越长, 意味着需要移动更远的距离/旋转更大的角度 (更大的 m) 才能经过一个完整的周期 (2π) → 更晚出现 **相位绕回**/**相位失真** → **长距离更稳定**; -->
- **底数 $d$ 增大/减小的影响**
    - $d$ **增大** → $\theta$ 减小 (波长变长) → 相邻位置的相位差减小小 → 损害模型捕捉 **局部模式** 的能力 / **增强模型的外推能力**;
    - $d$ **减小** → $\theta$ 变大 (波长变短) → 过早发生 **相位失真**/**相位绕回** → **损害模型的外推能力** / 增强了捕捉 **近距离** 位置变化的能力 / 高频震荡, 难以训练;
- **公式中加入 $i$ 的作用**:
    - **让向量中不同维度的分量负责不同尺度的位置信息**;
    - **低维** ($i$ 小): $θ_i$ 较大 (高频), 波长短; 这些维度对位置变化非常敏感, 旋转很快; 它们负责捕捉局部、精细的位置关系;
    - **高维** ($i$ 大): $θ_i$ 较小 (低频), 波长长; 这些维度旋转缓慢, 负责提供全局、稳定的位置信号;

</details>

#### 5.3.1. ⬆️ 什么是 **相位绕回**/**相位失真**?
> 三角函数具有 **周期性**, 当旋转角大于 $2\pi$ 时会出现相位重复, 导致远距离位置不可区分 (**相位失真**);

#### 5.3.2. ⬆️ 底数 `d` **太大/太小** (频率基 `θ` 太小/太大) 会发生什么?
> `d` 太大 → 损失捕捉 **局部模式** 的能力; `d` 太小 → 损失长距离 **外推能力**;

#### 5.3.3. ⬆️ 为什么 RoPE 默认设置 `d = 10000`?
> 不同维度负责不同尺度

#### 5.3.4. ⬆️ 为什么 **长上下文模型** 会设置一个更大的底数 (`d`)?
> 延缓相位失真;

#### 5.3.5. ⬆️ 旋转角公式中加入 `i` 的作用是什么?
> 控制位置编码中不同维度的旋转频率, 使同时具备长程和短程的多尺度信息, 提升模型对不同距离依赖的表达能力;

### 5.4. ⚠️ 列举几种提升 RoPE 长度外推稳定性的策略, 并说明各自副作用
> 增大频率基底数 $d$; 位置插值 (Position Interpolation); 频率重标定 (如 NTK-aware / 动态 NTK); 分段频率缩放 (如 YaRN / Hybrid Scaling);

<details><summary><b>详述</b></summary>

| 策略 | 核心思路 | 作用 | 副作用 |
|---|---|---|---|
| 增大频率基底数 $d$ | 降低整体频率 | 稳定长距外推 | 短距分辨率下降 |
| 位置插值 | 线性缩放位置索引 | 延缓相位绕回 | 损害高频信息/局部细节, 整体性能下降 |
| NTK-aware | 差异化缩放频率 (高频少缩, 低频多缩) | 长短距兼顾 | 参数敏感 |
| YaRN | 缩放频率 + 调节注意力分布 (温度缩放) | 平衡长短距 | 实现复杂, 调参成本高 |

</details>

## 6. ✅ 该如何选择不同的位置编码? (嵌入加法 / 注意力偏置 / 旋转编码)
> **偏好外推**: RoPE, ALiBi; **偏好相对性/平移不变性**: RoPE, 偏置+可学习; **短序列**: 可学习位置编码

### 6.1. ⚠️ 在 GPU 显存占用与推理时延上分别有什么差异?
> **显存占用**: 嵌入加法 O(Nd); 注意力偏置 O(N^2) 或 分桶 O(N); 旋转编码 O(1);  
> **推理时延**: 注意力偏置 > 旋转编码 > 嵌入加法; 对模型整体影响不大;

### 6.2. ⚠️ 如何将已有绝对位置模型迁移到 RoPE? 或者 ALiBi?
> RoPE: 架构改造 (替换位置编码) → 持续预训练 (低学习率) → 微调 → (如果是 RoPE) 推理时做 **位置插值** 或 频率基重标定 (NTK-aware scaling);

## 7. 💡 在视觉Transformer (ViT) 中, 是如何处理位置编码的?
> ViT 将图像切分成一系列的图像块 (Patch), 每个 Patch 被视为一个词元; 然后为每个 Patch 的位置 (通常是2D的) 生成一个可学习的位置编码, 与 Patch 的嵌入向量相加.

### 7.1. ❓ 2D 位置信息与 1D 文本位置信息有何不同? 如何为 2D 网格设计位置编码?
