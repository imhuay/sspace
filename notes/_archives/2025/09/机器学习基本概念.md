机器学习基本概念
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-03&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-13%2003%3A09%3A18&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-03 14:40:39
top: false
draft: false
hidden: true
level: 0
tags: [ml]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: 归纳偏置*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [归纳偏置 (Inductive Bias)](#归纳偏置-inductive-bias)
<!--END_SECTION:toc-->

---

<!--START_SECTION:keyword-->
<!--keyword_info
name: 归纳偏置
-->
## 归纳偏置 (Inductive Bias)
> Inductive Bias
<!--END_SECTION:keyword-->

**定义**
- 在机器学习中, **归纳偏置**指的是学习算法在面对未知样本时, 为了做出预测而**必须依赖的一组先验假设**;
- **必要性**:
    - "天下没有免费的午餐" 定理表明, 没有任何学习算法能在所有可能的任务上都表现最佳;
    - 因此, 算法必须对目标函数或数据分布做出某种偏好或限制, 才能在有限数据下实现泛化;
- **类比**:
    - 就像侦探破案时会优先考虑最可能的嫌疑人, 而不是所有人都查一遍;
    - 这种 "优先考虑" 的倾向, 就是归纳偏置;

**作用**
- 缩小假设空间 (Hypothesis Space), 减少搜索范围;
- 在有限样本下, 提高模型在新数据上的泛化能力;

**示例**
- **CNN (卷积神经网络) **: 假设图像具有**局部性**和**平移不变性**, 因此使用卷积核共享权重;
- **RNN (循环神经网络) **: 假设序列数据具有**时间顺序依赖**, 因此在时间步上共享参数;
- **KNN (最近邻) **: 假设特征空间中相邻样本更可能属于同一类;
- **SVM (支持向量机) **: 假设最优分类边界应最大化间隔;
