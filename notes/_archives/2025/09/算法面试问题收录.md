算法面试问题收录
===
<!--START_SECTION:badge-->

![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-04&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-05%2012%3A30%3A39&label_color=gray&color=thistle&style=flat-square)

<!--END_SECTION:badge-->
<!--info
date: 2025-09-04 16:00:22
top: false
draft: false
hidden: true
level: 0
tag: []
-->

<!--START_SECTION:keywords-->
> ***Keywords**: 算法面试问题收录*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [Transformer](#transformer)
- [LLM](#llm)
    - [微调相关](#微调相关)
    - [MoE 相关](#moe-相关)
<!--END_SECTION:toc-->

---

<!--START_SECTION:keyword-->
## Transformer
<!--END_SECTION:keyword-->

- 详述**多头注意力机制**

- **位置编码**的作用, 演进

- LayerNorm vs BatchNorm



<!--START_SECTION:keyword-->
## LLM
<!--END_SECTION:keyword-->

- CausalLM vs PrefixLM

### 微调相关

- LoRA 原理

- 怎么缓解大模型遗忘问题

- 怎么解决微调时模型遗忘


### MoE 相关


