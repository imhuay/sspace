强化学习基础 (LLM 视角)
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-25&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-25%2003%3A17%3A40&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-25 02:59:27
toc_title: '强化学习基础 ( **LLM-based** )'
top: false
draft: false
hidden: true
omit_in_tag_toc: false
section_number: false
level: 99
tags: []
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [偏好学习](./偏好学习.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [基础概念](#基础概念)
    - [术语映射](#术语映射)
        - [**智能体** (Agent)](#智能体-agent)
        - [**策略** (Policy)](#策略-policy)
        - [**环境** (Environment)](#环境-environment)
        - [**状态** (State)](#状态-state)
        - [**动作** (Action)](#动作-action)
        - [**奖励** (Reward)](#奖励-reward)
        - [**回报** (Return)](#回报-return)
<!--END_SECTION:toc-->

---

## 基础概念

- 强化学习算法可以分为两大类:
    - 基于 **值函数** 的强化学习
        - Q-Learning
        - SARSA
        - DQN
    - 基于 **策略** 的强化学习
        - REINFORCE
        - 自然策略梯度 (Natural Policy Gradient, NPG)
        - 信赖域策略优化 (Trust Region Policy Optimization, TRPO)
        - 近端策略优化 (Proximal Policy Optimization, PPO)
        - ...


### 术语映射
> RL 一般应用于游戏智能体等场景, 其概念或术语在 LLM 背景下需要做相应调整或说明.
<!-- - 将 RL 框架中的 **交互闭环** 映射到 LLM 的优化过程; -->
<!-- - 建立直觉, 将 LLM 的生成过程映射到 RL 框架; -->
<!-- - 下面是一些 **关键术语映射**: -->
概念 | RL 中的含义 | RLHF 中对应的含义
---------|----------|---------
**智能体** (Agent) | 学习者或决策者, 是优化的目标对象. | 在 RLHF 中, 智能体被称为 **策略模型 (Policy Model)**, 通常是基于 SFT 模型进行初始化的 LLM 本身.
**策略** $\pi$ (Policy) | • 智能体的行为函数, 通常参数化为 $\pi_{\theta}(\cdot)$;<br>• 它定义了从 **状态** $s$ 到 **动作** $a$ 的映射, 即 $a = \pi(s)$. | 在文本生成中, 即策略模型根据当前 **上下文 (状态)** 输出下一个 **token (动作)** 的概率分布.
**环境** (Environment) | 智能体与之交互的外部世界 | 即当前的 **上下文**, 由给定的 **提示 (Prompt)** 和 **已经生成的 token 序列** 共同构成.
**状态** $s$ (State) | 环境在当前时刻的描述 | 上下文在当前时刻的 **向量表示**
**动作** $a$ (Action) | 智能体在给定状态下采取的操作 | 给定上下文 **生成下一个 token**
**奖励** $r$ (Reward) | 环境在智能体执行动作后反馈的 **标量信号**, 用于评估动作的好坏. | 在 RLHF 中, 奖励由一个独立训练的 **奖励模型 (Reward Model, RM)** 提供, 负责评估 **Prompt+生成文本** 的质量.
**回报** $G_t$ (Return) | 从某个状态开始, 到 **一轮交互/一个回合** **结束** 所能获得的 **累积奖励**. | - <!-- 因为 RLHF 中的奖励是对完整回答的一次性评分, 故一般不涉及累积奖励; -->
**价值函数** (Value Function) | 从某个状态开始, 遵循当前策略所能获得的期望累积奖励, 包括:<br>• **状态价值函数** $V(s)$: 从状态 $s$ 开始, 遵循策略 $\pi$ 所能获得的期望累积奖励;<br>• **动作价值函数** $Q(s, a)$: 在状态 $s$ 执行动作 $a$ 后, **继续遵循策略** $\pi$ 所能获得的期望累积奖励. | • **状态价值函数**: 在诸如 TRPO/PPO 等 **基于 Actor-Critic 框架** 的 **在线策略 (On-Policy) 梯度算法** 中, 通常会训练一个 **价值模型 (Critic)** 来近似 $V(s)$;<br>• **离线策略 (Off-Policy)** 算法中一般 **不需要价值模型**. <!-- <br>• 其主要作用是计算 **优势函数** $A(s, a) = Q(s, a) - V(s)$;<br>• 能有效 **降低** **策略梯度估计的方差/训练方差**; -->
**优势函数** $A(s,a)$ (Advantage Function) | • 衡量在某状态下执行特定动作相对于平均水平的优劣,<br>• 具体的, **在状态 $s$ 下执行动作 $a$ 所能获得的期望累积奖励** 与 **遵循当前策略所能获得的期望累积奖励** 之间的 **差值**, 即 $A(s, a) = Q(s, a) - V(s)$ |
**折扣因子** $\gamma$ (Discount Factor) | 用于权衡即时奖励和未来奖励的重要性, 记 $\gamma \in (0,1)$;<br>• $\gamma \to 0$ 更注重短期收益,<br>• $\gamma \to 1$ 更注重长期收益. | • 在 RLHF 中, 通常 **不考虑长期奖励衰减**, 即未来 token 的重要性与当前 token 相当;<br>• 因此 $\gamma$ **常被设置为接近或等于 $1$**, 意味着较少考虑未来奖励的衰减;<br>• 但这并非固定不变, 可根据具体需求调整.
<!-- 这在 TRPO/PPO 等 **在线 (online) 策略梯度算法 (基于 Actor-Critic 框架)** 中对于降低训练方差至关重要. -->
<!--
RL 中的概念 | RLHF 中的含义或术语 | 说明
---|---|---
智能体 (Agent) / 动作模型 (Actor Model) | 策略模型 (Policy Model) | 即需要被微调的语言模型本身.
环境 (Environment) | 上下文 (Prompt + 已生成 token 序列) | 模型生成时所处的信息背景.
状态 (State) | 上下文在当前时刻的向量表示 | 在标准 RLHF 流程中, 状态通常是隐式地由模型内部隐藏状态表示的.
动作 (Action) | 生成 Response (episode-level) 或生成下一个 Token (token-level) | RLHF 中多将一次完整回答视为单个动作; token 级动作用于更细粒度的优化, 同时会加入 **回合** 的概念, 表示生成一次完整的回答.
奖励 (Reward) | **奖励模型** 分数 | 通常由奖励模型对完整的 "Prompt + Response" 计算得到一个标量得分.
策略 (Policy) | 策略模型的概率分布 | RL 中表示 **当前状态到下一个动作** 的映射 $\pi(a\|s)$; RLHF 中即 **从当前上下文生成下一个 token 的概率分布** (token-level) 或 **完整回答的概率分布** (episode-level).
价值函数 (Value Function) | 价值模型 (可选) | 估计状态或状态-动作对的期望奖励; 用于 **优势函数** 计算; RLHF 中用于估计从当前上下文继续生成至结束的期望奖励; 价值函数
折扣因子 $\gamma$ (Discount Factor) | 无直接对应或默认 $\gamma=1$ | 用于权衡当前奖励和未来奖励的重要性; RLHF 中通常不考虑长期奖励衰减 (未来 token 的重要性与当前 token 几乎相当), 因此 $\gamma$ 常被设置为一个接近 $1$ 的值.
-->

<!--
#### 核心组件

- **策略模型** (Policy Model)
    -
- **策略优化算法** / **策略梯度算法**
    > TRPO → PPO (Clip/KL) → DPO → GRPO/IPO/KTO/...
- **参考模型** (Reference Model)
- **奖励模型** (Reward Model, 可选)
- **价值模型** (Critic, 可选)
 -->


<table>

<tr>
<th>概念</th><th>RL 中的含义</th><th>RLHF 中对应的含义</th>
</tr>

<tr>
<td>

#### **智能体** (Agent)

</td>
<td>

• 学习者或决策者, 优化的目标对象.<br>

</td>
<td>

• 待优化的 **语言模型本身**, 通常被称为 **策略模型 (Policy Model)**.<br>
<!-- • 在 RLHF 中, 智能体被称为 **策略模型 (Policy Model)**, 通常是基于 SFT 模型进行初始化的 LLM 本身.<br> -->

</td>
</tr>
<tr>
<td>

#### **策略** (Policy)
> 记 $\pi$

</td>
<td>

• 智能体的行为函数, 通常参数化为 $\pi_{\theta}(\cdot)$;<br>
• 定义了从 **状态** $s$ 到 **动作** $a$ 的映射, 即 $a = \pi(s)$.<br>

</td>
<td>

• 策略模型根据当前 **上下文 (状态)** 输出下一个 **token (动作)** 的概率分布.<br>

</td>
</tr>
<tr>
<td>

#### **环境** (Environment)

</td>
<td>

• 智能体与之交互的外部世界<br>

</td>
<td>

• 当前 **上下文**, 由给定的 **提示 (Prompt)** 和 **已经生成的 token 序列** 共同构成.<br>

</td>
</tr>
<tr>
<td>

#### **状态** (State)
> 记 $s$

</td>
<td>

• 环境在当前时刻的描述<br>

</td>
<td>

• 上下文在当前时刻的 **向量表示**<br>

</td>
</tr>
<tr>
<td>

#### **动作** (Action)
> 记 $a$

</td>
<td>

• 智能体在给定状态下采取的操作<br>

</td>
<td>

• 给定上下文 **生成下一个 token**<br>

</td>
</tr>
<tr>
<td>

#### **奖励** (Reward)
> 记  $r$

</td>
<td>

• 环境在智能体执行动作后反馈的 **标量信号**, 用于评估动作的好坏.<br>

</td>
<td>

• ⚠️ 在 RLHF 中, **奖励** 通常指的是对一次 **完整输出** 的评分, 而不是对一次动作 (生成下一个 token);<br>
• 具体的, **奖励** 由一个独立训练的 **奖励模型 (Reward Model, RM)** 计算, 负责评估 **Prompt+生成文本** 的整体质量;<br>
• 形式上是一个 **标量分数**.<br>
<!-- • 在 RLHF 中, 奖励由一个独立训练的 **奖励模型 (Reward Model, RM)** 提供, 负责评估 **Prompt+生成文本** 的质量.<br> -->

</td>
</tr>
<tr>
<td>

#### **回报** (Return)
> 记 $G$
<!-- > $G_t$ 表示当前时刻的回报 -->

</td>
<td>

• 从某个状态开始, 到 **一轮交互完成/一个回合结束** 所能获得的 **累积奖励**.<br>

</td>
<td>

• <br>

</td>
</tr>

</table>