强化学习基础 (LLM 视角)
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-25&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-26%2013%3A19%3A16&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-25 02:59:27
toc_title: '**强化学习基础** ( **LLM-based** )'
top: false
draft: false
hidden: true
omit_in_tag_toc: false
section_number: false
level: 99
tags: []
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [偏好学习](./偏好学习.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [基础概念](#基础概念)
- [**RL 术语** 及其在 **RLHF** 中的含义](#rl-术语-及其在-rlhf-中的含义)
    - [**智能体** (Agent)](#智能体-agent)
    - [**环境** (Environment)](#环境-environment)
    - [**状态** (State)](#状态-state)
    - [**动作** (Action)](#动作-action)
    - [**策略** (Policy)](#策略-policy)
    - [**奖励** (Reward)](#奖励-reward)
    - [**回报** (Return)](#回报-return)
    - [**价值函数** (Value Function)](#价值函数-value-function)
        - [**状态价值函数** (State Value Function)](#状态价值函数-state-value-function)
        - [**动作价值函数** (Action Value Function)](#动作价值函数-action-value-function)
    - [**优势函数** (Advantage Function)](#优势函数-advantage-function)
    - [**折扣因子** (Discount Factor)](#折扣因子-discount-factor)
- [贝尔曼方程 (Bellman Equation)](#贝尔曼方程-bellman-equation)
    - [**状态价值函数** 的贝尔曼方程](#状态价值函数-的贝尔曼方程)
    - [**动作价值函数** 的贝尔曼公式](#动作价值函数-的贝尔曼公式)
- [时序差分算法 (Temporal Difference, TD)](#时序差分算法-temporal-difference-td)
- [广义优势估计 (GAE)](#广义优势估计-gae)
<!--END_SECTION:toc-->

---

## 基础概念

- 强化学习算法可以分为两大类:
    - 基于 **值函数** 的强化学习
        - Q-Learning
        - SARSA
        - DQN
    - 基于 **策略** 的强化学习
        - REINFORCE
        - 自然策略梯度 (Natural Policy Gradient, NPG)
        - 信赖域策略优化 (Trust Region Policy Optimization, TRPO)
        - 近端策略优化 (Proximal Policy Optimization, PPO)
        - ...


<!--START_SECTION:keyword-->
<!--keyword_info
name: '基础术语'
extra_url: false
-->
## **RL 术语** 及其在 **RLHF** 中的含义
<!--END_SECTION:keyword-->
> RL 一般应用于游戏智能体等场景, 其概念或术语在 LLM 背景下需要做相应调整或说明.

> **约定**: **大写字母** 表示 **随机变量**, 如 $S_t$; **小写字母** 为随机变量的 **具体取值**, 如 $s_t$;
<table>
<tr>
<th>概念</th><th>RL 中的含义</th><th>RLHF 中对应的含义</th>
</tr>

<tr>
<td>

### **智能体** (Agent)

</td>
<td>

• 学习者或决策者, 优化的目标对象<br>

</td>
<td>

• 待优化的 **语言模型本身**, 通常被称为 **策略模型 (Policy Model)**<br>
<!-- • 在 RLHF 中, 智能体被称为 **策略模型 (Policy Model)**, 通常是基于 SFT 模型进行初始化的 LLM 本身.<br> -->

</td>
</tr>
<tr>
<td>

### **环境** (Environment)

</td>
<td>

• 智能体与之交互的外部世界<br>

</td>
<td>

• 当前 **上下文**, 由给定的 **提示 (Prompt)** 和 **已经生成的 token 序列** 共同构成<br>

</td>
</tr>
<tr>
<td>

### **状态** (State)
> 记 $s_t$

</td>
<td>

• 环境在特定时刻 $t$ 下的描述, 通常满足马尔可夫性质<br>
<!-- • 对时刻 $t$, 记 $s_t$<br> -->

</td>
<td>

• 上下文在特定时刻的 **向量表示**, 它编码了截至当前时刻的所有上下文信息<br>

</td>
</tr>
<tr>
<td>

### **动作** (Action)
> 记 $a_t$

</td>
<td>

• 智能体在给定状态下采取的操作<br>

</td>
<td>

• 策略模型在给定上下文的情况下 **生成下一个 token**<br>

</td>
</tr>
<tr>
<td>

### **策略** (Policy)
> 记 $\pi$

</td>
<td>

• 智能体的行为函数, 通常参数化为 $\pi_{\theta}(\cdot)$;<br>
• 定义了从 **状态** $s_t$ 到 **动作** $a_t$ 的映射, 即 $a_t = \pi_{\theta}(s_t)$;<br>

</td>
<td>

• 策略模型根据当前 **上下文 (状态)** 输出下一个 **token (动作)** 的概率分布, 即 $\pi(a_t | s_t)$<br>

</td>
</tr>
<tr>
<td>

### **奖励** (Reward)
> 记  $r_t$

</td>
<td>

• 环境在智能体执行动作后反馈的 **标量信号**, 用于评估动作的好坏<br>

</td>
<td>

• ⚠️ 在 RLHF 中, **奖励** 通常指的是对一次 **完整输出** 的评分, 而不是对一次动作 (生成下一个 token);<br>
• 具体的, **奖励** 由一个独立训练的 **奖励模型 (Reward Model, RM)** 计算, 负责评估 **Prompt+生成文本** 的整体质量;<br>
• 形式上是一个 **标量分数**;<br>
<!-- • 在 RLHF 中, 奖励由一个独立训练的 **奖励模型 (Reward Model, RM)** 提供, 负责评估 **Prompt+生成文本** 的质量.<br> -->

</td>
</tr>
<tr>
<td>

### **回报** (Return)
> 记 $G_t$
<!-- > $G_t$ 表示当前时刻的回报 -->

</td>
<td>

• 从特定时刻的状态开始, 到 **一轮交互完成/一个回合结束** 所能获得的 **累积奖励**<br>
<!-- • 常用折扣因子 $\gamma$ 计算, 如 $G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}$;<br> -->

</td>
<td>

• 从当前 token 位置开始, 到生成结束所能获得的 **累积奖励**<br>
<!-- • 由于语言生成中未来 token 的重要性相近, 折扣因子 $\gamma$ 常设为 $1$, 即 $G_t = \sum_{k=0}^{T-t} r_{t+k}$; -->

</td>
</tr>
<tr>
<td>

<!--START_SECTION:keyword-->
<!--keyword_info
name: '价值函数'
extra_url: false
-->
### **价值函数** (Value Function)
<!--END_SECTION:keyword-->

#### **状态价值函数** (State Value Function)
> 记 $V(s_t)$

#### **动作价值函数** (Action Value Function)
> 记 $Q(s_t, a_t)$

</td>
<td>

• $V(s_t)$: 在 **状态** $s_t$ 下, 遵循 **策略** $\pi$ 所能获得的 **期望回报**, 反映了在该状态下执行 **所有可能动作** 的 **长期收益平均水平**;<br>
• $Q(s_t, a_t)$: 在 **状态** $s_t$ 下执行 **动作** $a_t$ 后, **继续遵循策略** $\pi$ 所能获得的 **期望回报**<br>
<!-- 从某个状态开始, 遵循当前策略所能获得的 **期望回报** (**期望累积奖励**), 包括:<br> -->
<!-- • **状态价值函数** $V(s)$: 从状态 $s$ 开始, 遵循策略 $\pi$ 所能获得的期望回报;<br> -->
<!-- • **动作价值函数** $Q(s, a)$: 在状态 $s$ 执行动作 $a$ 后, **继续遵循策略** $\pi$ 所能获得的期望回报.<br> -->

</td>
<td>

• 对于 **在线策略 (On-Policy) 梯度算法** —— **基于 Actor-Critic 框架**, 如 TRPO/PPO 等;<br>
• 其特点是 **使用当前策略生成的数据来优化策略本身**;<br>
• 在这类算法中, 通常会训练一个 **价值模型 (Critic)** 来近似估计状态价值函数 $V(\cdot)$, 作为评价策略好坏的基线;<br>
• 但一般 **不直接建模** $Q(\cdot)$; 而是通过 **即时奖励** 和 **下一状态的价值函数** 来估计动作价值, 其关系由 [**贝尔曼方程**](#贝尔曼方程-bellman-equation) 定义:<br>
  $$Q(s_t,a_t) = r_t + \gamma V(s_{t+1})$$

---

• 对于 **离线策略 (Off-Policy)** 算法 (如 DPO 等),<br>
• 其特点是 **利用历史策略或其他策略产生的数据进行学习**;<br>
• 以 **避免显式地训练一个独立的价值模型**;<br>
• 其目标函数的理论推导与最优策略下的 **优势函数** 或 **Q 函数** 有关;<br>

---
> • On-Policy 与 Off-Policy 的 **核心区别**: **用于训练的数据生成方式**;<br>
> • On-Policy **必须** 使用 当前策略 $\pi$ 最新生成的数据;<br>
> • Off-Policy **可以** 使用 历史数据 或 其他策略 生成的数据;<br>

<!-- 其核心目标是 **直接学习并优化动作价值函数** $Q(s, a)$ 本身, 如 DPO 等.<br> -->
<!-- • **状态价值函数**: 在诸如 TRPO/PPO 等 **基于 Actor-Critic 框架** 的 **在线策略 (On-Policy) 梯度算法** 中, 通常会训练一个 **价值模型 (Critic)** 来近似 $V(s)$;<br> -->
<!-- • **动作价值函数**: On-Policy 算法中, 通常不直接训练一个模型来近似 $Q$ 函数, 而是通过 **奖励模型的反馈** 和 **价值模型的预测** 相结合来估算, 一般取 $$;<br> -->
<!-- • **离线策略 (Off-Policy)** 算法一般 **不需要价值模型**, 而其核心目标就是 **直接学习并优化动作价值函数** $Q(s, a)$ 本身.<br> -->
<!-- • **广义优势估计** (GAE): 一种高效且低方差的方法是使用 **时序差分误差 (TD Error)** 的累积来估算;<br> -->
<!-- • 定义 **单步 TD 误差**: $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$;<br> -->
<!-- • 则 $Q(s_t, a_t)$ 可以 **近似** 为 $r_t + \gamma V(s_{t+1})$;<br> -->

</td>
</tr>
<tr>
<td>

<!--START_SECTION:keyword-->
<!--keyword_info
name: '优势函数'
extra_url: false
-->
### **优势函数** (Advantage Function)
<!--END_SECTION:keyword-->
> 记 $A(s,a)$

</td>
<td>

• 衡量在特定状态下执行某一动作相对于 **平均水平** 的优劣;<br>
• **数学定义**: $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$<br>
• 即 **在状态 $s_t$ 下执行动作 $a_t$ 所能获得的期望回报** $Q(s_t, a_t)$ 与 **遵循当前策略所能获得的期望回报** $V(s_t)$ 之间的 **差值**.<br>

</td>
<td>

• 在 LLM 语境下, 即评估给定上下文生成某个 token 的相对好坏;<br>
• 根据 [**贝尔曼方程**](#贝尔曼方程-bellman-equation) 与 [**时序差分 (TD) 算法**](#时序差分算法-temporal-difference-td) 有:<br>
  $$A_t \approx \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \quad \scriptstyle\text{// 单步优势估计}$$
• **一个更通用的形式是** [**广义优势估计 (GAE)**](#广义优势估计-gae):<br>
  $$A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$
<!-- • 其中 $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ 为 **时序差分误差**, $\lambda \in \lbrack 0, 1\rbrack$ 为 GAE 参数; 当 $\lambda = 0$ 时, 即退化为单步优势估计;<br> -->
• **在 RLHF 中, 估计优势函数是策略梯度计算的核心**;<br>
• 使用优势函数 (而非直接奖励 $r$) 可以显著 **降低方差**, 使训练更加稳定.<br>

</td>
</tr>
<tr>
<td>

### **折扣因子** (Discount Factor)
> 记 $\gamma$

</td>
<td>

• 用于权衡即时奖励和未来奖励的重要性, $\gamma \in (0,1)$;<br>
• $\gamma \to 0$ 更注重短期收益,<br>
• $\gamma \to 1$ 更注重长期收益.<br>

</td>
<td>

• 在 RLHF 中, 通常 **不考虑长期奖励衰减**;<br>
• 因此 $\gamma$ **常被设置为接近或等于 $1$**, 即未来 token 的重要性与当前 token 相当;<br>
• 但这并非固定不变, 可根据具体需求调整.

</td>
</tr>
</table>

<!--START_SECTION:keyword-->
<!--keyword_info
name: '贝尔曼方程'
extra_url: false
-->
## 贝尔曼方程 (Bellman Equation)
<!--END_SECTION:keyword-->
> • 贝尔曼方程描述了价值函数自身的递归关系, 是 **价值函数估计** 的理论基石;<br>
> • 一句话描述, 即 **当前时刻的价值** = **当前的即时奖励 (的期望)** + **折扣因子** × **下一时刻的价值 (的期望)**.<br>

### **状态价值函数** 的贝尔曼方程
- **定义**: 状态价值函数 $V(s_t)$ 表示从状态 $s_t$ 开始, 遵循策略所能获得的**期望回报**:
    $$V(s_t) = \mathbb{E}\big\lbrack\, G_t \,|\, S_t = s_t\,\big\rbrack$$
    其中 **回报** $G_t$ 为 **即时奖励** 和所有 **未来折扣奖励** 之和:
    $$\begin{aligned}
    G_t &= R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k} \\
        &= R_t + \gamma\sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k} \\
        &= R_t + \gamma G_{t+1}
    \end{aligned}$$
<!--
    $$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k} = R_t + \gamma\sum_{k=0}^{\infty} \gamma^k R_{t+1+k}$$
    或写作递归形式:
    $$G_t = R_t + \gamma G_{t+1}$$
    -->
- 根据期望的线性性质, 分离出即时奖励和未来奖励:
    $$V(s_t) = \mathbb{E}\big\lbrack\, R_t \,|\, S_t = s_t\big\rbrack + \gamma \mathbb{E}\big\lbrack\, G_{t+1} \,|\, S_t = s_t\,\big\rbrack$$
- 根据 **全期望公式 (塔性质)** 以及 **马尔可夫假设**, 有:
    $$
    \begin{aligned}
    \mathbb{E}\big\lbrack\, G_{t+1} \,|\, S_t = s_t\big\rbrack
        &= \mathbb{E}\Big\lbrack\,\mathbb{E}\big\lbrack\, G_{t+1} \,|\, S_{t+1}, S_t = s_t\,\big\rbrack \,\Big|\, S_t = s_t \,\Big\rbrack &\ \scriptstyle{//\ 期望迭代法则} \\
        &= \mathbb{E}\Big\lbrack\, \mathbb{E}\big\lbrack\, G_{t+1} \,|\, S_{t+1}\, \big\rbrack \,\Big|\, S_t = s_t \,\Big\rbrack &\ \scriptstyle{//\ 马尔可夫假设} \\
        &= \mathbb{E}\Big\lbrack\, V(S_{t+1}) \,\Big|\, S_t = s_t \,\Big\rbrack
    \end{aligned}
    $$
    > • **全期望公式**: $\mathbb{E}\big\lbrack\,\mathbb{E}\lbrack\,G_{t+1} \,|\, S_{t+1}, S_t\,\rbrack \mid S_t \,\big\rbrack = \mathbb{E}\lbrack\, G_{t+1} \mid S_t\,\rbrack$ —— 先在更多条件下取期望, 再在较少条件下取期望, 结果等于直接在较少条件下取期望;<br>
    > • **马尔可夫假设**: $\mathbb{E}\big\lbrack\, G_{t+1} \,|\, S_{t+1}, S_t\,\big\rbrack = \mathbb{E}\big\lbrack\, G_{t+1} \,|\, S_{t+1}\,\big\rbrack$ —— 给定当前状态 $S_{t+1}$, 未来回报 $G_{t+1}$ **不再依赖** 过去状态 $S_t$;<br>
- 综上:
    $$V(s_t) = \mathbb{E}\big\lbrack\, R_t \,|\, S_t = s_t\,\big\rbrack + \gamma \mathbb{E}\big\lbrack\, V(S_{t+1}) \,|\, S_t = s_t\,\big\rbrack$$
    > • 进一步引入策略 $\pi(a|s)$ 和状态转移概率 $P(s' \mid s, a)$, 可以得到更具体的表达式;<br>
    > • 智能体在状态 $s$ 下选择动作 $a$ 的概率是 $\pi(a|s)$, 执行动作 $a$ 后获得奖励 $\mathcal{R}(s, a, s')$, 并转移到状态 $s'$ 的概率是 $P(s' \mid s, a)$;<br>
    > • 下面是贝尔曼方程的完整形式 (**仅做参考, 具体推导略**):<br>
    > $$V(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' \mid s, a) \Big\lbrack\, \mathcal{R}(s, a, s') + \gamma V(s') \,\Big\rbrack$$
<!--
- _引入 **策略** 与 **动作** 的完整式_ (**仅做参考, 具体推导略**):
    $$V(s) = \sum_{a} \pi(a | s) \Big( \text{RM}(s,a) + \gamma \sum_{s'} P(s' \mid s,a) \, V(s') \Big)$$
    -->
- 在 **确定性环境** 下, 方程简化为:
    $$V(s_t) = r_t + \gamma V(s_{t+1})$$
- 在随机环境下, 可通过 **蒙特卡洛方法** 进行估计:
    - 单步采样 (无偏估计):
        $$\hat{V}(s_t) \approx r_t + \gamma \, V(s_{t+1})$$
        其中 $r_t$ 与 $s_{t+1}$ 来自采样轨迹;
    - 多样本平均可降低估计方差:
        $$\hat{V}(s_t) = \frac{1}{N} \sum_{i=1}^{N} \Big( r_t^{(i)} + \gamma \, V\big(s_{t+1}^{(i)}\big) \Big)$$
        > 这里 $N$ 条轨迹均从状态 $s_t$ 开始根据策略 $\pi$ 执行.

### **动作价值函数** 的贝尔曼公式
> 推导过程基本与 $V(\cdot)$ 相同
- **定义**: 动作价值函数 $Q(s_t, a_t)$ 表示在状态 $s_t$ 下执行动作 $a_t$ 后, 继续遵循策略 $\pi$ 所能获得的期望回报:
    $$Q(s_t, a_t) = \mathbb{E}\big\lbrack\, G_t \mid S_t = s_t, A_t = a_t\, \big\rbrack \xlongequal{简记} \mathbb{E}\big\lbrack\, G_t \mid s_t, a_t\, \big\rbrack$$
  > • 根据 $Q$ 函数的定义和 **全概率公式**, 有:<br>
  > $$\begin{aligned}
    V(s_t)
        = \mathbb{E}\big\lbrack\, G_t \mid S_t = s_t \,\big\rbrack
        &= \sum_{a} \pi(a \,|\, s_t)\ \mathbb{E}\big\lbrack G_t \mid S_t = s_t, A_t = a \big\rbrack \\
        &= \sum_{a} \pi(a \,|\, s_t) Q(s_t,a)
    \end{aligned}$$
- 类似 $V(s)$ 的推导, 应用期望的线性性质, 全期望公式及马尔可夫假设, 有:
    $$\begin{aligned}
    Q(s_t, a_t)
        &= \mathbb{E}\big\lbrack\, R_t \mid s_t, a_t \,\big\rbrack + \gamma \mathbb{E}\big\lbrack\, G_{t+1} \mid s_t, a_t \,\big\rbrack \\
        &= \mathbb{E}\big\lbrack\, R_t \mid s_t, a_t \,\big\rbrack + \gamma \mathbb{E}\Big\lbrack\, \mathbb{E}\big\lbrack\, G_{t+1} \mid S_{t+1}, s_t, a_t \,\big\rbrack \,\Big|\, s_t, a_t \,\Big\rbrack \\
        &= \mathbb{E}\big\lbrack\, R_t \mid s_t, a_t \,\big\rbrack + \gamma \mathbb{E}\Big\lbrack\, \mathbb{E}\big\lbrack\, G_{t+1} \mid S_{t+1} \,\big\rbrack \,\Big|\, s_t, a_t \,\Big\rbrack \\
        &= \mathbb{E}\big\lbrack\, R_t \mid s_t, a_t \,\big\rbrack + \gamma \mathbb{E}\Big\lbrack\, V(S_{t+1}) \,\Big|\, s_t, a_t \,\Big\rbrack \\
        &= \mathbb{E}\big\lbrack\, R_t + \gamma V(S_{t+1}) \,\Big|\, s_t, a_t \,\Big\rbrack
        \end{aligned}$$
    > • 引入状态转移概率, 其完整式为 (推导过程略):
    > $$Q(s, a) = \sum_{s'} P(s' \mid s, a) \Big\lbrack\, \mathcal{R}(s, a, s') + \gamma V(s') \,\Big\rbrack$$
    > • 代入 $V(s') = \sum_{a'} \pi(a'|s') Q(s', a')$, 得:
    > $$Q(s, a) = \sum_{s'} P(s' \mid s, a) \Big\lbrack\, \mathcal{R}(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q(s', a') \,\Big\rbrack$$
<!--
- 在 **确定性环境** 可简化为:
    $$Q(s_t, a_t) = r_t + \gamma Q(s_{t+1}, a_{t+1})$$
-->
- 在 PPO 等算法中, **通常不会直接建模** $Q(\cdot)$, 比如通过 **蒙特卡洛方法** 近似:
    $$\hat{Q}(s_t, a_t) \approx r_t + \gamma V(s_{t+1})$$
- 基于此近似, 可以构造 **优势函数** $\mathcal{A}(s_t, a_t)$ 的估计:
    $$\mathcal{A}(s_t, a_t) = Q(s_t, a_t) - V(s_t) \approx r_t + \gamma V(s_{t+1}) - V(s_t)$$
    > 此即 **一阶时序差分误差** (TD Error) 的形式;


<!--START_SECTION:keyword-->
<!--keyword_info
name: '时序差分算法'
extra_url: false
-->
## 时序差分算法 (Temporal Difference, TD)
<!--END_SECTION:keyword-->
> 一种结合 **蒙特卡洛采样** 与 **贝尔曼方程** (**动态规划思想**) 的强化学习方法, 通过 **时序差分误差 (TD Error)** 逐步逼近价值函数, 从而在 **无需环境模型** 的情况下高效学习策略.

<!--START_SECTION:keyword-->
<!--keyword_info
name: '广义优势估计'
extra_url: false
-->
## 广义优势估计 (GAE)
<!--END_SECTION:keyword-->
> **广义优势估计** (Generalized Advantage Estimation, GAE) 是一种 **平衡偏差与方差** 的优势估计方法.

- GAE 的推导基于 **时序差分误差 (TD Error)**:
    $$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$
- **$k$-步优势估计** 可以表示为未来 $k$ 步 TD 误差的折扣和:
    $$ A_t^{(k)} = \delta_t + \gamma \delta_{t+1} + \dots + \gamma^{k-1} \delta_{t+k-1} = \sum_{l=0}^{k-1} \gamma^l \delta_{t+l} $$
- 当 $k \to \infty$ 时, 即为无限步长的优势估计:
    $$ A_t^{(\infty)} = \sum_{l=0}^{\infty} \gamma^l \delta_{t+l} $$
- GAE 引入了参数 $\lambda \in [0, 1)$, 将不同步长的估计进行 **指数加权平均**, 公式如下:
    $$ A_t^{\text{GAE}(\gamma, \lambda)}
        = (1-\lambda) (A_t^{(1)} + \lambda A_t^{(2)} + \lambda^2 A_t^{(3)} + \dots )
        = (1-\lambda) \sum_{k=1}^{\infty} \lambda^{k-1} A_t^{(k)}
    $$
- 将 **$k$-步估计** 代入并整理, 可得其紧凑形式:
    $$ A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l} $$
- 当 $\lambda=0$ 时, GAE **退化为单步优势估计**: $A_t = \delta_t$;
- 当 $\lambda\to 1$ 时, GAE **等价于无限步长的 TD 误差和**, 即 $A_t^{(\infty)}$;
- 通过调整 $\lambda$, 可以在 **偏差** 与 **方差** 之间取得平衡;
    - 小 $\lambda$ → 方差较小, 偏差较大;
    - 大 $\lambda$ → 偏差较小, 方差较大.

