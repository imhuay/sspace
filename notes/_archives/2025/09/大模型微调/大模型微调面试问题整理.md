大模型微调面试问题整理
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-13&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-17%2000%3A32%3A34&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-13 16:23:17
toc_title: 面试问题整理
top: false
draft: false
hidden: true
section_number: true
level: -1
tag: [llm]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: 大模型微调面试问题整理*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [1. 🏷️ **基础概念**](#1-️-基础概念)
    - [1.1. ✅ 什么是 **大模型微调**?](#11--什么是-大模型微调)
        - [1.1.1. ☑️ 为什么需要 **微调**?](#111-️-为什么需要-微调)
        - [1.1.2. ☑️ **微调** 与 **预训练** 的区别](#112-️-微调-与-预训练-的区别)
    - [1.2. ✅ 说明大模型微调的 **一般流程**](#12--说明大模型微调的-一般流程)
    - [1.3. ✅ 什么是 **灾难性遗忘**? 如何缓解?](#13--什么是-灾难性遗忘-如何缓解)
    - [1.4. 💡 如何设计高质量 SFT 数据? 如何保证 **覆盖率**/**多样性**/**一致性**?](#14--如何设计高质量-sft-数据-如何保证-覆盖率多样性一致性)
- [2. 🏷️ 训练稳定性](#2-️-训练稳定性)
    - [2.1. 有哪些提高训练稳定性的技巧?](#21-有哪些提高训练稳定性的技巧)
- [3. 🏷️ **参数高效微调 (PEFT, Parameter-Efficient Fine-Tuning)**](#3-️-参数高效微调-peft-parameter-efficient-fine-tuning)
    - [3.1. ✅ 比较 **全量微调** 和 **参数高效微调 (PEFT)**](#31--比较-全量微调-和-参数高效微调-peft)
    - [3.2. ✅ 介绍常见的 **PEFT** 技术](#32--介绍常见的-peft-技术)
        - [3.2.1. 💡 比较 **LoRA** / **Prefix Tuning** / **P-Tuning V2** / **Adapter** / **BitFit**](#321--比较-lora--prefix-tuning--p-tuning-v2--adapter--bitfit)
    - [3.3. 🔥 **LoRA (Low-Rank Adaptation)**](#33--lora-low-rank-adaptation)
        - [3.3.1. ✅ 什么是 LoRA? 它解决了什么问题? 适用什么场景?](#331--什么是-lora-它解决了什么问题-适用什么场景)
        - [3.3.2. ✅ 与全参微调相比, LoRA 的 **表达上限** 如何?](#332--与全参微调相比-lora-的-表达上限-如何)
        - [3.3.3. ✅ LoRA 的参数量如何计算? 与原参数量的比例?](#333--lora-的参数量如何计算-与原参数量的比例)
        - [3.3.4. ✅ LoRA 一般作用于哪些层?](#334--lora-一般作用于哪些层)
        - [3.3.5. 💡 写出 LoRA 的 **数学形式**, 并解释各参数的含义与约束](#335--写出-lora-的-数学形式-并解释各参数的含义与约束)
            - [3.3.5.1. ✅ 为何需要 **缩放项** `α/r`? 去掉会怎样?](#3351--为何需要-缩放项-αr-去掉会怎样)
            - [3.3.5.2. ✅ 为什么常将 `A` 正态初始化, `B` 初始化为 `0`? 如果不这么做会怎么样?](#3352--为什么常将-a-正态初始化-b-初始化为-0-如果不这么做会怎么样)
            - [3.3.5.3. ✅ 如何选择 `r` (Rank)? 不同任务/数据规模下的建议是什么?](#3353--如何选择-r-rank-不同任务数据规模下的建议是什么)
            - [3.3.5.4. 💡 如果希望逼近全参微调效果, 除了增大 r 还能做什么?](#3354--如果希望逼近全参微调效果-除了增大-r-还能做什么)
        - [3.3.6. 🔖 LoRA 的变体与对比](#336--lora-的变体与对比)
            - [3.3.6.1. QLoRA/AdaLoRA/DoRA 的核心思路是什么?](#3361-qloraadaloradora-的核心思路是什么)
<!--END_SECTION:toc-->

---

## 1. 🏷️ **基础概念**

### 1.1. ✅ 什么是 **大模型微调**?
> - 大模型微调是利用 **特定数据** 对 **通用预训练模型** 进行 **再训练**, 以高效地让其 **适配下游任务** 的关键技术;

#### 1.1.1. ☑️ 为什么需要 **微调**?
> - 高适配, 低成本, 高效率

#### 1.1.2. ☑️ **微调** 与 **预训练** 的区别
> - 训练目标不同, 数据规模不同, 训练成本不同, 模型能力侧重不同

<details><summary><b>详述</b></summary>

| 维度 | 预训练 | 微调 |
| --- | --- | --- |
| 训练目标不同 | 学习通用语言模式与知识表示, 构建广泛的基础能力 | 适配特定任务或领域, 提升在该场景下的性能 |
| 数据规模不同 | 使用大规模、跨领域的通用数据集 | 使用较小规模、领域相关或任务特定的数据集 |
| 训练成本不同 | 需要极高的计算资源与时间成本 | 在已有模型基础上进行, 成本显著降低 |
| 模型能力侧重不同 | 获得广泛的通用能力 | 强化特定任务能力, 可能牺牲部分通用性 |

</details>

### 1.2. ✅ 说明大模型微调的 **一般流程**
> 1. **需求分析**: 确定 **输入输出** 形式, 评价指标;
> 2. **数据准备/预处理**: 收集高质量数据, 清洗, 去噪, 格式化; 数据集划分;
> 3. **模型选择**: 考虑因素包括 **模型能力** (通用/领域), **参数量** 和 **计算资源**;
> 4. **微调策略**: 全参数微调 / 参数高效微调 (LoRA 等方法); 偏好学习 (RL)
> 5. **训练与监控**: 训练环境, 超参数 (学习率、批大小、优化器), 验证集指标变化;
> 6. **模型评估**: 测试集性能评估 (过拟合/欠拟合), case 分析;
> 7. **部署与持续优化**: 模型部署; 收集反馈数据, 增量微调/再训练;

<details><summary><b>备注</b></summary>

- 大模型把所有任务统一到了 **生成任务**, 因此确定 **输入输出形式** 比 **任务类型** 更重要;

</details>

### 1.3. ✅ 什么是 **灾难性遗忘**? 如何缓解?
> - **含义**: 模型在新数据上学习时, 覆盖了之前学到的知识;  
> - **缓解方法**: PEFT, 混合训练数据, 更小的学习率, 正则化约束, 参数隔离, 渐进式微调

### 1.4. 💡 如何设计高质量 SFT 数据? 如何保证 **覆盖率**/**多样性**/**一致性**?
> - **高质量**: 多样性, 覆盖率; 一致性;
>> [构建高质量SFT数据](./构建高质量SFT数据.md)

<details><summary><b>简述</b></summary>

- **覆盖率**:
    - **构建数据分类体系** (taxonomy)
    - **覆盖率取决于分类体系的完整度**;
- **多样性**:
    - **指令多样性**: 模板参数化, 同义改写, 风格变换, 语言映射, 噪声与错别字扰动;
    - **深度多样性**: 直接回答, 思维链, 深度思考;
    - **风格多样性**: 角色, 语气;
    - **上下文多样性**: 单轮, 多轮;
- **一致性**:
    - 多人/多 Agent/多 Prompt

</details>

## 2. 🏷️ 训练稳定性

### 2.1. 有哪些提高训练稳定性的技巧?
> - 数据质量, 模型结构, 初始化策略, 优化器, 训练策略, 调试与监控


## 3. 🏷️ **参数高效微调 (PEFT, Parameter-Efficient Fine-Tuning)**

### 3.1. ✅ 比较 **全量微调** 和 **参数高效微调 (PEFT)**
> - 参数更新范围, 资源成本/训练速度, 数据需求, 灾难性遗忘, 适用场景
> - **总结**: 全量微调追求极致性能但成本高, PEFT 以低成本实现高适配性并保留通用能力 (减轻灾难性遗忘);

<details><summary><b>详述</b></summary>

| 对比维度 | 全量微调 | 参数高效微调 (PEFT) |
|---|---|---|
| **参数更新范围** | **全部参数** | **少量新增或选定的参数** |
| **资源成本/训练速度** | 计算量与显存占用**高**, **训练慢** | 计算量与显存占用**低**, **训练快** |
| **数据需求** | 需要**大规模数据**以避免过拟合 | 对数据量要求相对较低 |
| **灾难性遗忘** | **严重** | **较轻** |
| **适用场景** | **资源充足**, 追求极致性能 | **资源有限**, 多任务部署或快速迭代 |

</details>

### 3.2. ✅ 介绍常见的 **PEFT** 技术
> LoRA/QLoRA, Adapter, Prefix/Prompt Tuning, P-Tuning V1/V2, BitFit
>> [PEFT 整理](./PEFT.md)

#### 3.2.1. 💡 比较 **LoRA** / **Prefix Tuning** / **P-Tuning V2** / **Adapter** / **BitFit**
> 几个关键维度: 表示能力, 推理延迟, 可训练参数量
> - **表示能力**: Adapter ≥ LoRA ≥ P‑Tuning V2 > Prefix Tuning > BitFit
> - **推理延迟**: LoRA ≈ BitFit < P‑Tuning V2 ≈ Prefix Tuning < Adapter
> - **参数量**: BitFit < P‑Tuning V2 ≈ Prefix Tuning < LoRA < Adapter

### 3.3. 🔥 **LoRA (Low-Rank Adaptation)**
> [LoRA](./LoRA.md)

#### 3.3.1. ✅ 什么是 LoRA? 它解决了什么问题? 适用什么场景?
> - **LoRA** 是一种当前非常流行的 **参数高效微调 (PEFT)** 技术;  
> - **优势/解决的问题**: 全参数微调成本高, 多任务存储冗余, 部署灵活性不足, 减少灾难性遗忘;
> - **适用场景**: 资源受限, 快速迭代, 多任务部署, 避免灾难性遗忘;

#### 3.3.2. ✅ 与全参微调相比, LoRA 的 **表达上限** 如何?
> - **表达上限**: LoRA 的权重更新被约束在了一个 **低秩子空间** 内, 其复杂性受限于 **秩 ($r$)** 的大小;
> - 对于 **与预训练分布差异大** 的任务, 表达上限可能会低于全参微调;
> - 从数学角度看, 当 $r$ 设置的足够大时, 模型完全有可能在新任务上重塑特征空间 (虽然这与 LoRA 设计的初衷不符);

#### 3.3.3. ✅ LoRA 的参数量如何计算? 与原参数量的比例?
> - **LoRA 的参数量**: $r(d_{in}+d_{out})$
> - **参数比**: $\dfrac{r(d_{in}+d_{out})}{d_{in} \times d_{out}}$; 通常能压缩到百分之一到千分之一的量级;

#### 3.3.4. ✅ LoRA 一般作用于哪些层?
> - 大部分线性层 `nn.Linear`:
>   - 注意力层中的 $W_q, W_k, W_v, W_o$
>   - MLP 层中的 $W_{up}, W_{down}, W_{gate}$

#### 3.3.5. 💡 写出 LoRA 的 **数学形式**, 并解释各参数的含义与约束
> $h = Wx + \dfrac{\alpha}{r}B(Ax)$
>> [LoRA](./LoRA.md#基础概念)

##### 3.3.5.1. ✅ 为何需要 **缩放项** `α/r`? 去掉会怎样?
> - **作用**: 稳定训练并控制更新幅度; 去掉会让更新量随秩变化而失控, 增加训练不稳定与调参难度;

##### 3.3.5.2. ✅ 为什么常将 `A` 正态初始化, `B` 初始化为 `0`? 如果不这么做会怎么样?
> - **`A` 正态初始化**: 保证了各方向的更新潜力均衡, 避免某些方向先天缺乏梯度信号;
> - **`B` 初始化为 `0`**: 在 **训练开始** 时保持与原模型一致, 确保模型从 **安全可控** 的状态开始学习;
> - 不这么做: **梯度爆炸**, **训练震荡**, **收敛困难**, **灾难性遗忘**;

##### 3.3.5.3. ✅ 如何选择 `r` (Rank)? 不同任务/数据规模下的建议是什么?
> - 简单任务/小数据从 r=4/8 开始, 中等任务从 r=16/32 开始, 复杂任务/大数据可尝试 r=64/128, 并配合缩放项与验证集监控动态调整;

##### 3.3.5.4. 💡 如果希望逼近全参微调效果, 除了增大 r 还能做什么?
> - [改进 LoRA 结构](./LoRA.md#结构改进); **多位置接入** (MLP 层); **逐步解冻**/**混合训练**; **超参优化**(学习率/eopch 等); 更先进的算法 (DoRA 等);

#### 3.3.6. 🔖 LoRA 的变体与对比

##### 3.3.6.1. QLoRA/AdaLoRA/DoRA 的核心思路是什么?
> - **QLoRA**: 低比特量化加载基座模型; LoRA 部分使用全精度 (FP16/BF16);
> - **AdaLoRA**: 在训练过程中 **动态调整** 每层 LoRA 的秩 $r$;
>   - 初期用较高秩训练, 通过奇异值分布或梯度范数评估各层重要性;
>   - 对重要层保留较高秩, 对不重要层降低秩;
> - **DoRA**: 将原始权重分解为幅度与方向 ($W_0 = m \frac{V}{\|V\|_c}$), 幅度由独立可训练参数控制 ($m$), LoRA 仅作用于方向部分的更新 ($\Delta V$), 避免低秩更新浪费在幅度缩放上;

<details><summary><b>显存减少量计算</b></summary>

- **计算公式**:
    - 显存占用 (字节) = `参数量 * 字节数 (byte)`
- 主流精度的字节数:
    - `FP32 (32-bit)`: `4 bytes`
    - `FP16 (16-bit)`: `2 bytes`
    - `BF16 (16-bit)`: `2 bytes`
    - `INT8 (8-bit)`: `1 byte`
    - `INT4 (4-bit)`: `0.5 byte5`
        - 相比 `FP32` 降低 8 倍,
        - 相比 `FP16/BF16` 降低 4 倍,

</details>


---

<details><summary><b>参考资料</b></summary>

<!-- omit in toc -->
<!-- ## 📍 参考资料 -->
- [大模型微调模拟面试 - DeepSeek](https://chat.deepseek.com/a/chat/s/7342a1fe-2d3f-487f-aa41-bec5af706268)
- [大模型微调模拟面试 - Copilot](https://copilot.microsoft.com/chats/BmUFVXkQA5qt96LBUEpJK)

</details>
