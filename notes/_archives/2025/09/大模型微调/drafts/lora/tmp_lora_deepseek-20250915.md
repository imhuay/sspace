```md
我正在准备有关 LoRA 的面试, 请帮我尽可能详细的罗列相关知识点; 
然后准备一份尽可能全的面试问题清单, 注意清单中所有问题的答案要尽可能在知识点中直接找到或推理得到, 并附上可能得追问;
```
> https://chat.deepseek.com/a/chat/s/f6f751a2-66a6-4dc9-b117-f8e8cc5fe4d2

好的，准备LoRA的面试需要从**核心概念、数学原理、实现细节、应用实践**以及**与其他方法的对比**等多个维度进行深入理解。下面我为你梳理了一份极其详细的知识点清单，并基于此设计了一套全面的面试问题与追问。

---

### **第一部分：LoRA 详细知识点梳理**

#### **1. 背景与核心思想 (Background & Core Idea)**
*   **问题定义**： 全参数微调（Full Fine-Tuning）大型预训练模型（如GPT-3, LLaMA）成本极高，需要为每个下游任务存储和部署一份独立的模型副本，计算和存储开销巨大。
*   **核心假设**： 论文的核心假设是**模型在适应下游任务时，权重更新（ΔW）具有内在的低秩特性（Low Intrinsic Rank）**。即一个完整的权重更新矩阵ΔW（尺寸为`d x k`）可以用一个低秩矩阵`B * A`（其中`B`是`d x r`, `A`是`r x k`, 且 `r << min(d, k)`）来有效地近似。
*   **核心思想**： 冻结预训练模型的原始权重（W0），并不直接计算其更新ΔW，而是**通过一个旁路（Side Path）来学习低秩的权重更新ΔW = B * A**。在推理时，将低秩更新与原始权重合并：`h = W0*x + ΔW*x = W0*x + B*A*x`。

#### **2. 方法细节 (Methodology Details)**
*   **参数化**：
    *   **原始权重**： `W0` (形状 `d x k`)，被冻结，不参与梯度更新。
    *   **适配器权重**： `A` (形状 `r x k`) 和 `B` (形状 `d x r`)。
    *   `A` 通常使用随机高斯初始化（均值0，标准差某值）。
    *   `B` 通常初始化为全零矩阵。**这样做的目的是确保训练开始时ΔW = B*A = 0，因此旁路不起作用，整个模型的行为与原始预训练模型完全一致，保证了训练的稳定性。**
*   **秩 (Rank - r)**： 最重要的超参数。它控制了适配器的参数量和表征能力。`r` 越小，参数量越少（`d*r + r*k`），但能力可能越弱；`r` 越大，能力越强，但参数量越多。通常 `r` 取值在4-64之间，极小于 `d` 和 `k`（通常是1024+）。
*   **缩放因子 (Scaling Factor - α/r)**： 在训练过程中，ΔW 会被缩放：`ΔW = (α/r) * B * A`。其中 `α` 是一个超参数，可以理解为学习率。**固定 `α` 和 `r` 的比例关系，可以在改变 `r` 时减少重新调整学习率的需求**。
*   **应用位置**： LoRA通常应用于Transformer架构中的**自注意力机制（Self-Attention）的查询（Q）、键（K）、值（V）、输出（O）投影矩阵**，以及**前馈网络（FFN）的两个线性层**。论文发现只适配Q和V矩阵效果就不错。
*   **推理部署**： 训练完成后，可以将低秩更新与原始权重直接合并：`W_new = W0 + (α/r) * B * A`。合并后，**模型架构和参数量与原始预训练模型完全一致**，不存在任何推理延迟（Inference Latency）。

#### **3. 优势与特性 (Advantages & Properties)**
*   **大幅降低显存需求**： 只需要优化极少的适配器参数，大大减少了训练时的显存占用，使得在消费级GPU上微调大模型成为可能。
*   **减少存储开销**： 对于多个下游任务，只需保存不同的微型LoRA适配器（几MB到几十MB），而不是整个模型（几GB到几十GB）。
*   **无推理延迟**： 权重合并后，推理速度与原始模型完全相同。
*   **模块化**： 不同的LoRA适配器可以在推理时动态加载、切换或组合，提供了极大的灵活性。
*   **训练效率高**： 由于大部分权重被冻结，反向传播只需要计算适配器参数的梯度，训练更快。
*   **避免灾难性遗忘 (Catastrophic Forgetting)**： 由于原始权重被冻结，模型在很大程度上保留了预训练中获得的通用知识。

#### **4. 变体与相关技术 (Variants & Related Techniques)**
*   **QLoRA**： 将预训练模型权重量化为4-bit（使用NF4），并添加一小组可学习的“低精度补偿”参数，进一步降低微调所需的显存。它使用**Paged Optimizers**等技术来防止GPU内存不足错误。
*   **AdaLoRA**： 动态地分配参数预算给最重要的权重矩阵和矩阵内最重要的奇异值，而不是使用固定的秩 `r`。它会**奇异值分解（SVD）** 并基于重要性评分修剪不重要的奇异值。
*   **LoRA+**： 为适配器 `A` 和 `B` 设置不同的学习率（`η_B >> η_A`），理论上可以加速收敛。
*   **相关技术对比**：
    *   **Adapter Tuning**： 在Transformer层中插入小型神经网络模块。会增加模型深度，引入推理延迟。
    *   **Prompt Tuning / Prefix Tuning**： 在输入中添加可训练的软提示（Soft Prompts）。不修改模型内部权重，但会占用输入序列长度。
    *   **BitFit**： 只微调模型中的偏置（Bias）参数。参数量极少，但性能通常弱于LoRA。

---

### **第二部分：面试问题清单与答案指引**

以下问题均基于上述知识点，并附有追问。

#### **Category 1: 基础概念与动机**

1.  **问题： 请简要介绍LoRA方法，并解释它主要是为了解决什么问题？**
    *   **答案指引**： LoRA是一种参数高效的微调方法。它要解决的是**全参数微调大模型时计算和存储成本过高**的问题。通过假设权重更新具有低秩特性，它使用一个低秩分解的旁路来学习更新，从而大幅减少可训练参数量。
    *   **追问**： 为什么全参数微调的成本会那么高？（考察对问题背景的理解）

2.  **问题： LoRA的核心假设是什么？这个假设为什么是合理的？**
    *   **答案指引**： 核心假设是**模型在适应下游任务时的权重更新ΔW具有内在的低秩特性**。合理性在于：过度参数化的模型本身存在信息冗余，其内在维度（Intrinsic Dimension）可能很低，即不需要修改所有参数就能有效学习新任务。
    *   **追问**： 你能想到任何支持或反对这个假设的观察吗？

#### **Category 2: 方法细节**

3.  **问题： 请描述一下LoRA适配器（Adapter）的具体结构和初始化方式。**
    *   **答案指引**： 结构是两个线性层 `A` 和 `B`，没有非线性激活函数。`A`（`r x k`）用随机高斯初始化，`B`（`d x r`）用零初始化。初始化方式确保了训练开始时旁路输出为零。
    *   **追问**： 为什么 `B` 要用零初始化，而不是像 `A` 一样用随机初始化？（考察对训练稳定性的理解）

4.  **问题： LoRA中的秩（r）和缩放因子（α）分别是什么？它们的作用是什么？**
    *   **答案指引**： `r` 是低秩矩阵的秩，控制适配器的容量和参数量。`α` 是一个缩放超参数，与 `r` 共同作用 `ΔW = (α/r) * B*A`。固定 `α/r` 的比例可以在调整 `r` 时减少重新调参的需要。
    *   **追问**： 如果我增大 `r`，模型的拟合能力会如何变化？训练速度呢？

5.  **问题： 在Transformer模型中，LoRA通常被应用到哪些层或结构上？为什么？**
    *   **答案指引**： 主要应用于Self-Attention的Q, K, V, O投影矩阵。因为这些矩阵是密集层，参数量大，且对任务适配至关重要。有时也会应用于FFN层。
    *   **追问**： 如果只让你选两个矩阵应用LoRA，你会选哪两个？根据论文结论，通常是Q和V。

#### **Category 3: 训练与推理**

6.  **问题： LoRA在训练和推理阶段是如何工作的？**
    *   **答案指引**： **训练**： 冻结 `W0`，只更新 `A` 和 `B`。前向传播为 `h = W0*x + (α/r)*B*A*x`。**推理**： 将 `A` 和 `B` 合并到 `W0` 中：`W_merged = W0 + (α/r)*B*A`，然后用 `W_merged` 进行推理，无任何额外开销。
    *   **追问**： 推理时进行权重合并有什么好处？（无延迟、部署方便）

7.  **问题： 使用LoRA微调相比全参数微调，大约能节省多少显存？为什么？**
    *   **答案指引**： 显存节省巨大。节省主要来自**不存储优化器状态（如Momentum, Variance）对于原始参数**。假设原始参数为 `Φ`，LoRA参数为 `Φ_LoRA`（`|Φ_LoRA| << |Φ|`），显存占用从 `O(4|Φ|)` 或 `O(8|Φ|)`（Adam优化器）降低到 `O(4|Φ_LoRA| + |Φ|)`，其中 `|Φ|` 是冻结参数的存储开销。
    *   **追问**： 为什么优化器状态是显存占用的大头？（解释Adam优化器需要为每个参数存储一阶矩、二阶矩和梯度）

#### **Category 4: 高级话题与对比**

8.  **问题： LoRA和Adapter Tuning、Prompt Tuning有什么区别？各自的优缺点是什么？**
    *   **答案指引**：
        *   **LoRA**： 优 - 无推理延迟，性能好。缺 - 修改模型内部权重。
        *   **Adapter**： 优 - 模块化。缺 - 增加层深，引入推理延迟。
        *   **Prompt/Prefix Tuning**： 优 - 不修改权重，无延迟。缺 - 占用序列长度，性能可能不稳定。
    *   **追问**： 在什么场景下你会优先选择Prompt Tuning而不是LoRA？（例如，没有模型修改权限时）

9.  **问题： 你了解QLoRA吗？它是如何进一步降低显存需求的？**
    *   **答案指引**： QLoRA是LoRA的量化版本。它**将预训练模型量化为4-bit**，但计算时反量化为BF16/FP16。同时，它引入了**Paged Optimizers**（将优化器状态转移到CPU内存）和**Double Quantization**（对量化常数再次量化）等技术。可训练参数仍然是LoRA适配器。
    *   **追问**： 4-bit量化通常会导致精度下降，QLoRA是如何缓解这个问题的？（使用NF4量化格式和Block-wise Quantization）

10. **问题： AdaLoRA相比于原始LoRA有什么改进？它是如何做到的？**
    *   **答案指引**： AdaLoRA**动态分配参数预算**，而不是给所有矩阵分配固定的秩 `r`。它通过对增量矩阵ΔW进行**参数化的SVD**，并根据重要性评分（基于梯度的敏感度）迭代地修剪不重要的奇异值，将参数重新分配到更重要的地方。
    *   **追问**： 这种动态分配参数的理念有什么好处？（在总参数量不变的情况下，可能获得更好的性能）

#### **Category 5: 实践与应用**

11. **问题： 在实际项目中，你是如何选择LoRA的超参数（如rank r, alpha, target modules）的？**
    *   **答案指引**： 这是一个实践性问题。
        *   `r`： 通常从8、16、32开始尝试。任务越复杂，可能需要越大的 `r`。
        *   `alpha`： 通常设置为 `r` 的1-2倍，如 `r=16, alpha=32`。
        *   `target_modules`： 通常是 `q_proj, v_proj` 或 `query, value`。可以尝试添加到 `k_proj, o_proj` 和FFN层。
        *   最终选择依赖于在验证集上的实验。
    *   **追问**： 如果模型出现 overfitting，你应该调整哪些LoRA超参数？（减小 `r`，增大dropout，或者收集更多数据）

12. **问题： 多个LoRA适配器可以组合使用吗？例如，结合一个数学能力的适配器和一个中文能力的适配器。**
    *   **答案指引**： **可以**。这是LoRA模块化的优势之一。在推理时，可以将多个LoRA适配器的增量权重同时加到原始权重上：`W_merged = W0 + ΔW_math + ΔW_chinese`。但需要注意适配器之间的兼容性，可能需要调整缩放因子。

**祝你面试顺利！** 请务必结合你自身的项目经验来准备，因为面试官很可能让你分享具体的实践案例。