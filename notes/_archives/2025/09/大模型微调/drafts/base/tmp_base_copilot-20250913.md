```md
我正在准备有关 大模型微调 的面试, 请帮我尽可能详细的罗列相关知识点;
然后准备一份尽可能全的面试问题清单, 注意清单中所有问题的答案要尽可能在知识点中直接找到或推理得到, 并附上可能得追问;
```
> https://copilot.microsoft.com/chats/8MR1M88y29mMgBdwbQ2Ae

# 大模型微调知识图谱

下面这份框架力求系统、可查、可推。后面的面试题清单按相同结构组织，所有答案都能在本节直接找到或由此推理得到。

---

## 任务与数据

#### 任务类型与目标

- **SFT 指令微调:** 用高质量指令—回答样本让模型学会“怎么回答”。目标是最小化监督式交叉熵损失。  
- **偏好对齐:** 让模型更“像人”，常见范式包括 RLHF、DPO、ORPO、KTO、RLAIF。目标是对齐人类偏好、减少有害/幻觉输出。  
- **领域自适应:** 用领域语料做继续预训练或领域 SFT，提升专业知识与术语风格。  
- **多轮对话:** 严格遵循 chat 模板（system/user/assistant 角色、BOS/EOS、分隔 token），保持上下文一致性。  
- **工具调度/函数调用:** 微调模型学会结构化输出与 API/函数调用参数填充。  

#### 数据设计与治理

- **数据格式:**  
  - **SFT 样本:** {instruction,input,output} 或对话 turn 列表；统一 chat 模板、确保 EOS。  
  - **偏好数据:** 成对比较(A vs B)或全序排列；RM 训练用打分/排序，DPO/ORPO 直接用偏好对。  
- **数据来源与质量:**  
  - **高质优先:** 权威文档/手册/指南/源码/FAQ；拒绝低质爬虫噪声。  
  - **清洗与去重:** 语义/指纹去重、脏词/广告/版权文本剔除、格式正则化、标点/空白统一。  
  - **污染控制:** 避免测试集泄露；与评测集严格隔离；记录来源与版本。  
  - **多样化与覆盖:** 同一知识点的多视角问法、反例/边界条件、长短混合、结构化与自然文混合。  
- **规模与配比（经验启发）:**  
  - **指令 SFT:** 几千—几万条高质样本可显著见效；复杂综合能力可到几十万。  
  - **偏好对齐:** 成对偏好数万—几十万起步更稳；领域窄时“精标+强约束”优于纯规模堆叠。  
  - **继续预训练:** 视领域覆盖与重复度，常以数十亿 token 计，重质不重“无效重复”。  
- **安全与合规:**  
  - **敏感场景标注:** 有害/非法/歧视/隐私；拒答与解释模板。  
  - **对抗与越狱:** 添加红队样本、策略样本、诱骗防护样本；宪法式原则约束。  
  - **许可与版权:** 模型、数据、代码三方许可相容性审查（如 Llama、Qwen、数据集条款）。  

---

## 方法与优化

#### 全量与参数高效微调（PEFT）

- **全量微调:** 训练全部参数，效果强、成本高、易灾难性遗忘，需要大算力与谨慎学习率。  
- **PEFT 家族:** 只训练小量增量参数，保留底座权重稳定性，显著降低显存/成本。  
  - **LoRA/QLoRA/AdaLoRA/DoRA/LoRA-FA:** 在线性层插入低秩适配器；QLoRA 结合 4bit NF4 权重量化实现“笔记本也能训”。  
  - **IA3:** 缩放注意力/FFN 通道的激活，不引入矩阵乘。  
  - **Prefix/Prompt Tuning, P-Tuning v2:** 学习可训练前缀/虚拟 token，极低参数改动。  
  - **BitFit:** 仅调 bias，极小开销但增益有限。  
  - **Adapter/AdapterFusion:** 模块化适配、可多领域融合。  

| 方法 | 训练参数比例 | 显存/成本 | 优点 | 局限 |
|---|---:|---:|---|---|
| LoRA | 0.1%–2% | 低 | 效果稳、生态成熟 | 秩超参敏感 |
| QLoRA | 0.1%–2% | 极低 | 消耗小、可大模型 | 量化误差/实现细节复杂 |
| IA3 | <0.1% | 极低 | 无矩阵插入、轻量 | 表达力有限 |
| Prefix/Prompt | <0.1% | 极低 | 数据/任务切换快 | 长上下文成本上升 |
| BitFit | ~0.01% | 极低 | 实施最简 | 提升受限 |

> 选择建议：数据少/算力紧→Prompt/Prefix/IA3；中等数据→LoRA/QLoRA；极致效果/风格塑形→全量或DoRA + 合理秩。

#### 对齐范式与目标函数

- **SFT 交叉熵:**  
  - **目标:** 最小化生成答案 token 的负对数似然：  
    $$
    \mathcal{L}_{\text{SFT}}=-\sum_{t}\log p_\theta(y_t\mid y_{<t},x)
    $$
- **RLHF（RM+PPO/SFT+KL）:**  
  - **RM 训练:** 拟合偏好分数或成对比较。  
  - **PPO 阶段:** 在 KL 约束下最大化奖励期望：  
    $$
    \max_\theta \mathbb{E}\_{x,y\sim\pi_\theta}\left \lbrack r(x,y)-\beta \cdot \text{KL}\big(\pi_\theta(\cdot|x)\,\|\,\pi_{\text{ref}}(\cdot|x)\big)\right \rbrack
    $$
- **DPO（无 RM 直接偏好优化）:**  
  - **目标:** 让偏好答案优于不偏好答案，相对参考模型：  
    $$
    \mathcal{L}_{\text{DPO}}=-\log \sigma\!\Big(\beta\big \lbrack \log \pi_\theta(y^+|x)-\log \pi_\theta(y^-|x)-\log \pi_{\text{ref}}(y^+|x)+\log \pi_{\text{ref}}(y^-|x)\big \rbrack \Big)
    $$
- **ORPO/KTO/RLAIF:**  
  - **ORPO:** 在单次优化中联合监督与偏好正则，减少两阶段复杂度。  
  - **KTO:** 仅需“好/坏”标记的对比式优化，适合弱标注。  
  - **RLAIF:** 以 AI 代人工打标，成本更低但需质量把控与自举策略。  

#### 训练配方与稳定性

- **学习率与调度:**  
  - **范围经验:** 全量 $1\mathrm{e}{-5}\sim5\mathrm{e}{-5}$，LoRA $5\mathrm{e}{-5}\sim2\mathrm{e}{-4}$；warmup 1%–5%，余弦/线性衰减。  
  - **AdamW/Adafactor:** β1/β2/weight decay 合理搭配；梯度裁剪 0.5–1.0。  
- **批量与累积:** 有效 batch 规模影响收敛与泛化；梯度累积平衡吞吐与显存。  
- **混合精度:** BF16 首选，FP16 次之；开启 loss scaling 防止下溢。  
- **高效注意力与长上下文:** FlashAttention、窗口注意力、RoPE 多项式缩放/NTK-aware scaling。  
- **序列打包与填充:** Pack 长样本提升吞吐；对齐损失 mask，避免跨样本泄漏。  
- **正则与防遗忘:** KL 正则、L2/SP 正则、策略拒答模板；回放少量原域数据。  

---

## 低精度与内存优化

- **权重量化（训练/推理）:**  
  - **QLoRA:** 4-bit NF4 + 双重量化 + 分页优化器，极大节省显存。  
  - **8-bit/FP8:** 训练/推理折中方案；需校准与算子支持。  
  - **GPTQ/AWQ:** 侧重推理前量化（PTQ），训练少改或不改权重。  
- **激活与优化器内存:**  
  - **梯度检查点:** 以算换存；层粒度权衡开销。  
  - **ZeRO/FSDP:** 参数/优化器/梯度切分；重组通信与重计算策略。  
  - **Paged Optimizer/KV Cache 优化:** 减少碎片与 OOM；与长上下文共用内存策略。  

---

## 评测、风险与上线

- **自动评测:**  
  - **困惑度/交叉熵:** 语言流畅度与过拟合信号。  
  - **任务指标:** EM/F1/ROUGE/BLEU/CodePass@k/Math acc；多 turn 一致性。  
  - **对齐评测:** 开放问答遵循度、拒答合规率、无害性/有用性人评。  
  - **综合基准:** MT-Bench、AlpacaEval、HELM 风格离线套件（可自建领域版）。  
- **人评与红队:**  
  - **多维标尺:** 正确性、相关性、风格一致性、可执行性、安全性。  
  - **对抗样本:** 注入/转义/越狱/语义绕过；监测拒答边界与幻觉。  
- **上线与回归:**  
  - **提示模板锁定:** 训练/评测/上线严格一致；系统提示不可随意漂移。  
  - **灰度与 A/B:** 监控关键业务指标、反馈采集闭环。  
  - **漂移与修复:** 数据分布变更报警、快速小步回归微调。  

---

## 工程与可复现

- **实验追踪:** 记录数据切分、样本版本、超参、随机种子、提交哈希；可复现实验卡片。  
- **检查点与合并:** LoRA 合并/分离、权重平均（EMA/SWA）、多适配器融合。  
- **确定性与随机性:** 种子固定但算子非完全确定；统计多次均值与置信区间。  
- **部署与吞吐:**  
  - **推理引擎:** 支持 KV Cache、连续批处理、张量并行、Speculative Decoding。  
  - **延迟与成本:** 批大小、并发、EOS 预测、Top-p/温度对吞吐与质量的影响。  
  - **兼容性:** 合并 LoRA 后的数值漂移与量化再校准；Tokenizer/模板版本一致。  

---

## 常见坑与快速排查

- **模板不一致:** 训练与推理 chat 模板/BOS/EOS/系统提示不一致→行为异常。  
- **Tokenizer 不匹配:** 词表、特殊 token ID 差异→损失飘、输出乱码。  
- **数据泄漏与评测失真:** 训练集污染评测集→指标虚高。  
- **过拟合/灾难性遗忘:** 训练步过多、学习率大、原域回放不足→泛化差/性格突变。  
- **偏好数据噪声:** 生成式弱标注误导→对齐错位；需加置信度/过滤器。  
- **量化/精度陷阱:** 4bit 量化校准不良→梯度不稳；优先 BF16。  
- **损失掩码错误:** pack 序列跨样本监督泄漏→指标异常提升但线上退化。  

---

# 关键公式与配置速查

- **SFT 交叉熵:**  
  $$
  \mathcal{L}_{\text{SFT}}=-\sum_{t}\log p_\theta(y_t\mid y_{<t},x)
  $$
- **KL 正则 SFT（稳定风格/避免漂移）:**  
  $$
  \mathcal{L}=\mathcal{L}_{\text{SFT}}+\lambda\ \text{KL}\big(\pi_\theta(\cdot|x)\,\|\,\pi_{\text{ref}}(\cdot|x)\big)
  $$
- **PPO 目标（RLHF）：**  
  $$
  \max_\theta \mathbb{E}\big \lbrack \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t-\beta\ \text{KL}(\pi_\theta\|\pi_\text{ref})\big \rbrack
  $$
- **DPO 损失：**  
  $$
  \mathcal{L}_{\text{DPO}}=-\log \sigma\!\Big(\beta\big \lbrack \log \pi_\theta(y^+|x)-\log \pi_\theta(y^-|x)-\log \pi_{\text{ref}}(y^+|x)+\log \pi_{\text{ref}}(y^-|x)\big \rbrack \Big)
  $$
- **经验超参（起点）:**  
  - **LoRA:** rank 8–64，α=16–64，dropout 0.05–0.1；lr $5\mathrm{e}{-5}\sim2\mathrm{e}{-4}$。  
  - **SFT:** lr $1\mathrm{e}{-5}\sim5\mathrm{e}{-5}$，warmup 1%–5%，cosine/linear decay；clip 1.0。  
  - **Batch:** 有效 256–2k tokens/step（依资源扩放）；epochs 1–3（大模型低 epoch 更稳）。  
  - **精度:** BF16 + Grad Checkpoint；QLoRA 用 4bit NF4 + Paged Optimizer。  

---

# 实战清单与流程

#### 数据到位

- **定义场景:** 明确任务边界、风格与拒答策略。  
- **数据准备:** 高质 SFT + 多样化难例 + 安全红队 + 偏好对；严格模板化。  
- **版本化:** 样本来源、清洗规则、切分与时间戳；评测集冻结。  

#### 训练到位

- **小步快跑:** 先 LoRA/QLoRA 小规模试配方；观察 loss/困惑度/过拟合信号。  
- **对齐阶段:** 先 SFT，再 DPO/ORPO；简单双阶段可替 PPO，稳定高效。  
- **稳定性:** 学习率扫描、warmup、梯度裁剪、正则；选择 FlashAttention。  

#### 评测到位

- **离线套件:** 任务指标+对齐指标+安全评测；多种提示模板鲁棒性测试。  
- **人评闭环:** 纠错与风格纠偏；采集失败案例回灌数据集。  

#### 上线到位

- **模板锁:** 训练/评测/上线模板一致；系统提示配置化。  
- **性能优化:** KV Cache、批处理、Top-p/温度调参；灰度/回滚机制。  

---

# 面试问题清单（含可能追问）

以下按知识图谱结构组织，答案可在上文直接查到或由其推理获得。

### 任务与数据

- **基础：** 什么是指令微调（SFT）？与领域继续预训练的差别是什么？  
  - 追问：多轮对话数据应如何组织角色与特殊 token？  
- **数据质量：** 如何设计高质量 SFT 数据？如何保证多样化与覆盖率？  
  - 追问：你如何避免评测污染？如何做语义去重与来源追溯？  
- **规模：** 指令微调与偏好对齐分别需要多大规模数据？为什么“质>量”？  
  - 追问：在小数据场景，如何通过模板化与难例采样弥补？  
- **偏好数据：** 成对偏好、全序、RM 打分各自适用什么方法（RLHF、DPO、KTO）？  
  - 追问：如何控制弱标注/AI 代标的偏差？  
- **安全与合规：** 如何构建拒答与合规样本？如何做红队与越狱防护？  
  - 追问：上线后如何持续监控与修复越狱路径？

### 方法与优化

- **PEFT 选择：** LoRA、QLoRA、IA3、Prefix/Prompt Tuning、BitFit 各自优劣？  
  - 追问：在算力紧张、数据很少/很多时分别怎么选？  
- **对齐范式：** RLHF 与 DPO/ORPO 的流程/目标函数有何差异？  
  - 追问：为何 DPO 能绕过 RM 训练？何时仍需 PPO？  
- **目标函数：** 请写出 SFT 交叉熵、KL 正则、DPO 的公式并解释各项含义。  
  - 追问：KL 系数/β 如何影响风格漂移与合规性？  
- **训练配方：** 学习率、warmup、衰减、梯度裁剪的经验区间与相互作用？  
  - 追问：为何大模型更偏好小学习率与少 epoch？  
- **长上下文：** 如何安全扩展上下文（RoPE 缩放/FlashAttention/窗口注意力）？  
  - 追问：上下文“可用但不可靠”的边界如何评测？

### 低精度与内存

- **QLoRA 原理：** 4bit NF4、双重量化、分页优化器如何协同节省显存？  
  - 追问：量化误差会如何影响收敛？如何发现与缓解？  
- **精度选择：** BF16 vs FP16 vs FP8 的取舍与硬件依赖？  
  - 追问：为何 BF16 更稳？在何种情况下选 FP8？  
- **分布式与切分：** ZeRO/FSDF（FSDP）的内存切分要点与权衡？  
  - 追问：重计算与通信的开销如何平衡？

### 评测与安全

- **指标体系：** 困惑度、EM/F1、ROUGE、代码/数学指标、人评如何组合？  
  - 追问：当自动指标与人评冲突时如何决策？  
- **对齐评测：** 如何评估遵从度、无害性与拒答质量？  
  - 追问：如何设计场景化评测来贴近业务？  
- **红队测试：** 常见越狱策略与防护样本设计？  
  - 追问：上线后如何做“黑样本挖掘—修复—回归”的闭环？

### 工程与上线

- **数据/实验可复现：** 你如何版本化数据、固定模板、记录超参与种子？  
  - 追问：在不可完全确定的算子下如何做稳定性评估？  
- **模板锁定：** 为什么训练/评测/上线必须使用一致的 chat 模板？  
  - 追问：系统提示微调对行为的影响如何量化？  
- **服务化与性能：** KV Cache、批处理、温度/Top-p 对吞吐与质量的影响？  
  - 追问：如何在低延迟场景保持对齐与安全性不退化？  
- **兼容性：** 合并 LoRA 后可能出现什么数值漂移？如何校准与回归？  
  - 追问：Tokenizer 或特殊 token 变更造成的隐患如何快速定位？

### 常见坑与排查

- **模板/Tokenizer 不一致:** 会导致哪些可观测症状？  
  - 追问：你会用哪些最小化实验来定位问题？  
- **灾难性遗忘:** 如何通过 KL 正则、回放与小步调参进行防治？  
  - 追问：如何区分“风格变化”与“能力下降”？  
- **数据污染:** 你如何证明评测集未被污染？给出具体流程。  
  - 追问：业务环境下如何做持续防污染监控？  
- **量化训练不稳:** 如何诊断来自量化的训练不稳定而非配方问题？  
  - 追问：给出一套逐项排除的 ablation 方案。

---

# 个性化补充建议

- **若你目标是“面试中的可讲可演示”:**  
  - **准备 1 套可运行 demo:** 小模型 + QLoRA + 你熟悉的领域 3k–10k SFT；附模板锁与评测脚本。  
  - **准备 1 套偏好对齐实验:** 1000–5000 对偏好 + DPO；对齐前后对比样例与人评。  
  - **准备 1 页“坑位清单”:** 模板/Tokenizer/评测污染/量化不稳/打包泄漏，配最小复现实验。

---

如果你给我目标岗位（如模型优化/对齐工程/数据治理/平台工程）与可用算力，我可以把上面清单压缩为针对性“背诵版”与“追问答辩卡”，并附一页速记图。