偏好学习 (RLHF)
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-18&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-25%2003%3A17%3A40&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-18 16:14:23
toc_title: '**偏好学习** (**RLHF**)'
top: false
draft: false
hidden: true
section_number: false
level: 0
tags: [llm_sft]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: 偏好学习*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [基础概念](#基础概念)
- [实施流程](#实施流程)
    - [1. **监督微调 (SFT)**](#1-监督微调-sft)
    - [2. **人类反馈收集与奖励模型 (Reward Model)**](#2-人类反馈收集与奖励模型-reward-model)
        - [奖励模型训练流程](#奖励模型训练流程)
        - [Bradley–Terry 模型介绍](#bradleyterry-模型介绍)
    - [3. **策略优化 (Policy Optimization)**](#3-策略优化-policy-optimization)
        - [**策略梯度算法**](#策略梯度算法)
- [面试问题整理](#面试问题整理)
<!--END_SECTION:toc-->

---

## 基础概念

- **RLHF** (**R**einforcement **L**earning from **H**uman **F**eedback, 基于人类反馈的强化学习)
    - 是一种通过结合 **人类偏好** 与 **强化学习** 来微调大语言模型的 **技术** 或 **范式**;
    - **RLHF 是建立在 SFT 基础上的进一步微调**;
    <!-- - 是一种通过结合 **人类偏好** 与 **强化学习**, **在 SFT 的基础上进一步微调大语言模型** 的 **技术** 或 **范式**; -->
- **背景**
    - **预训练的局限**:
        - **大规模无监督语料预训练** 虽然能使 **LLM** 获得强大的 **语言建模能力**,
        - 但输出内容可能与 **人类价值观**, **安全性要求** 或 **交互习惯** 等不一致;
    - **SFT (有监督微调) 的不足**:
        - **SFT** 的目标是 **拟合训练数据的条件分布**, 虽然能让模型在 **特定任务** 上更可控,
        - 但其性能上限取决于 **数据的质量和范围**,
        - 对 **未见场景的泛化** 与 **细粒度偏好的对齐** 能力有限;
        <!-- - 其 **策略空间** 被限制在训练数据附近, **损失函数** (交叉熵损失) 会惩罚模型输出与 **标准答案** 之间的差异;  -->
- **动机/思路**
    <!--
    - **RLHF** 的核心在于 **将人类偏好作为优化目标**, 这使模型具有 **更大的策略搜索空间**,
        - 从而 **有可能** 生成 **更符合要求** 的输出, 最终突破 **静态数据的限制** 或 **原始数据的分布**;
    - 具体来说, RLHF 通过训练一个 **奖励模型** 来作为 **人类代理**,
        - 将 **人类偏好** 量化为一个 **标量分数**, 当做评价一个回答好坏的信号;
        - 这鼓励模型去探索那些未在原始训练集中, 但可能获得高得分 (即更符合人类偏好) 的回答;
    -->
    - **RLHF** 的核心在于 **将人类偏好作为优化目标**;
        - 通过将 **人类偏好** 量化为一个 **标量奖励分数** (当做评价一个回答好坏的信号),
        - **鼓励** 模型去探索那些未在原始训练集中出现, 但可能获得高奖励 (即更符合人类偏好) 的回答,
        - 使模型具有 **更大的策略搜索空间**, 从而有可能 **突破静态数据的限制**, 生成 **更符合要求** 的输出;
- **优势 (相比仅 SFT)**
    - **更大的策略空间**:
        - SFT 的损失函数 (交叉熵) **惩罚模型输出与标准答案之间的差异**, 因此其搜索空间被限制在示范数据附近;
        - RLHF 的目标是 **最大化奖励信号**, 这允许模型 **尝试训练中未出现, 但高奖励 (更符合偏好) 的表达**, 因此拥有更大的策略探索空间;
        <!-- - RLHF 的目标是 **最大化奖励信号**, 这允许模型 **尝试训练中未出现, 但能获得高奖励的表达**, 从而拥有更大的策略探索空间; -->
        <!-- - RLHF 的目标是 **最大化奖励信号**, 这鼓励模型探索任何能获得高奖励的回复, 即使与标准答案不同, 从而拥有更大的策略探索空间; -->
    - **更容易对齐抽象偏好**:
        - SFT 模仿给定的示范数据, **难以学习复杂/模糊/抽象的维度或概念**, 比如 逻辑性/幽默感 等;
        - RLHF 将人类反馈转化为 **序列级奖励**/**排序信号**/**比较信号**
            - 抽象偏好虽然难以用单一答案标注, 但可以通过 **成对比较** 或 **排序** 体现 (如 A 比 B 更有说服力);
    - **更低的数据成本**:
        - SFT 需要提供高质量的示范数据, 对抽象偏好更是难以统一标准;
        - RLHF 只需要获得数据间的 **相对关系**, 比如 A 和 B 哪个更好/更幽默/更简洁/更有逻辑 等;
            > 虽然不同人的判断标准依然有差异, 但相比直接撰写一个 "好的/幽默的/简洁的/有逻辑的" 的回答, 难度降低了非常多;
    - 其他优势, 比如 **更好的泛化性**, **更好的安全性**, **更少的幻觉/错误**, **更灵活的优化方式**, **平衡多个优化目标** 等等, 基本都是以上优势的拓展;

    <!--
    - 能够突破原始数据的分布, 生成比微调数据中所有样本都更优质的输出;
    - SFT 主要学习模仿人类提供的示范数据, 目标是拟合训练数据的分布; 而 RLHF 通过强化学习框架, 能够学习并优化一个更复杂、更抽象的目标 (即奖励模型所代表的人类偏好), 旨在生成更符合人类价值观和喜好的内容;
    - 对齐目标更高级; SFT 主要学习模仿人类提供的示范数据, 目标是拟合训练数据的分布; 而 RLHF 通过强化学习框架, 能够学习并优化一个更复杂、更抽象的目标 (即奖励模型所代表的人类偏好), 旨在生成更符合人类价值观和喜好的内容;
    - 处理主观和复杂偏好; 对于**没有唯一正确答案或充满主观偏好的任务** (如 创意写作, 风格模仿, 安全性), SFT 难以覆盖所有可能; RLHF 则可以通过奖励模型量化这些主观偏好, 并引导模型朝着综合评分更高的方向优化;
    - 突破示范数据天花板; SFT 的性能**严重依赖于示范数据的质量和数量**, 其上限被训练数据集所限制; RLHF 则可以通过策略梯度等方法探索和生成比示范数据更优质的回答, 从而有可能突破 SFT 的性能瓶颈;
    - 数据利用效率更高; SFT 需要大量高质量的输入-输出配对数据; 而 RLHF 所需的偏好数据 (比较性排序数据) **更容易大规模获取**且成本更低, 因为标注者只需判断哪个回答更好, 而无需亲自撰写最佳回答;
    - 更好的泛化性和安全性; RLHF 通过奖励模型可以对 "胡说八道", 有害或有偏见的内容给予低分惩罚, 从而在整个输出空间内 **约束模型行为**, 使其在未见过的输入上也能表现出更高的安全性和可靠性;
    -->

## 实施流程
> RLHF 通常包含三个核心步骤: **SFT → RM → RL**

### 1. **监督微调 (SFT)**
> **构建基础能力**

<!-- 1. **监督微调 (SFT)** —— **构建基础能力** -->
<!-- > RLHF 是建立在 SFT 之上的进一步微调; -->
- **动机**:
    - 预训练模型虽然知识丰富, 但其输出格式和内容质量可能不符合特定任务的要求;
- **目的/预期**:
    <!-- - **构建基础能力**; -->
    - 将预训练语言模型调整到具备基本的 **指令遵循能力**, 为后续步骤提供一个性能良好的 **基线模型**;
    - 提供一个稳定可用的初始策略, **避免 RL 从随机策略开始导致训练不稳定**;
    <!-- - 防止 RL 从随机策略开始, **避免训练不稳定**; -->
    - **SFT 是 RLHF 的基础**, 若 SFT 模型效果不佳, 会导致后续 **奖励模型失真** / **RL 优化失败** 等一系列问题;
- **输入**:
    - 预训练语言模型;
    - 高质量指令-响应对数据集 (Prompt + 人工撰写的理想回答);
- **输出**:
    - **初步对齐** 的 SFT 模型
- **失败模式**:
    - **数据质量差**: 学到错误或不一致的模式;
    - **数据覆盖低**: 在未见任务上表现差;
    - **灾难性遗忘**: 在微调过程中可能遗忘预训练阶段获得的知识;

### 2. **人类反馈收集与奖励模型 (Reward Model)**
> **量化人类偏好**

<!-- 2. **人类反馈收集与奖励模型 (RM) 训练** —— **量化人类偏好** -->
- **动机**:
    - 人类偏好难以用简单规则定义, 需要一种可泛化到新样本的自动化 **评分机制**;
- **目的/预期**:
    <!-- - **量化人类偏好**; -->
    - 将 **人类偏好** 转化为可量化的 **标量奖励信号**;
    - 训练一个能够 **对模型输出打分** 的 **奖励模型** 作为评价人类偏好的代理, 其评分能反映模型回答的好坏;
- **输入**:
    - **SFT 模型 + 人类偏好数据集**,
        > 给定 **提示** 后, 由 SFT 模型 **生成多个回答**, 每组回答有人类标注的 **偏好排序**;
- **输出**:
    - **奖励模型**: 一个能够为任何「提示+回答」对打分的模型;
    - **模型输入**: 提示+回答 → **模型输出**: 标量分数;
- **失败模式**:
    - **奖励失真**/**标注偏差**: 奖励模型学习到错误的偏好模式;
    - **数据多样性不足**/**泛化能力不足**: 奖励模型在 **分布外数据** 上表现不佳, 无法准确评估训练集范围外的回答;
    - **过拟合**: 在训练样本上打分准确, 但对新样本失效;


<!--START_SECTION:keyword-->
<!--keyword_info
name: '奖励模型'
extra_url: false
-->
#### 奖励模型训练流程
<!--END_SECTION:keyword-->

- **目标**:
    - 将人类的相对偏好转化为一个可优化的 **标量奖励函数** $\mathcal{R}_\theta(\cdot)$;
    - 训练一个模型, 使其能够为给定的 **提示-回答** 对 $(x, y)$ 分配一个 **标量分数** $r = \mathcal{R}_\theta(x, y)$;
- **训练数据**:
    - **来源**:
        - 使用 **阶段一训练的 SFT 模型** 在同一提示下生成的两个或多个候选输出;
        - 由人类标注者进行成对比较或排序;
    - **数据形式 (每条样本包含)**:
        - **上下文** (Prompt): $x$;
        <!-- - **一对偏好回答** (Chosen/Rejected Response): $(y_j, y_k, y_j \succ y_k)$ -->
        - **获胜回答** (Chosen Response) $y_j$;
        - **失败回答** (Rejected Response) $y_k$;
        <!-- - 人类偏好标签 ($y_j \succ y_k$); -->
- **模型结构**:
    - **骨干**:
        - **使用阶段一微调后的 SFT 模型初始化 Transformer 主体**, 并 **移除 `lm_head` 层** (即预测下一个 token 的线性层)
            > 为了减少训练成本; 也可以是另一个经过微调的 LM; 还可以是根据偏好数据从头开始训练的 LM (Anthropic)
        <!-- - 微调 LM 模型 (移除 `lm_head`) + 一个 **线性层** 输出标量分数; -->
    - **输出层**
        - 一般取 **最后一个 token** (通常是 **`EOS`**) 对应的隐藏向量 $h_{\scriptscriptstyle EOS}$ 作为整段输入的表示;
        - 通过一个新的 **线性层** $Wh_{\scriptscriptstyle EOS}+b$ 得到标量分数;
            > - 通常线性层的 **输入维度** 等于隐层维度, **输出维度** 为 1;
- **损失函数**
    - **Bradley–Terry Loss / Pairwise Logistic Loss** (成对样本, 一正一负)
        - 对给定 **上下文** $x$ 和 **一对回答** $(y_j, y_k)$, 其中人类标注认为 $y_j$ **优于** $y_k$, 记作 $y_j \succ y_k$;
        - 根据 **Bradley-Terry 模型**, 定义 $y_j \succ y_k$ 的概率为:
            $$
            \begin{aligned}
                P(y_j \succ y_k)
                &= \frac{\exp\big(\mathcal{R}_\theta(x, y_j)\big)}{\exp\big(\mathcal{R}_\theta(x, y_j)\big) + \exp\big(\mathcal{R}_\theta(x, y_k)\big)} \\
                &= \frac{1}{1 + \dfrac{\exp\big(\mathcal{R}_\theta(x, y_k)\big)}{\exp\big(\mathcal{R}_\theta(x, y_j)\big)}} \\
                &= \frac{1}{1 + e^{- \big(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)\big)}} \\
                &= \sigma\big(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)\big) \quad \text{(根据 Sigmoid 函数定义 } \sigma(z) = \frac{1}{1+e^{-z}} \text{)}
            \end{aligned}
            $$
            <!-- > 其中 $\mathcal{R}_\theta(x, y)$ 是奖励模型为 $x$ 和 $y$ 预测的标量分数; $\sigma(\cdot)$ 是 $\text{Sigmoid}$ 函数; -->
        <!-- - 训练目标是 **最大化似然函数** $\mathcal{L}(\theta; D)$, 对应的损失函数为 **负对数似然损失**: -->
        - 训练目标是 **最大化似然函数** $L$, 或最小化对应的 **负对数似然损失**:
            <!-- \mathcal{L}(\theta; D) = -\sum_{i=1}^N \log \, \sigma(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)) -->
            $$
            \mathcal{L}(\theta; D) = -\mathbb{E}_{(x, y_j, y_k) \sim D} \Big \lbrack
                \log \, \sigma\big(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)\big)
                \Big \rbrack
            $$
    - **InfoNCE Loss** (候选回答大于 2 时, 一正多负)
        $$
        \mathcal{L}_{\text{InfoNCE}}(\theta) = -\mathbb{E} \left \lbrack \log \frac{\exp\big(\mathcal{R}_\theta(x, y^+)\big)}{\exp\big(\mathcal{R}_\theta(x, y^+)\big) + \sum_{i=1}^{N-1} \exp\big(\mathcal{R}_\theta(x, y_i^-)\big)} \right \rbrack
        $$
    <!--
    - **带边距的排序损失**
    - **Listwise 损失** (多候选)
    -->
- **训练阶段**:
    - **在一次前向计算中**, 奖励模型分别对 $(x, y_j)$ 和 $(x, y_k)$ 各跑一遍, 得到两个标量分数 $r_j$ 和 $r_k$;
    - 若 $y_j \succ y_k$, 则损失为 $l = -\log \, \sigma(r_j - r_k)$
- **推理阶段**:
    - **输入**:
        - Prompt + 单个候选输出;
    - **输出**:
        - 标量分数 (越高表示越符合奖励模型学到的人类偏好);


<!--START_SECTION:keyword-->
<!--keyword_info
name: 'BT 模型'
extra_url: false
-->
#### Bradley–Terry 模型介绍
<!--END_SECTION:keyword-->
> 训练奖励模型时, 在 **损失函数** 中使用了 Bradley–Terry (BT) 模型, 这里简单介绍下.

- **背景**:
    - BT 模型最早应用于体育领域,
    - 其 **目的** 是通过分析一系列两两对战的胜负结果, 估算出参赛对象的潜在 **能力分数**;
    - **核心思想**: **能力分数越高, 获胜概率越大**;
- **基本假设**:
    - 每个对象 $y$ 存在一个 **不可直接观测的** 的 **标量能力分数** $r$, 用于衡量其 **强度**;
    - 当两个对象 $y_j$ 和 $y_k$ 进行比较时, **结果是一个随机事件**, 其 **概率** 仅取决于它们之间的 **能力分数差**, 即 $r_j - r_k$;
    - 具体地, $y_j$ **战胜** $y_k$ (记 $y_j \succ y_k$) 的概率由以下公式定义:
        $$
        P(y_j \succ y_k) = \frac{\exp(r_j)}{\exp(r_j) + \exp(r_k)} = \sigma (r_j - r_k)
        $$
        其中 $\sigma(z) = \dfrac{1}{1 + e^{-z}}$ 为 **Sigmoid 函数**, 显然有 $P(y_j \succ y_k) + P(y_k \succ y_j) = 1$;
    - 因此, BT 模型本质上是通过 **Sigmoid 函数** 将两个对象的 **能力分数差** 映射为其比较的胜率;
    <!-- - 即, **BT 模型** 试图通过 **Sigmoid 函数** 将两个对象的分数差 $(r_j - r_k)$ 转换为一个比较概率; -->
    <!--
    - 定义一个对象 $y$ 的 **能力分数** 为 $\alpha = \exp(r)$,
    - 则 对象 $y_j$ **击败** 另一个对象 $y_k$ 的 **概率** 可以表示为 $P(y_j \succ y_k) = \dfrac{\alpha_j}{\alpha_j + \alpha_k}$:
    - 为了便于优化和理解, 常写成指数形式, 记 $\alpha = \exp(r)$, 则
        $$
        P(y_j \succ y_k) = \frac{\exp(r_j)}{\exp(r_j) + \exp(r_k)}
        $$
     -->
- **目的**:
    -  在经典 BT 模型中, 其目标是利用观测到的大量 **成对比较结果** $D = \{(y_j, y_k) \mid y_j \succ y_k\}$,
    -  学习一个 **评分函数** $\mathcal{R}_\theta(y)$; 该函数接收一个对象 $y$ 作为输入, 输出其能力分数;
    <!-- - 在经典 BT 模型中, 其目标是利用观测到的大量 **成对比较结果** $D = \{(y_j, y_k) \mid y_j \succ y_k\}$, 为每个对象 **估计出一个静态的能力分数** $r$; -->
    <!-- - BT 模型试图通过一系列 **成对比较结果** $(y_j, y_k)$, **为每个对象估算出一个标量分数** $r$, 代表其强度; -->
    <!-- - 两个对象之间的 比较关系/胜负结果 是可以获取/累计的; -->
    <!-- - **BT 模型的目的** 是基于一系列 **成对比较的结果** $(y_j, y_k)$, 学习 **计算对象强度 (一个标量分数) 的函数/模型** $\mathcal{R}_\theta(\cdot)$; -->
<!--
- **定义**:
    - 假设有两个对象 $y_j$ 和 $y_k$, 其强度分数为 $r_j$ 和 $r_k$
    - 则定义 $y_j$ 优于 $y_k$ (记 $y_j \succ y_k$) 的概率为
        $$
        P(y_j \succ y_k) = \frac{\exp(r_j)}{\exp(r_j) + \exp(r_k)}
        $$
    - 根据 $\textbf{Sigmoid}$ **函数** $\sigma(z)=\dfrac{1}{1+e^{-z}}$, 即
        $$
        P(y_j \succ y_k)
            = \frac{1}{1 + \exp\big(-(r_j - r_k)\big)}
            = \sigma (r_j - r_k)
        $$
    - 其中 $r_j = \mathcal{R}_\theta(y_j)$, $r_k = \mathcal{R}_\theta(y_k)$
    - 因此, BT 模型本质上通过 **Sigmoid 函数** 将两个对象的分数差 $(r_j - r_k)$ 转换为一个比较概率;
-->
- **优化过程/损失函数**:
    - 模型的优化目标是 **最大化** 所有观测结果在模型下的 **似然估计**;
    - 其对应的 **负对数似然损失函数** 为:
        $$
        \mathcal{L}(\theta;D) = -\mathbb{E}_{(y_j, y_k) \sim D} \Big \lbrack \log \, \sigma\big(\mathcal{R}_\theta(y_j) - \mathcal{R}_\theta(y_k)\big) \Big \rbrack
        $$
    - 该损失函数的直观作用是: **鼓励模型拉大强弱对象之间的分数差**, 使得模型的预测结果与观测到的胜负关系一致;
    <!--
    - 综上, 可以通过 **最大化 模型预测出的强度关系 与 真实比较结果 的似然** 来优化模型, 其对应的 **负对数似然损失函数** 为:
        $$
        \mathcal{L}(\theta;D) = -\mathbb{E}_{(y_j, y_k) \sim D} \Big \lbrack \log \, \sigma\big(\mathcal{R}_\theta(y_j) - \mathcal{R}_\theta(y_k)\big) \Big \rbrack
        $$
    - 该损失函数的直观作用是: **鼓励模型拉大强弱对象之间的分数差**, 使得模型的预测结果与观测到的胜负关系一致;
    -->

<!--START_SECTION:keyword-->
<!--keyword_info
name: '策略优化'
extra_url: false
-->
### 3. **策略优化 (Policy Optimization)**
<!--END_SECTION:keyword-->
> 基于 **强化学习 (RL) 框架** 进一步优化 SFT 模型, **对齐人类偏好**.

<!--START_SECTION:keyword-->
<!--keyword_info
name: 'RL 基础'
extra_url: true
-->
- **强化学习基础**
    > _[RL 基础 (基于 LLM 背景)](./强化学习基础.md)_
<!--END_SECTION:keyword-->
- **动机**:
    - SFT 模型的学习受限于静态数据, 缺乏主动探索能力;
    - 策略优化旨在让模型通过与 **环境** 的交互, 在 **更大的策略空间** 中寻找更优的行为;
        > LLM 微调语境下的 **环境**, 即模型生成下一个 token 时的 **上下文** (Prompt + 已生成的 token 序列)
    <!-- - **对齐人类偏好**; -->
    <!-- - 在奖励模型引导下, 在更大的策略空间中寻找高奖励输出, 生成更符合人类偏好的输出; -->
- **核心目标**:
    - 通过优化策略参数, 最大化策略模型的期望累计回报;
    <!-- - 最大化一个反映人类偏好的目标函数; -->
    - 通常会引入一个正则项 (比如 KL 散度), 以防止优化后的策略过度偏离原始的 SFT 模型, 以保持生成内容的稳定性和质量;
    <!-- - 通过优化策略 (即模型参数 $\pi_{\theta}$), 最大化从 奖励模型 (Reward Model) 获得的累积奖励期望值 $\mathbb{E}\lbrack R\rbrack$; 同时, 通常会引入一个 KL 散度惩罚项, 以防止优化后的策略过度偏离原始的 SFT 模型, 保持生成内容的稳定性和质量; -->
- **输入 (关键组件)**:
    > 以 PPO 算法为例, 不同策略优化算法需要的输入不同.
    - Prompt 数据集;
    - **策略模型**: 待优化的模型, 通常由 SFT 模型初始化;
    - **奖励模型** (可选): 作为人类的 **偏好代理**, 为模型输出打分; 在策略优化阶段不参与更新;
    - **参考模型** (可选): 提供 **KL 散度约束**; 初始 SFT 模型的副本, 不参与更新;
    - **价值模型** (可选): 估计 **给定状态的回报**; 结构与奖励模型一致, 在策略优化阶段 **独立于策略模型进行训练**, 使用 SFT 模型初始化;
- **输出**:
    - 对齐人类偏好的最终模型 (经过 RLHF 微调后的模型)
- **失败模式**:
    - **奖励过度**/**过度优化**: 在奖励模型的打分上越来越高, 但实际输出质量下降;
    - **模式坍塌**: 过度优化的副作用之一, 输出模式单一, 失去多样性 (模型为了获得高分只会生成「安全」回答);
    - **奖励黑客 (Reward Hacking)**: 模型学会利用奖励模型漏洞而非真正提升质量; 比如更冗长的回答/更礼貌的回答等, 观感提升但忽视质量;
    - 除此之外, 还有 **偏好冲突**/**价值错配** 等失败模式,
    - **根源** 一般是 "**奖励信号不完美/失真 + 优化算法过度依赖奖励模型**";


#### **策略梯度算法**
> _[策略梯度 (Policy Gradient) 算法笔记](./策略梯度算法.md)_
>> TRPO → PPO (Clip/KL) → DPO → GRPO/IPO/KTO/...


---

<!--START_SECTION:keyword-->
<!--keyword_info
name: 'QA'
extra_url: true
-->
## 面试问题整理
> _[RLHF 面试问题整理](./偏好学习_QA.md)_
<!--END_SECTION:keyword-->