偏好学习 (RLHF)
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-18&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-18%2016%3A07%3A56&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-18 16:14:23
toc_title: 偏好学习 (RLHF)
top: false
draft: false
hidden: true
section_number: false
level: 0
tags: [llm_sft]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: 偏好学习*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [基础概念](#基础概念)
    - [实现方法/一般流程](#实现方法一般流程)
    - [面试问题整理](#面试问题整理)
<!--END_SECTION:toc-->

---

## 基础概念

- **RLHF** (**R**einforcement **L**earning from **H**uman **F**eedback, 基于人类反馈的强化学习)
    - 是一种通过结合 **人类偏好** 与 **强化学习**, **在 SFT 的基础上进一步微调大语言模型** 的 **技术** 或 **范式**;
    <!-- - 是一种通过结合 **人类偏好** 与 **强化学习** 来 **微调大语言模型** 的 **技术** 或 **范式**; -->
- **背景**
    - **预训练的局限**: 
        - **大规模无监督语料预训练** 虽然能使 **LLM** 获得强大的 **语言建模能力**, 
        - 但输出内容可能与 **人类价值观**, **安全性要求** 或 **交互习惯** 等不一致;
    - **SFT (有监督微调) 的不足**: 
        - **SFT** 的目标是 **拟合训练数据的条件分布**, 虽然能让模型在 **特定任务** 上更可控, 
        - 但其性能上限取决于 **数据的质量和范围**, 
        - 对 **未见场景的泛化** 与 **细粒度偏好的对齐** 能力有限;
        <!-- - 其 **策略空间** 被限制在训练数据附近, **损失函数** (交叉熵损失) 会惩罚模型输出与 **标准答案** 之间的差异;  -->
- **动机/思路**
    <!-- 
    - **RLHF** 的核心在于 **将人类偏好作为优化目标**, 这使模型具有 **更大的策略搜索空间**, 
        - 从而 **有可能** 生成 **更符合要求** 的输出, 最终突破 **静态数据的限制** 或 **原始数据的分布**;
    - 具体来说, RLHF 通过训练一个 **奖励模型** 来作为 **人类代理**, 
        - 将 **人类偏好** 量化为一个 **标量分数**, 当做评价一个回答好坏的信号;
        - 这鼓励模型去探索那些未在原始训练集中, 但可能获得高得分 (即更符合人类偏好) 的回答; 
    -->
    - **RLHF** 的核心在于 **将人类偏好作为优化目标**;
        - 通过将 **人类偏好** 量化为一个 **标量奖励分数** (当做评价一个回答好坏的信号),
        - **鼓励** 模型去探索那些未在原始训练集中出现, 但可能获得高奖励 (即更符合人类偏好) 的回答, 
        - 使模型具有 **更大的策略搜索空间**, 从而有可能 **突破静态数据的限制**, 生成 **更符合要求** 的输出;
- **优势 (相比仅 SFT)**
    - **更大的策略空间**:
        - SFT 的损失函数 (交叉熵) **惩罚模型输出与标准答案之间的差异**, 因此其搜索空间被限制在示范数据附近;
        - RLHF 的目标是 **最大化奖励信号**, 这允许模型 **尝试训练中未出现, 但高奖励 (更符合偏好) 的表达**, 因此拥有更大的策略探索空间;
        <!-- - RLHF 的目标是 **最大化奖励信号**, 这允许模型 **尝试训练中未出现, 但能获得高奖励的表达**, 从而拥有更大的策略探索空间; -->
        <!-- - RLHF 的目标是 **最大化奖励信号**, 这鼓励模型探索任何能获得高奖励的回复, 即使与标准答案不同, 从而拥有更大的策略探索空间; -->
    - **更容易对齐抽象偏好**:
        - SFT 模仿给定的示范数据, **难以学习复杂/模糊/抽象的维度或概念**, 比如 逻辑性/幽默感 等;
        - RLHF 将人类反馈转化为 **序列级奖励** 或 **排序信号**
            - 抽象偏好虽然难以用单一答案标注, 但可以通过 **成对比较** 或 **排序** 体现 (如 A 比 B 更有说服力);
    - **更低的数据成本**:
        - SFT 需要提供高质量的示范数据, 对抽象偏好更是难以统一标准;
        - RLHF 只需要获得数据间的 **相对关系**, 比如 A 和 B 哪个更好/更幽默/更简洁/更有逻辑 等;
            > 虽然不同人的判断标准依然有差异, 但相比直接撰写一个 "好的/幽默的/简洁的/有逻辑的" 的回答, 难度降低了非常多;
    - 其他优势, 比如 **更好的泛化性**, **更好的安全性**, **更少的幻觉/错误**, **更灵活的优化方式**, **平衡多个优化目标** 等等, 基本都是以上优势的拓展;

    <!-- 
    - 能够突破原始数据的分布, 生成比微调数据中所有样本都更优质的输出;
    - SFT 主要学习模仿人类提供的示范数据, 目标是拟合训练数据的分布; 而 RLHF 通过强化学习框架, 能够学习并优化一个更复杂、更抽象的目标 (即奖励模型所代表的人类偏好), 旨在生成更符合人类价值观和喜好的内容;
    - 对齐目标更高级; SFT 主要学习模仿人类提供的示范数据, 目标是拟合训练数据的分布; 而 RLHF 通过强化学习框架, 能够学习并优化一个更复杂、更抽象的目标 (即奖励模型所代表的人类偏好), 旨在生成更符合人类价值观和喜好的内容;
    - 处理主观和复杂偏好; 对于**没有唯一正确答案或充满主观偏好的任务** (如 创意写作, 风格模仿, 安全性), SFT 难以覆盖所有可能; RLHF 则可以通过奖励模型量化这些主观偏好, 并引导模型朝着综合评分更高的方向优化;
    - 突破示范数据天花板; SFT 的性能**严重依赖于示范数据的质量和数量**, 其上限被训练数据集所限制; RLHF 则可以通过策略梯度等方法探索和生成比示范数据更优质的回答, 从而有可能突破 SFT 的性能瓶颈;
    - 数据利用效率更高; SFT 需要大量高质量的输入-输出配对数据; 而 RLHF 所需的偏好数据 (比较性排序数据) **更容易大规模获取**且成本更低, 因为标注者只需判断哪个回答更好, 而无需亲自撰写最佳回答;
    - 更好的泛化性和安全性; RLHF 通过奖励模型可以对 “胡说八道”, 有害或有偏见的内容给予低分惩罚, 从而在整个输出空间内 **约束模型行为**, 使其在未见过的输入上也能表现出更高的安全性和可靠性;
    -->

### 实现方法/一般流程

**RLHF 通常包含三个核心步骤**
- **步骤一**: **有监督微调 (SFT)**
    > RLHF 是建立在 SFT 之上的进一步微调;
    - **目的/预期**
        - 将预训练语言模型调整到 **具备基本的指令遵循能力**, 为后续步骤提供一个性能良好的 **基线模型**;
        - 为 RLHF 提供一个稳定可用的初始策略, 避免 RL 从随机策略开始导致训练不稳定;
    - **输入**
- **步骤二**: **训练奖励模型**
- **步骤三**: **策略优化** (强化学习)



<!--START_SECTION:keyword-->
<!--keyword_info
name: 'QA'
extra_url: true
-->
### 面试问题整理
> [**偏好学习**面试问题整理](./偏好学习_QA.md)
<!--END_SECTION:keyword-->