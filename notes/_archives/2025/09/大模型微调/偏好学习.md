偏好学习 (RLHF)
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-18&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-24%2005%3A24%3A30&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-18 16:14:23
toc_title: 偏好学习 (**RLHF**)
top: false
draft: false
hidden: true
section_number: false
level: 0
tags: [llm_sft]
-->

<!--START_SECTION:keywords-->
> ***Keywords**: 偏好学习*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [基础概念](#基础概念)
- [实施流程](#实施流程)
    - [1. **监督微调 (SFT)**](#1-监督微调-sft)
    - [2. **人类反馈收集与奖励模型 (Reward Model)**](#2-人类反馈收集与奖励模型-reward-model)
        - [奖励模型训练流程](#奖励模型训练流程)
        - [Bradley–Terry 模型介绍](#bradleyterry-模型介绍)
    - [3. **策略优化 (Policy Optimization)**](#3-策略优化-policy-optimization)
        - [强化学习基础](#强化学习基础)
        - [**策略梯度算法**](#策略梯度算法)
- [面试问题整理](#面试问题整理)
<!--END_SECTION:toc-->

---

## 基础概念

- **RLHF** (**R**einforcement **L**earning from **H**uman **F**eedback, 基于人类反馈的强化学习)
    - 是一种通过结合 **人类偏好** 与 **强化学习** 来微调大语言模型的 **技术** 或 **范式**;
    - **RLHF 是建立在 SFT 基础上的进一步微调**;
    <!-- - 是一种通过结合 **人类偏好** 与 **强化学习**, **在 SFT 的基础上进一步微调大语言模型** 的 **技术** 或 **范式**; -->
- **背景**
    - **预训练的局限**:
        - **大规模无监督语料预训练** 虽然能使 **LLM** 获得强大的 **语言建模能力**,
        - 但输出内容可能与 **人类价值观**, **安全性要求** 或 **交互习惯** 等不一致;
    - **SFT (有监督微调) 的不足**:
        - **SFT** 的目标是 **拟合训练数据的条件分布**, 虽然能让模型在 **特定任务** 上更可控,
        - 但其性能上限取决于 **数据的质量和范围**,
        - 对 **未见场景的泛化** 与 **细粒度偏好的对齐** 能力有限;
        <!-- - 其 **策略空间** 被限制在训练数据附近, **损失函数** (交叉熵损失) 会惩罚模型输出与 **标准答案** 之间的差异;  -->
- **动机/思路**
    <!--
    - **RLHF** 的核心在于 **将人类偏好作为优化目标**, 这使模型具有 **更大的策略搜索空间**,
        - 从而 **有可能** 生成 **更符合要求** 的输出, 最终突破 **静态数据的限制** 或 **原始数据的分布**;
    - 具体来说, RLHF 通过训练一个 **奖励模型** 来作为 **人类代理**,
        - 将 **人类偏好** 量化为一个 **标量分数**, 当做评价一个回答好坏的信号;
        - 这鼓励模型去探索那些未在原始训练集中, 但可能获得高得分 (即更符合人类偏好) 的回答;
    -->
    - **RLHF** 的核心在于 **将人类偏好作为优化目标**;
        - 通过将 **人类偏好** 量化为一个 **标量奖励分数** (当做评价一个回答好坏的信号),
        - **鼓励** 模型去探索那些未在原始训练集中出现, 但可能获得高奖励 (即更符合人类偏好) 的回答,
        - 使模型具有 **更大的策略搜索空间**, 从而有可能 **突破静态数据的限制**, 生成 **更符合要求** 的输出;
- **优势 (相比仅 SFT)**
    - **更大的策略空间**:
        - SFT 的损失函数 (交叉熵) **惩罚模型输出与标准答案之间的差异**, 因此其搜索空间被限制在示范数据附近;
        - RLHF 的目标是 **最大化奖励信号**, 这允许模型 **尝试训练中未出现, 但高奖励 (更符合偏好) 的表达**, 因此拥有更大的策略探索空间;
        <!-- - RLHF 的目标是 **最大化奖励信号**, 这允许模型 **尝试训练中未出现, 但能获得高奖励的表达**, 从而拥有更大的策略探索空间; -->
        <!-- - RLHF 的目标是 **最大化奖励信号**, 这鼓励模型探索任何能获得高奖励的回复, 即使与标准答案不同, 从而拥有更大的策略探索空间; -->
    - **更容易对齐抽象偏好**:
        - SFT 模仿给定的示范数据, **难以学习复杂/模糊/抽象的维度或概念**, 比如 逻辑性/幽默感 等;
        - RLHF 将人类反馈转化为 **序列级奖励**/**排序信号**/**比较信号**
            - 抽象偏好虽然难以用单一答案标注, 但可以通过 **成对比较** 或 **排序** 体现 (如 A 比 B 更有说服力);
    - **更低的数据成本**:
        - SFT 需要提供高质量的示范数据, 对抽象偏好更是难以统一标准;
        - RLHF 只需要获得数据间的 **相对关系**, 比如 A 和 B 哪个更好/更幽默/更简洁/更有逻辑 等;
            > 虽然不同人的判断标准依然有差异, 但相比直接撰写一个 "好的/幽默的/简洁的/有逻辑的" 的回答, 难度降低了非常多;
    - 其他优势, 比如 **更好的泛化性**, **更好的安全性**, **更少的幻觉/错误**, **更灵活的优化方式**, **平衡多个优化目标** 等等, 基本都是以上优势的拓展;

    <!--
    - 能够突破原始数据的分布, 生成比微调数据中所有样本都更优质的输出;
    - SFT 主要学习模仿人类提供的示范数据, 目标是拟合训练数据的分布; 而 RLHF 通过强化学习框架, 能够学习并优化一个更复杂、更抽象的目标 (即奖励模型所代表的人类偏好), 旨在生成更符合人类价值观和喜好的内容;
    - 对齐目标更高级; SFT 主要学习模仿人类提供的示范数据, 目标是拟合训练数据的分布; 而 RLHF 通过强化学习框架, 能够学习并优化一个更复杂、更抽象的目标 (即奖励模型所代表的人类偏好), 旨在生成更符合人类价值观和喜好的内容;
    - 处理主观和复杂偏好; 对于**没有唯一正确答案或充满主观偏好的任务** (如 创意写作, 风格模仿, 安全性), SFT 难以覆盖所有可能; RLHF 则可以通过奖励模型量化这些主观偏好, 并引导模型朝着综合评分更高的方向优化;
    - 突破示范数据天花板; SFT 的性能**严重依赖于示范数据的质量和数量**, 其上限被训练数据集所限制; RLHF 则可以通过策略梯度等方法探索和生成比示范数据更优质的回答, 从而有可能突破 SFT 的性能瓶颈;
    - 数据利用效率更高; SFT 需要大量高质量的输入-输出配对数据; 而 RLHF 所需的偏好数据 (比较性排序数据) **更容易大规模获取**且成本更低, 因为标注者只需判断哪个回答更好, 而无需亲自撰写最佳回答;
    - 更好的泛化性和安全性; RLHF 通过奖励模型可以对 "胡说八道", 有害或有偏见的内容给予低分惩罚, 从而在整个输出空间内 **约束模型行为**, 使其在未见过的输入上也能表现出更高的安全性和可靠性;
    -->

## 实施流程
> RLHF 通常包含三个核心步骤: **SFT → RM → RL**

### 1. **监督微调 (SFT)**
> **构建基础能力**

<!-- 1. **监督微调 (SFT)** —— **构建基础能力** -->
<!-- > RLHF 是建立在 SFT 之上的进一步微调; -->
- **动机**:
    - 预训练模型虽然知识丰富, 但其输出格式和内容质量可能不符合特定任务的要求;
- **目的/预期**:
    <!-- - **构建基础能力**; -->
    - 将预训练语言模型调整到具备基本的 **指令遵循能力**, 为后续步骤提供一个性能良好的 **基线模型**;
    - 提供一个稳定可用的初始策略, **避免 RL 从随机策略开始导致训练不稳定**;
    <!-- - 防止 RL 从随机策略开始, **避免训练不稳定**; -->
    - **SFT 是 RLHF 的基础**, 若 SFT 模型效果不佳, 会导致后续 **奖励模型失真** / **RL 优化失败** 等一系列问题;
- **输入**:
    - 预训练语言模型;
    - 高质量指令-响应对数据集 (Prompt + 人工撰写的理想回答);
- **输出**:
    - **初步对齐** 的 SFT 模型
- **失败模式**:
    - **数据质量差**: 学到错误或不一致的模式;
    - **数据覆盖低**: 在未见任务上表现差;
    - **灾难性遗忘**: 在微调过程中可能遗忘预训练阶段获得的知识;

### 2. **人类反馈收集与奖励模型 (Reward Model)**
> **量化人类偏好**

<!-- 2. **人类反馈收集与奖励模型 (RM) 训练** —— **量化人类偏好** -->
- **动机**:
    - 人类偏好难以用简单规则定义, 需要一种可泛化到新样本的自动化 **评分机制**;
- **目的/预期**:
    <!-- - **量化人类偏好**; -->
    - 将 **人类偏好** 转化为可量化的 **标量奖励信号**;
    - 训练一个能够 **对模型输出打分** 的 **奖励模型** 作为评价人类偏好的代理, 其评分能反映模型回答的好坏;
- **输入**:
    - **SFT 模型 + 人类偏好数据集**,
        > 给定 **提示** 后, 由 SFT 模型 **生成多个回答**, 每组回答有人类标注的 **偏好排序**;
- **输出**:
    - **奖励模型**: 一个能够为任何「提示+回答」对打分的模型;
    - **模型输入**: 提示+回答 → **模型输出**: 标量分数;
- **失败模式**:
    - **奖励失真**/**标注偏差**: 奖励模型学习到错误的偏好模式;
    - **数据多样性不足**/**泛化能力不足**: 奖励模型在 **分布外数据** 上表现不佳, 无法准确评估训练集范围外的回答;
    - **过拟合**: 在训练样本上打分准确, 但对新样本失效;


<!--START_SECTION:keyword-->
<!--keyword_info
name: '奖励模型'
extra_url: false
-->
#### 奖励模型训练流程
<!--END_SECTION:keyword-->

- **目标**:
    - 将人类的相对偏好转化为一个可优化的 **标量奖励函数** $\mathcal{R}_\theta(\cdot)$;
    - 训练一个模型, 使其能够为给定的 **提示-回答** 对 $(x, y)$ 分配一个 **标量分数** $r = \mathcal{R}_\theta(x, y)$;
- **训练数据**:
    - **来源**:
        - 使用 **阶段一训练的 SFT 模型** 在同一提示下生成的两个或多个候选输出;
        - 由人类标注者进行成对比较或排序;
    - **数据形式 (每条样本包含)**:
        - **上下文** (Prompt): $x$;
        <!-- - **一对偏好回答** (Chosen/Rejected Response): $(y_j, y_k, y_j \succ y_k)$ -->
        - **获胜回答** (Chosen Response) $y_j$;
        - **失败回答** (Rejected Response) $y_k$;
        <!-- - 人类偏好标签 ($y_j \succ y_k$); -->
- **模型结构**:
    - **骨干**:
        - **使用阶段一微调后的 SFT 模型初始化 Transformer 主体**, 并 **移除 `lm_head` 层** (即预测下一个 token 的线性层)
            > 为了减少训练成本; 也可以是另一个经过微调的 LM; 还可以是根据偏好数据从头开始训练的 LM (Anthropic)
        <!-- - 微调 LM 模型 (移除 `lm_head`) + 一个 **线性层** 输出标量分数; -->
    - **输出层**
        - 一般取 **最后一个 token** (通常是 **`EOS`**) 对应的隐藏向量 $h_{\scriptscriptstyle EOS}$ 作为整段输入的表示;
        - 通过一个新的 **线性层** $Wh_{\scriptscriptstyle EOS}+b$ 得到标量分数;
            > - 通常线性层的 **输入维度** 等于隐层维度, **输出维度** 为 1;
- **损失函数**
    - **Bradley–Terry Loss / Pairwise Logistic Loss** (成对样本, 一正一负)
        - 对给定 **上下文** $x$ 和 **一对回答** $(y_j, y_k)$, 其中人类标注认为 $y_j$ **优于** $y_k$, 记作 $y_j \succ y_k$;
        - 根据 **Bradley-Terry 模型**, 定义 $y_j \succ y_k$ 的概率为:
            $$
            \begin{aligned}
                P(y_j \succ y_k)
                &= \frac{\exp\big(\mathcal{R}_\theta(x, y_j)\big)}{\exp\big(\mathcal{R}_\theta(x, y_j)\big) + \exp\big(\mathcal{R}_\theta(x, y_k)\big)} \\
                &= \frac{1}{1 + \dfrac{\exp\big(\mathcal{R}_\theta(x, y_k)\big)}{\exp\big(\mathcal{R}_\theta(x, y_j)\big)}} \\
                &= \frac{1}{1 + e^{- \big(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)\big)}} \\
                &= \sigma\big(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)\big) \quad \text{(根据 Sigmoid 函数定义 } \sigma(z) = \frac{1}{1+e^{-z}} \text{)}
            \end{aligned}
            $$
            <!-- > 其中 $\mathcal{R}_\theta(x, y)$ 是奖励模型为 $x$ 和 $y$ 预测的标量分数; $\sigma(\cdot)$ 是 $\text{Sigmoid}$ 函数; -->
        <!-- - 训练目标是 **最大化似然函数** $\mathcal{L}(\theta; D)$, 对应的损失函数为 **负对数似然损失**: -->
        - 训练目标是 **最大化似然函数** $L$, 或最小化对应的 **负对数似然损失**:
            <!-- \mathcal{L}(\theta; D) = -\sum_{i=1}^N \log \, \sigma(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)) -->
            $$
            \mathcal{L}(\theta; D) = -\mathbb{E}_{(x, y_j, y_k) \sim D} \Big \lbrack
                \log \, \sigma\big(\mathcal{R}_\theta(x, y_j) - \mathcal{R}_\theta(x, y_k)\big)
                \Big \rbrack
            $$
    - **InfoNCE Loss** (候选回答大于 2 时, 一正多负)
        $$
        \mathcal{L}_{\text{InfoNCE}}(\theta) = -\mathbb{E} \left \lbrack \log \frac{\exp\big(\mathcal{R}_\theta(x, y^+)\big)}{\exp\big(\mathcal{R}_\theta(x, y^+)\big) + \sum_{i=1}^{N-1} \exp\big(\mathcal{R}_\theta(x, y_i^-)\big)} \right \rbrack
        $$
    <!--
    - **带边距的排序损失**
    - **Listwise 损失** (多候选)
    -->
- **训练阶段**:
    - **在一次前向计算中**, 奖励模型分别对 $(x, y_j)$ 和 $(x, y_k)$ 各跑一遍, 得到两个标量分数 $r_j$ 和 $r_k$;
    - 若 $y_j \succ y_k$, 则损失为 $l = -\log \, \sigma(r_j - r_k)$
- **推理阶段**:
    - **输入**:
        - Prompt + 单个候选输出;
    - **输出**:
        - 标量分数 (越高表示越符合奖励模型学到的人类偏好);


<!--START_SECTION:keyword-->
<!--keyword_info
name: 'BT 模型'
extra_url: false
-->
#### Bradley–Terry 模型介绍
<!--END_SECTION:keyword-->
> 训练奖励模型时, 在 **损失函数** 中使用了 Bradley–Terry (BT) 模型, 这里简单介绍下.

- **背景**:
    - BT 模型最早应用于体育领域,
    - 其 **目的** 是通过分析一系列两两对战的胜负结果, 估算出参赛对象的潜在 **能力分数**;
    - **核心思想**: **能力分数越高, 获胜概率越大**;
- **基本假设**:
    - 每个对象 $y$ 存在一个 **不可直接观测的** 的 **标量能力分数** $r$, 用于衡量其 **强度**;
    - 当两个对象 $y_j$ 和 $y_k$ 进行比较时, **结果是一个随机事件**, 其 **概率** 仅取决于它们之间的 **能力分数差**, 即 $r_j - r_k$;
    - 具体地, $y_j$ **战胜** $y_k$ (记 $y_j \succ y_k$) 的概率由以下公式定义:
        $$
        P(y_j \succ y_k) = \frac{\exp(r_j)}{\exp(r_j) + \exp(r_k)} = \sigma (r_j - r_k)
        $$
        其中 $\sigma(z) = \dfrac{1}{1 + e^{-z}}$ 为 **Sigmoid 函数**, 显然有 $P(y_j \succ y_k) + P(y_k \succ y_j) = 1$;
    - 因此, BT 模型本质上是通过 **Sigmoid 函数** 将两个对象的 **能力分数差** 映射为其比较的胜率;
    <!-- - 即, **BT 模型** 试图通过 **Sigmoid 函数** 将两个对象的分数差 $(r_j - r_k)$ 转换为一个比较概率; -->
    <!--
    - 定义一个对象 $y$ 的 **能力分数** 为 $\alpha = \exp(r)$,
    - 则 对象 $y_j$ **击败** 另一个对象 $y_k$ 的 **概率** 可以表示为 $P(y_j \succ y_k) = \dfrac{\alpha_j}{\alpha_j + \alpha_k}$:
    - 为了便于优化和理解, 常写成指数形式, 记 $\alpha = \exp(r)$, 则
        $$
        P(y_j \succ y_k) = \frac{\exp(r_j)}{\exp(r_j) + \exp(r_k)}
        $$
     -->
- **目的**:
    -  在经典 BT 模型中, 其目标是利用观测到的大量 **成对比较结果** $D = \{(y_j, y_k) \mid y_j \succ y_k\}$,
    -  学习一个 **评分函数** $\mathcal{R}_\theta(y)$; 该函数接收一个对象 $y$ 作为输入, 输出其能力分数;
    <!-- - 在经典 BT 模型中, 其目标是利用观测到的大量 **成对比较结果** $D = \{(y_j, y_k) \mid y_j \succ y_k\}$, 为每个对象 **估计出一个静态的能力分数** $r$; -->
    <!-- - BT 模型试图通过一系列 **成对比较结果** $(y_j, y_k)$, **为每个对象估算出一个标量分数** $r$, 代表其强度; -->
    <!-- - 两个对象之间的 比较关系/胜负结果 是可以获取/累计的; -->
    <!-- - **BT 模型的目的** 是基于一系列 **成对比较的结果** $(y_j, y_k)$, 学习 **计算对象强度 (一个标量分数) 的函数/模型** $\mathcal{R}_\theta(\cdot)$; -->
<!--
- **定义**:
    - 假设有两个对象 $y_j$ 和 $y_k$, 其强度分数为 $r_j$ 和 $r_k$
    - 则定义 $y_j$ 优于 $y_k$ (记 $y_j \succ y_k$) 的概率为
        $$
        P(y_j \succ y_k) = \frac{\exp(r_j)}{\exp(r_j) + \exp(r_k)}
        $$
    - 根据 $\textbf{Sigmoid}$ **函数** $\sigma(z)=\dfrac{1}{1+e^{-z}}$, 即
        $$
        P(y_j \succ y_k)
            = \frac{1}{1 + \exp\big(-(r_j - r_k)\big)}
            = \sigma (r_j - r_k)
        $$
    - 其中 $r_j = \mathcal{R}_\theta(y_j)$, $r_k = \mathcal{R}_\theta(y_k)$
    - 因此, BT 模型本质上通过 **Sigmoid 函数** 将两个对象的分数差 $(r_j - r_k)$ 转换为一个比较概率;
-->
- **优化过程/损失函数**:
    - 模型的优化目标是 **最大化** 所有观测结果在模型下的 **似然估计**;
    - 其对应的 **负对数似然损失函数** 为:
        $$
        \mathcal{L}(\theta;D) = -\mathbb{E}_{(y_j, y_k) \sim D} \Big \lbrack \log \, \sigma\big(\mathcal{R}_\theta(y_j) - \mathcal{R}_\theta(y_k)\big) \Big \rbrack
        $$
    - 该损失函数的直观作用是: **鼓励模型拉大强弱对象之间的分数差**, 使得模型的预测结果与观测到的胜负关系一致;
    <!--
    - 综上, 可以通过 **最大化 模型预测出的强度关系 与 真实比较结果 的似然** 来优化模型, 其对应的 **负对数似然损失函数** 为:
        $$
        \mathcal{L}(\theta;D) = -\mathbb{E}_{(y_j, y_k) \sim D} \Big \lbrack \log \, \sigma\big(\mathcal{R}_\theta(y_j) - \mathcal{R}_\theta(y_k)\big) \Big \rbrack
        $$
    - 该损失函数的直观作用是: **鼓励模型拉大强弱对象之间的分数差**, 使得模型的预测结果与观测到的胜负关系一致;
    -->

<!--START_SECTION:keyword-->
<!--keyword_info
name: '策略优化'
extra_url: false
-->
### 3. **策略优化 (Policy Optimization)**
<!--END_SECTION:keyword-->
> 基于 **强化学习 (RL) 框架** 进一步优化 SFT 模型, 对齐人类偏好.

- **动机**:
    - SFT 模型受限于静态数据, 无法主动探索更优解, 需要更大的策略空间;
- **目的/预期**:
    <!-- - **对齐人类偏好**; -->
    - 在奖励模型引导下, 在更大的策略空间中寻找高奖励输出, 生成更符合人类偏好的输出;
- **输入**:
    > 以 PPO 算法为例, 不同策略优化算法需要的输入不同.
    - 初始策略模型 (SFT 模型);
    - 奖励模型 (人类偏好代理);
    - 参考模型 (提供 KL 约束);
    - 无标注 Prompt 数据集;
- **输出**:
    - 对齐人类偏好的最终模型 (经过 RLHF 微调后的模型)
- **失败模式**:
    - **奖励过度**/**过度优化**: 在奖励模型的打分上越来越高, 但实际输出质量下降;
    - **模式坍塌**: 过度优化的副作用之一, 输出模式单一, 失去多样性 (模型为了获得高分只会生成「安全」回答);
    - **奖励黑客 (Reward Hacking)**: 模型学会利用奖励模型漏洞而非真正提升质量; 比如更冗长的回答/更礼貌的回答等, 观感提升但忽视质量;
    - 除此之外, 还有 **偏好冲突**/**价值错配** 等失败模式,
    - **根源** 一般是 "**奖励信号不完美/失真 + 优化算法过度依赖奖励模型**";


<!-- ### 策略优化 (Policy Optimization) -->


<!--START_SECTION:keyword-->
<!--keyword_info
name: 'RL 基础'
extra_url: false
-->
#### 强化学习基础
<!--END_SECTION:keyword-->
> RL 一般应用于游戏智能体等场景, 其具体过程和术语需要做相应的调整以适应序列生成任务.

<!-- - 将 RL 框架中的 **交互闭环** 映射到 LLM 的优化过程; -->
<!-- - 建立直觉, 将 LLM 的生成过程映射到 RL 框架; -->
<!-- - 下面是一些 **关键术语映射**: -->
概念 | RL 中的含义 | RLHF 中对应的含义
---------|----------|---------
**智能体** (Agent) | 学习者或决策者, 是优化的目标对象. | 在 RLHF 中, 智能体被称为 **策略模型 (Policy Model)**, 通常是基于 SFT 模型进行初始化的 LLM 本身.
**策略** (Policy, $\pi$) | • 智能体的行为函数, 通常参数化为 $\pi_{\theta}(\cdot)$;<br>• 它定义了从 **状态** $s$ 到 **动作** $a$ 的映射, 即 $a = \pi(s)$. | 在文本生成中, 即策略模型根据当前 **上下文 (状态)** 输出下一个 **token (动作)** 的概率分布.
**环境** (Environment) | 智能体与之交互的外部世界 | 即当前的 **上下文**, 由给定的 **提示 (Prompt)** 和 **已经生成的 token 序列** 共同构成.
**状态** (State, $s$) | 环境在当前时刻的描述 | 上下文在当前时刻的 **向量表示**
**动作** (Action, $a$) | 智能体在给定状态下采取的操作 | 给定上下文 **生成下一个 token**
**奖励** (Reward, $r$) | 环境在智能体执行动作后反馈的 **标量信号**, 用于评估动作的好坏. | 在 RLHF 中, 奖励由一个独立训练的 **奖励模型 (Reward Model, RM)** 提供, 负责评估 **Prompt+生成文本** 的质量.
**价值函数** (Value Function, **可选**) | 从某个状态开始, 遵循当前策略所能获得的期望累积奖励, 包括:<br>• **状态价值函数** $V(s)$: 从状态 $s$ 开始, 遵循策略 $\pi$ 所能获得的期望累积回报;<br>• **动作价值函数** $Q(s, a)$: 在状态 $s$ 执行动作 $a$ 后, 遵循策略 $\pi$ 所能获得的期望累积回报. | • 在诸如 TRPO/PPO 等 **基于 Actor-Critic 框架** 的 **在线策略 (On-Policy) 梯度算法** 中, 通常会训练一个 **价值模型 (Critic)** 来近似 **状态价值函数** $V(s)$;<br>• 其主要作用是计算 **优势函数** $A(s, a) = Q(s, a) - V(s)$;<br>• 能有效 **降低** **策略梯度估计的方差/训练方差**;<br>• **离线策略 (Off-Policy)** 算法中一般 **不需要价值模型**.
**折扣因子** (Discount Factor, $\gamma$) | 用于权衡即时奖励和未来奖励的重要性, 记 $\gamma \in (0,1)$;<br>• $\gamma \to 0$ 更注重短期收益,<br>• $\gamma \to 1$ 更注重长期收益. | • 在 RLHF 中, 通常 **不考虑长期奖励衰减**, 即未来 token 的重要性与当前 token 相当;<br>• 因此 $\gamma$ **常被设置为接近或等于 $1$**, 意味着较少考虑未来奖励的衰减;<br>• 但这并非固定不变, 可根据具体需求调整.
<!-- 这在 TRPO/PPO 等 **在线 (online) 策略梯度算法 (基于 Actor-Critic 框架)** 中对于降低训练方差至关重要. -->
<!--
RL 中的概念 | RLHF 中的含义或术语 | 说明
---|---|---
智能体 (Agent) / 动作模型 (Actor Model) | 策略模型 (Policy Model) | 即需要被微调的语言模型本身.
环境 (Environment) | 上下文 (Prompt + 已生成 token 序列) | 模型生成时所处的信息背景.
状态 (State) | 上下文在当前时刻的向量表示 | 在标准 RLHF 流程中, 状态通常是隐式地由模型内部隐藏状态表示的.
动作 (Action) | 生成 Response (episode-level) 或生成下一个 Token (token-level) | RLHF 中多将一次完整回答视为单个动作; token 级动作用于更细粒度的优化, 同时会加入 **回合** 的概念, 表示生成一次完整的回答.
奖励 (Reward) | **奖励模型** 分数 | 通常由奖励模型对完整的 "Prompt + Response" 计算得到一个标量得分.
策略 (Policy) | 策略模型的概率分布 | RL 中表示 **当前状态到下一个动作** 的映射 $\pi(a\|s)$; RLHF 中即 **从当前上下文生成下一个 token 的概率分布** (token-level) 或 **完整回答的概率分布** (episode-level).
价值函数 (Value Function) | 价值模型 (可选) | 估计状态或状态-动作对的期望奖励; 用于 **优势函数** 计算; RLHF 中用于估计从当前上下文继续生成至结束的期望奖励; 价值函数
折扣因子 $\gamma$ (Discount Factor) | 无直接对应或默认 $\gamma=1$ | 用于权衡当前奖励和未来奖励的重要性; RLHF 中通常不考虑长期奖励衰减 (未来 token 的重要性与当前 token 几乎相当), 因此 $\gamma$ 常被设置为一个接近 $1$ 的值.
-->

<!--
#### 核心组件

- **策略模型** (Policy Model)
    -
- **策略优化算法** / **策略梯度算法**
    > TRPO → PPO (Clip/KL) → DPO → GRPO/IPO/KTO/...
- **参考模型** (Reference Model)
- **奖励模型** (Reward Model, 可选)
- **价值模型** (Critic, 可选)
 -->

<!--START_SECTION:keyword-->
<!--keyword_info
name: 'PPO,DPO,GRPO,..'
extra_url: true
-->
#### **策略梯度算法**
> _[策略梯度 (Policy Gradient) 算法笔记](./策略梯度算法.md)_
>> TRPO → PPO (Clip/KL) → DPO → GRPO/IPO/KTO/...
<!--END_SECTION:keyword-->


---

<!--START_SECTION:keyword-->
<!--keyword_info
name: 'QA'
extra_url: true
-->
## 面试问题整理
> _[RLHF 面试问题整理](./偏好学习_QA.md)_
<!--END_SECTION:keyword-->