RLHF 中的策略梯度算法 (Policy Gradient)
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2025-09-24&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-25%2003%3A17%3A40&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
date: 2025-09-24 06:04:17
toc_title: '**策略梯度算法**'
top: false
draft: false
hidden: true
omit_in_tag_toc: false
section_number: true
level: 0
tags: []
-->

<!--START_SECTION:keywords-->
> ***Keywords**: [偏好学习](./偏好学习.md)*
<!--END_SECTION:keywords-->

<!--START_SECTION:paper_title-->
<!--END_SECTION:paper_title-->

<!--START_SECTION:toc-->
- [基础概念](#基础概念)
- [PPO (Proximal Policy Optimization)](#ppo-proximal-policy-optimization)
- [DPO (Direct Preference Optimization)](#dpo-direct-preference-optimization)
- [GRPO (Group Relative Policy Optimization)](#grpo-group-relative-policy-optimization)
- [参考](#参考)
<!--END_SECTION:toc-->

---

## 基础概念

- **背景**
    -
- **核心目标**:
    - 通过优化模型参数, 最大化策略模型在环境中能获得的期望累积奖励 (回报);


<!--START_SECTION:keyword-->
<!--keyword_info
name: 'PPO'
extra_url: false
-->
## PPO (Proximal Policy Optimization)
<!--END_SECTION:keyword-->



<!--START_SECTION:keyword-->
<!--keyword_info
name: 'DPO'
extra_url: false
-->
## DPO (Direct Preference Optimization)
<!--END_SECTION:keyword-->



<!--START_SECTION:keyword-->
<!--keyword_info
name: 'GRPO'
extra_url: false
-->
## GRPO (Group Relative Policy Optimization)
<!--END_SECTION:keyword-->


---

<!-- no toc -->
## 参考