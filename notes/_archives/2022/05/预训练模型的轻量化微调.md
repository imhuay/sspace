预训练模型的轻量化微调
===
<!--START_SECTION:badge-->

![create date](https://img.shields.io/static/v1?label=create%20date&message=2022-05-xx&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-08-03%2022%3A42%3A16&label_color=gray&color=thistle&style=flat-square)

<!--END_SECTION:badge-->
<!--info
top: false
draft: true
hidden: true
tag: [dl_sft]
-->

> ***Keywords**: sft*

<!--START_SECTION:toc-->
- [背景](#背景)
    - [三种解决思路](#三种解决思路)
- [轻量化微调 (Lightweight Finetuning) ](#轻量化微调lightweight-finetuning)
    - [Adapter](#adapter)
    - [LoRA](#lora)
    - [Prefix-Tuning](#prefix-tuning)
    - [其他论文](#其他论文)
- [参考](#参考)
<!--END_SECTION:toc-->

---

## 背景
> [从遗忘问题到预训练轻量化微调 - 李rumor](https://mp.weixin.qq.com/s/C_6qlTq63IBnRSEMnDO7SQ)

- "**预训练+微调**" 范式有效的前提是, 我们假设模型在海量数据上学到了大量**先验知识**, 而这些知识被存储在模型的**参数**中;
- 对整个预训练模型进行微调, 意味着会**改动**这些参数; 如果变动太大, 那么就可能会带来 "**灾难性遗忘**" 的问题;
    - 一个简单的验证方法: "有的时候, 大家可以试试学习率大一些跑跑, 会发现学几代以后loss就会骤变, 这个其实就是重现遗忘最简单的方式."
    - 不过参数变动不代表知识一定会丢失, 大多数情况下, "预训练+微调" 依然是有效的;


### 三种解决思路

1. **Replay**: 重播, 就是在新任务中, 把老的内容复习一遍, 能让模型保留住.
    - 虽然实现简单, 但是现在的预训练模型和数据都很 "重" , 成本很大;
2. **Regularization-Based**: 在损失函数上应用正则化方法, 使新模型和原模型之间的差距不会很大 (跟蒸馏的思想很接近)
    - 实现简单, 对模型的改动很小;
    - 这个方法有两个问题: 1) 需要微调整个模型, 效率低; 2) 可能会导致**训练目标的偏移**, 即得到的不是最优解;
3. **Lightweight Finetuning**: 轻量化微调, 目前比较常见的方法是**参数孤立化** (Parameter Isolation) , 即冻住预训练模型, 加入新的模块, 只微调该模块来避免遗忘的问题.
    - 目前比较主流的方法, 兼顾了 "遗忘问题" 和训练效率;


## 轻量化微调 (Lightweight Finetuning)

- 目前**轻量化微调**的主要方法是**参数孤立化**, 即不动预训练模型, 而是在预训练模型的基础上增加新的可训练参数
    > **关键词**: `Parameter Isolation`、 `Parameter-Efficient`
- 下面介绍三种常见的**参数孤立化**方法;

### Adapter
> [[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)


### LoRA
> [[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)


### Prefix-Tuning
> [[2101.00190] Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)


### 其他论文

- [[2110.04366] Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.04366)
    - 对以上三种方法进行了总结,


## 参考
- A continual learning survey: Defying forgetting in classification tasks
    > 从 NLP 领域出发, 介绍遗忘存在的本质, 并综述了一些比较常见的解决方案