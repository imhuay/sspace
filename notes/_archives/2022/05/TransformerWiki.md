Transformer Wiki
===
<!--START_SECTION:badge-->
![create date](https://img.shields.io/static/v1?label=create%20date&message=2022-05-xx&label_color=gray&color=lightsteelblue&style=flat-square)
![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2025-09-19%2004%3A11%3A35&label_color=gray&color=thistle&style=flat-square)
<!--END_SECTION:badge-->
<!--info
top: false
draft: true
hidden: false
tags: [dl_bert]
-->

> ***Keywords**: transformer*

<!--START_SECTION:toc-->
- [背景](#背景)
- [常见 Transformer 变体](#常见-transformer-变体)
    - [长度外推性](#长度外推性)
- [常见面试问题](#常见面试问题)
<!--END_SECTION:toc-->

---

## 背景
- 原始 Transformer 指的是一个基于 Encoder-Decoder 框架的 Seq2Seq 模型, 用于解决机器翻译任务;
- 后其 Encoder 部分被用于 BERT 而广为人知, 因此有时 Transformer 也特指其 Encoder 部分;
- 相关论文:
    - [[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - [[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

## 常见 Transformer 变体
> [Transformers系列模型](../10/Transformer系列模型.md)

### 长度外推性
> [Transformer与长度外推性](../../2023/02/Transformer与长度外推性.md)

## 常见面试问题
> [Transformer 常见面试问题](./Transformer常见问题.md)
